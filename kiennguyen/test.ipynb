{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e015c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Statement Data Sets\n",
      "Financial Statement Data\n",
      "Sets\n",
      "Contents\n",
      "1\n",
      "Overview\n",
      ".. 1\n",
      "2\n",
      "Scope\n",
      ". 2\n",
      "3\n",
      "Organization\n",
      ". 2\n",
      "4\n",
      "File Formats\n",
      ". 3\n",
      "5\n",
      "Table Definitions\n",
      ". 3\n",
      "5.1\n",
      "SUB (Submissions)\n",
      "3\n",
      "5.2\n",
      "TAG (Tags)\n",
      "7\n",
      "5.3\n",
      "NUM (Numbers)\n",
      "7\n",
      "5.4\n",
      "PRE (Presentation of Statements)\n",
      "8\n",
      "Figure 1. Data relationships\n",
      ". 3\n",
      "Figure 2. Fields in the SUB data set\n",
      "4\n",
      "Figure 3. Fields in the TAG data set\n",
      "7\n",
      "Figure 4. Fields in the NUM data set\n",
      "8\n",
      "Figure 5. Fields in the PRE data set\n",
      "8\n",
      "1\n",
      "Overview\n",
      "The following data sets provide information extracted from XBRL\n",
      "submissions filed with the Commission in a flattened data format to assist\n",
      "users in more easily consuming the data for analysis. The data is sourced from\n",
      "selected information found in the XBRL tagged financial statements submitted by\n",
      "filers to the Commission. ¬†These data sets currently include quarterly and\n",
      "annual numeric data rendered by the Commission in the primary financial\n",
      "statements submitted by filers. Certain additional fields (e.g. Standard\n",
      "Industrial Classification (SIC)) used in the Commission‚Äôs EDGAR system are also\n",
      "included to help in supporting the use of the data. ¬†The information has\n",
      "been taken directly from submissions created by each registrant, and the data\n",
      "is ‚Äúas filed‚Äù by the registrant. ¬†The information will be updated\n",
      "quarterly. Data contained in documents filed after the last business day of the\n",
      "quarter will be included in the next quarterly posting.\n",
      "DISCLAIMER: The Financial Statement Data Sets contain\n",
      "information derived from structured data filed with the Commission by\n",
      "individual registrants as well as Commission-generated filing identifiers.\n",
      "Because the data sets are derived from information provided by individual\n",
      "registrants, we cannot guarantee the accuracy of the data sets. In addition, it\n",
      "is possible inaccuracies or other errors were introduced into the data sets\n",
      "during the process of extracting the data and compiling the data sets. Finally,\n",
      "the data sets do not reflect all available information, including certain\n",
      "metadata associated with Commission filings. The data sets are intended to\n",
      "assist the public in analyzing data contained in Commission filings; however,\n",
      "they are not a substitute for such filings. Investors should review the full\n",
      "Commission filings before making any investment decision.\n",
      "The data extracted from the XBRL submissions is organized\n",
      "into four data sets containing information about submissions, numbers, taxonomy\n",
      "tags, and presentation.¬† Each data set consists of rows and columns and is\n",
      "provided as a tab-delimited TXT format file.¬† The data sets are as\n",
      "follows:\n",
      "¬∑\n",
      "SUB\n",
      "‚Äì Submission data set; this includes one record for each XBRL\n",
      "submission with amounts rendered by the Commission in the primary financial\n",
      "statements. The set includes fields of information pertinent to the submission\n",
      "and the filing entity. Information is extracted from the SEC‚Äôs EDGAR system and\n",
      "the filings submitted to the SEC by registrants.\n",
      "¬∑\n",
      "NUM\n",
      "‚Äì Number data set; this includes one row for each distinct\n",
      "amount appearing on the primary financial statements rendered by the Commission\n",
      "from each submission included in the SUB data set.\n",
      "¬∑\n",
      "TAG\n",
      "‚Äì Tag data set; includes defining information about each numerical\n",
      "tag. ¬†Information includes tag descriptions (documentation labels),\n",
      "taxonomy version information and other tag attributes.\n",
      "¬∑\n",
      "PRE\n",
      "‚Äì Presentation data set; this provides information about how\n",
      "the tags and numbers were presented in the primary financial statements as\n",
      "rendered by the Commission.\n",
      "2\n",
      "Scope\n",
      "The scope of the data in the financial statement data sets\n",
      "consists of:\n",
      "¬∑\n",
      "Numeric data on the primary financial statements as rendered by the\n",
      "Commission (Balance Sheet, Income Statement, Cash Flows, Changes in Equity, and\n",
      "Comprehensive Income) and page footnotes on those statements;\n",
      "¬∑\n",
      "From XBRL ¬†submissions which include financial statements rendered by\n",
      "the Commission ¬†(e.g., 10-K, 10-Q, 20-F, 40-F);\n",
      "¬∑\n",
      "Submitted from 4/15/2009 through the ‚ÄúData Cutoff Date‚Äù inclusive (there\n",
      "is a file named 2009q1.zip on the SEC website that contains data sets with\n",
      "column headings only and no rows, merely so that all years prior to this year\n",
      "will consist of four zip files).\n",
      "All numeric data is ‚Äúas filed.‚Äù\n",
      "3\n",
      "Organization\n",
      "Note that this data set represents quarterly and annual\n",
      "uncorrected and ‚Äúas filed‚Äù EDGAR document submissions containing multiple\n",
      "reporting periods (including amendments of prior submissions). Data in this\n",
      "submitted form may contain redundancies, inconsistencies, and discrepancies\n",
      "relative to other publication formats. There are four data sets.\n",
      "1.\n",
      "SUB\n",
      "identifies all the EDGAR submissions with amounts rendered by the Commission on\n",
      "the primary financial statements in the data set, with each row having the\n",
      "unique (primary) key\n",
      "adsh,\n",
      "a 20 character EDGAR Accession Number with\n",
      "dashes in positions 11 and 14.\n",
      "2.\n",
      "TAG\n",
      "is a data set of all numerical tags used in the submissions, both standard and\n",
      "custom. ¬†A unique key of each row is a combination of these fields:\n",
      "1)\n",
      "tag ‚Äì\n",
      "tag used by the filer\n",
      "2)\n",
      "version\n",
      "‚Äì if a standard tag, the taxonomy of origin, otherwise\n",
      "equal to adsh.\n",
      "3.\n",
      "NUM\n",
      "is a data set of all numeric XBRL facts presented on the primary financial\n",
      "statements as rendered by the Comission. A unique key of each row is a\n",
      "combination of the following fields:\n",
      "1)\n",
      "adsh\n",
      "- EDGAR accession number\n",
      "2)\n",
      "tag ‚Äì\n",
      "tag used by the filer\n",
      "3)\n",
      "version\n",
      "‚Äì if a standard tag, the taxonomy of origin, otherwise\n",
      "equal to adsh.\n",
      "4)\n",
      "ddate\n",
      "- period end date\n",
      "5)\n",
      "qtrs\n",
      "- duration in number of quarters\n",
      "6)\n",
      "uom -\n",
      "unit of measure\n",
      "7)\n",
      "segments\n",
      "‚Äì XBRL tags used to represent axis and member reporting\n",
      "8)\n",
      "coreg -\n",
      "coregistrant of the parent company registrant (if\n",
      "applicable)\n",
      "4.\n",
      "PRE\n",
      "is a data set that provides the text assigned by the filer to each line item in\n",
      "the primary financial statements, the order in which the line item appeared,\n",
      "and the tag assigned to it.¬† A unique key of each row is a combination of\n",
      "the following fields:\n",
      "1)\n",
      "adsh ‚Äì\n",
      "EDGAR accession number\n",
      "2)\n",
      "report\n",
      "‚Äì sequential number of report within the statements\n",
      "3)\n",
      "line\n",
      "‚Äì sequential number of line within a report.\n",
      "The relationship of the data sets is as shown in Figure 1.\n",
      "The Accession Number\n",
      "(adsh)\n",
      "found in the NUM data set can be used to\n",
      "retrieve information about the submission in SUB. ¬†Each row of data in NUM\n",
      "was tagged by the filer using a tag. Information about the tag used can be\n",
      "found in ¬†TAG.¬† Each row of data in NUM appears on one or more lines\n",
      "of reports detailed in PRE.\n",
      "Figure 1. Data relationships\n",
      "Dataset\n",
      "Columns referencing other datasets\n",
      "Referenced dataset\n",
      "Referenced columns\n",
      "NUM\n",
      "adsh\n",
      "SUB\n",
      "adsh\n",
      "tag, version\n",
      "TAG\n",
      "tag, version\n",
      "PRE\n",
      "adsh\n",
      "SUB\n",
      "adsh\n",
      "tag, version\n",
      "TAG\n",
      "tag, version\n",
      "adsh, tag, version\n",
      "NUM\n",
      "adsh, tag, version\n",
      "Note: The SEC website folder http://www.sec.gov/Archives/edgar/data/{\n",
      "cik\n",
      "}/{\n",
      "accession\n",
      "}/\n",
      "will always contain all the files for a given submission, where {\n",
      "accession\n",
      "}\n",
      "is the\n",
      "adsh\n",
      "with the ‚Äò-‚Äòcharacters removed.\n",
      "4\n",
      "File Formats\n",
      "Each of the four data sets is\n",
      "provided in a single encoding, as follows:\n",
      "Tab Delimited Value (.txt): utf-8,\n",
      "tab-delimited, \\n- terminated lines, with the first line containing the column\n",
      "names in lowercase.\n",
      "5\n",
      "Table Definitions\n",
      "The columns in the figures below (figures 2 ‚Äì 5) provide the\n",
      "following information: field name, description, source (SUB file only), data\n",
      "format, maximum field size, an indication of whether or not the field may be\n",
      "NULL (yes or no), and key.\n",
      "The Source column in the SUB file has two possible values:\n",
      "¬∑\n",
      "EDGAR indicates that the source of the data is the filer‚Äôs EDGAR\n",
      "submission header.\n",
      "¬∑\n",
      "XBRL indicates that the source of the data is the filer‚Äôs XBRL\n",
      "submission.\n",
      "The Key column indicates whether the field is part of a\n",
      "unique index on the data.¬† There are two possible values for this column:\n",
      "¬∑\n",
      "‚Äú*‚Äù\n",
      "‚Äì Indicates\n",
      "the field is part of a unique key for the row.\n",
      "¬∑\n",
      "Empty (nothing in column) ‚Äì the column is a function of all or some of a\n",
      "unique key.\n",
      "5.1\n",
      "SUB\n",
      "(Submissions)\n",
      "The submissions data set contains summary information about\n",
      "an entire EDGAR submission. Some fields were sourced directly from EDGAR\n",
      "submission information, while other columns of data were sourced from the XBRL\n",
      "submission. Note: EDGAR derived fields represent the most recent EDGAR\n",
      "assignment as of a given filing‚Äôs submission date and do not necessarily\n",
      "represent the most current assignments.\n",
      "Figure\n",
      "2. Fields in the SUB data set\n",
      "Field Name\n",
      "Field Description\n",
      "Source\n",
      "Format\n",
      "Max Size\n",
      "May¬†be NULL\n",
      "Key\n",
      "adsh\n",
      "Accession Number. The 20-character string formed from\n",
      "  the 18-digit number assigned by the SEC to each EDGAR submission.\n",
      "EDGAR\n",
      "ALPHANUMERIC (nnnnnnnnnn-nn-nnnnnn)\n",
      "20\n",
      "No\n",
      "*\n",
      "cik\n",
      "Central Index Key (CIK). Ten digit number assigned by\n",
      "  the SEC to each registrant that submits filings.\n",
      "EDGAR\n",
      "NUMERIC\n",
      "10\n",
      "No\n",
      "name\n",
      "Name of registrant. This corresponds to the name of the\n",
      "  legal entity as recorded in EDGAR as of the filing date.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "150\n",
      "No\n",
      "sic\n",
      "Standard Industrial Classification (SIC). Four digit\n",
      "  code assigned by the SEC as of the filing date, indicating the registrant‚Äôs\n",
      "  type of business.\n",
      "EDGAR\n",
      "NUMERIC\n",
      "4\n",
      "Yes\n",
      "countryba\n",
      "The ISO 3166-1 country of the registrant's business\n",
      "  address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "stprba\n",
      "The state or province of the registrant‚Äôs business\n",
      "  address, if field countryba is US or CA.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "cityba\n",
      "The city of the registrant's business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "30\n",
      "Yes\n",
      "zipba\n",
      "The zip code of the registrant‚Äôs business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "10\n",
      "Yes\n",
      "bas1\n",
      "The first line of the street of the registrant‚Äôs\n",
      "  business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "40\n",
      "Yes\n",
      "bas2\n",
      "The second line of the street of the registrant‚Äôs\n",
      "  business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "40\n",
      "Yes\n",
      "baph\n",
      "The phone number of the registrant‚Äôs business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "20\n",
      "Yes\n",
      "countryma\n",
      "The ISO 3166-1 country of the registrant's mailing\n",
      "  address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "stprma\n",
      "The state or province of the registrant‚Äôs mailing\n",
      "  address, if field countryma is US or CA.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "cityma\n",
      "The city of the registrant's mailing address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "30\n",
      "Yes\n",
      "zipma\n",
      "The zip code of the registrant‚Äôs mailing address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "10\n",
      "Yes\n",
      "mas1\n",
      "The first line of the street of the registrant‚Äôs\n",
      "  mailing address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "40\n",
      "Yes\n",
      "mas2\n",
      "The second line of the street of the registrant‚Äôs\n",
      "  mailing address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "40\n",
      "Yes\n",
      "countryinc\n",
      "The ISO 3166-1 country of incorporation for the\n",
      "  registrant.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "3\n",
      "Yes\n",
      "stprinc\n",
      "The state or province of incorporation for the\n",
      "  registrant, if countryinc is US or CA.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "ein\n",
      "Employee Identification Number, 9 digit identification\n",
      "  number assigned by the Internal Revenue Service to business entities\n",
      "  operating in the United States.\n",
      "EDGAR\n",
      "NUMERIC\n",
      "10\n",
      "Yes\n",
      "former\n",
      "Most recent former name of the registrant, if any.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "150\n",
      "Yes\n",
      "changed\n",
      "Date of change from the former name, if any.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "8\n",
      "Yes\n",
      "afs\n",
      "Filer status with the SEC at the time of submission:\n",
      "1-LAF=Large Accelerated,\n",
      "2-ACC=Accelerated,\n",
      "3-SRA=Smaller Reporting Accelerated,\n",
      "4-NON=Non-Accelerated,\n",
      "5-SML=Smaller Reporting Filer,\n",
      "NULL=not assigned.\n",
      "XBRL\n",
      "ALPHANUMERIC\n",
      "5\n",
      "Yes\n",
      "wksi\n",
      "Well Known Seasoned Issuer (WKSI). An issuer that meets\n",
      "  specific SEC requirements at some point during a 60-day period preceding the\n",
      "  date the issuer satisfies its obligation to update its shelf registration\n",
      "  statement.\n",
      "XBRL\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n",
      "fye\n",
      "Fiscal Year End Date, rounded to nearest month-end.\n",
      "XBRL\n",
      "ALPHANUMERIC (mmdd)\n",
      "4\n",
      "Yes\n",
      "form\n",
      "The submission type of the registrant‚Äôs filing.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "10\n",
      "No\n",
      "period\n",
      "Balance Sheet Date, rounded to nearest month-end.\n",
      "XBRL\n",
      "DATE (yyyymmdd)\n",
      "8\n",
      "No\n",
      "fy\n",
      "Fiscal Year Focus (as defined in the EDGAR XBRL Guide Ch.\n",
      "  3.1.8).\n",
      "XBRL\n",
      "YEAR (yyyy)\n",
      "4\n",
      "Yes\n",
      "fp\n",
      "Fiscal Period Focus (as defined in the EDGAR XBRL Guide\n",
      "  Ch. 3.1.8) within Fiscal Year.\n",
      "XBRL\n",
      "ALPHANUMERIC (FY, Q1, Q2, Q3, Q4)\n",
      "2\n",
      "Yes\n",
      "filed\n",
      "The date of the registrant‚Äôs filing with the\n",
      "  Commission.\n",
      "EDGAR\n",
      "DATE (yyyymmdd)\n",
      "8\n",
      "No\n",
      "accepted\n",
      "The acceptance date and time of the registrant‚Äôs filing\n",
      "  with the Commission.\n",
      "EDGAR\n",
      "DATETIME (yyyy‚Äëmm‚Äëdd¬†hh:mm:ss)\n",
      "19\n",
      "No\n",
      "prevrpt\n",
      "Previous Report ‚ÄìTRUE indicates that the submission\n",
      "  information was subsequently amended.\n",
      "EDGAR\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n",
      "detail\n",
      "TRUE indicates that the XBRL submission contains\n",
      "  quantitative disclosures within the footnotes and schedules at the required\n",
      "  detail level (e.g., each amount).\n",
      "XBRL\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n",
      "instance\n",
      "The name of the submitted XBRL Instance Document. The\n",
      "  name often begins with the company ticker symbol.\n",
      "EDGAR\n",
      "ALPHANUMERIC (e.g. abcd‚Äëyyyymmdd.xml)\n",
      "40\n",
      "No\n",
      "nciks\n",
      "Number of Central Index Keys (CIK) of registrants\n",
      "  (i.e., business units) included in the consolidating entity‚Äôs submitted\n",
      "  filing.\n",
      "EDGAR\n",
      "NUMERIC\n",
      "4\n",
      "No\n",
      "aciks\n",
      "Additional CIKs of co-registrants included in¬†a\n",
      "  consolidating entity‚Äôs EDGAR submission, separated by spaces. If there are no\n",
      "  other co-registrants (i.e., nciks=1), the value of aciks is NULL.¬† For a\n",
      "  very small number of filers, the entire list of co-registrants is too long to\n",
      "  fit in the field.¬† Where this is the case, users should refer to the\n",
      "  complete submission file for all CIK information.\n",
      "EDGAR\n",
      "ALPHANUMERIC (space delimited)\n",
      "120\n",
      "Yes\n",
      "Note: To access the complete submission files for a given\n",
      "filing, please see the\n",
      "SEC EDGAR website\n",
      ".¬†\n",
      "The SEC website folder http://www.sec.gov/Archives/edgar/data/{\n",
      "cik\n",
      "}/{\n",
      "accession\n",
      "}/\n",
      "will always contain all the files for a given submission.¬† To assemble the\n",
      "folder address to any filing referenced in the SUB data set, simply substitute\n",
      "{\n",
      "cik\n",
      "} with the\n",
      "cik\n",
      "field and replace {\n",
      "accession\n",
      "} with the\n",
      "adsh\n",
      "field (after removing the dash character).¬† The following sample SQL Query\n",
      "provides an example of how to generate a list of addresses for filings\n",
      "contained in the SUB data set:\n",
      "¬∑\n",
      "select name,form,period,\n",
      "'http://www.sec.gov/Archives/edgar/data/' + ltrim(str(\n",
      "cik\n",
      ",10))+'/' +\n",
      "replace(\n",
      "adsh\n",
      ",'-','')+'/'+\n",
      "instance\n",
      "as url from SUB sub order by\n",
      "period desc, name\n",
      "5.2\n",
      "TAG (Tags)\n",
      "The TAG data set contains the standard taxonomy tags and the\n",
      "custom taxonomy tags defined in the submissions.¬† The source is the ‚Äúas\n",
      "filed‚Äù XBRL filer submissions.¬† The standard tags are derived from\n",
      "taxonomies in\n",
      "https://www.sec.gov/data-research/standard-taxonomies\n",
      ".\n",
      "Figure\n",
      "3. Fields in the TAG data set\n",
      "Field Name\n",
      "Field Description\n",
      "Field Type\n",
      "Max Size\n",
      "May¬†be NULL\n",
      "Key\n",
      "tag\n",
      "The unique identifier (name) for a tag in a specific\n",
      "  taxonomy release.\n",
      "ALPHANUMERIC\n",
      "256\n",
      "No\n",
      "*\n",
      "version\n",
      "For a standard tag, an identifier for the taxonomy;\n",
      "  otherwise the accession number where the tag was defined.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "custom\n",
      "1 if tag is custom (version=adsh), 0 if it is standard.\n",
      "Note: This flag is technically redundant with the ¬†version and adsh\n",
      "  columns.\n",
      "BOOLEAN (1 if true and 0 if false)\n",
      "1\n",
      "No\n",
      "abstract\n",
      "1 if the tag is not used to represent a numeric fact.\n",
      "BOOLEAN (1 if true and 0 if false)\n",
      "1\n",
      "No\n",
      "datatype\n",
      "If abstract=1, then NULL, otherwise the data type\n",
      "  (e.g., monetary) for the tag.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "Yes\n",
      "iord\n",
      "If abstract=1, then NULL; otherwise, ‚ÄúI‚Äù if the value\n",
      "  is a point-in time, or ‚ÄúD‚Äù if the value is a duration.\n",
      "ALPHANUMERIC\n",
      "1\n",
      "No\n",
      "crdr\n",
      "If datatype = monetary, then the tag‚Äôs natural\n",
      "  accounting balance (debit or credit); if not defined, then NULL.\n",
      "ALPHANUMERIC (‚ÄúC‚Äù or ‚ÄúD‚Äù)\n",
      "1\n",
      "Yes\n",
      "tlabel\n",
      "If a standard tag, then the label text provided by the\n",
      "  taxonomy, otherwise the text provided by the filer.¬† A tag which had\n",
      "  neither would have a NULL value here.\n",
      "ALPHANUMERIC\n",
      "512\n",
      "Yes\n",
      "doc\n",
      "The detailed definition for the tag. If a standard tag,\n",
      "  then the text provided by the taxonomy, otherwise the text assigned by the\n",
      "  filer.¬† Some tags have neither, and this field is NULL.\n",
      "ALPHANUMERIC\n",
      "Yes\n",
      "5.3\n",
      "NUM (Numbers)\n",
      "The NUM data set contains numeric data, one row per data\n",
      "point as rendered by the Commission on the primary financial statements. The\n",
      "source for the table is the ‚Äúas filed‚Äù XBRL filer submissions.\n",
      "Figure 4. Fields in the NUM data set\n",
      "Field Name\n",
      "Field Description\n",
      "Field Type (format)\n",
      "Max Size\n",
      "May¬†be NULL\n",
      "Key\n",
      "adsh\n",
      "Accession Number. The 20-character string formed from\n",
      "  the 18-digit number assigned by the SEC to each EDGAR submission.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "tag\n",
      "The unique identifier (name) for a tag in a specific\n",
      "  taxonomy release.\n",
      "ALPHANUMERIC\n",
      "256\n",
      "No\n",
      "*\n",
      "version\n",
      "For a standard tag, an identifier for the taxonomy;\n",
      "  otherwise the accession number where the tag was defined.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "ddate\n",
      "The end date for the data value, rounded to the nearest\n",
      "  month end.\n",
      "DATE (yyyymmdd)\n",
      "8\n",
      "No\n",
      "*\n",
      "qtrs\n",
      "The count of the number of quarters represented by the\n",
      "  data value, rounded to the nearest whole number. ‚Äú0‚Äù indicates it is a\n",
      "  point-in-time value.\n",
      "NUMERIC\n",
      "8\n",
      "No\n",
      "*\n",
      "uom\n",
      "The unit of measure for the value.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "segments\n",
      "Tags used to represent axis and member reporting.\n",
      "ALPHANUMERIC\n",
      "1024\n",
      "Yes\n",
      "*\n",
      "coreg\n",
      "If specified, indicates a specific co-registrant, the\n",
      "  parent company, or other entity (e.g., guarantor). ¬†NULL indicates the consolidated\n",
      "  entity.\n",
      "ALPHANUMERIC\n",
      "256\n",
      "Yes\n",
      "*\n",
      "value\n",
      "The value. This is not scaled, it is as found in the\n",
      "  Interactive Data file, but is limited to four digits to the right of the\n",
      "  decimal point.\n",
      "NUMERIC(28,4)\n",
      "16\n",
      "Yes\n",
      "footnote\n",
      "The text of any superscripted footnotes on the value,\n",
      "  as shown on the statement page, truncated to 512 characters, or if there is\n",
      "  no footnote, then this field will be blank.\n",
      "ALPHANUMERIC\n",
      "512\n",
      "Yes\n",
      "5.4\n",
      "PRE\n",
      "(Presentation of Statements)\n",
      "The PRE data set contains one row for each line of the\n",
      "financial statements tagged by the filer.¬† The source for the data set is\n",
      "the ‚Äúas filed‚Äù XBRL filer submissions.¬†¬† Note that there may be more\n",
      "than one row per entry in NUM because the same tag can appear in more than one\n",
      "statement (the tag NetIncome, for example can appear in both the Income\n",
      "Statement and Cash Flows in a single financial statement, and the tag Cash may\n",
      "appear in both the Balance Sheet and Cash Flows).\n",
      "Figure 5. Fields in the PRE data set\n",
      "Field Name\n",
      "Field Description\n",
      "Field Type (format)\n",
      "Max Size\n",
      "May¬†be NULL\n",
      "Key\n",
      "adsh\n",
      "Accession Number. The\n",
      "  20-character string formed from the 18-digit number assigned by the SEC to\n",
      "  each EDGAR submission.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "report\n",
      "Represents the report grouping. This field corresponds\n",
      "  to the statement (stmt) field, which indicates the type of statement. The\n",
      "  numeric value refers to the ‚ÄúR file‚Äù as posted on the EDGAR Web site.\n",
      "NUMERIC\n",
      "6\n",
      "No\n",
      "*\n",
      "line\n",
      "Represents the tag‚Äôs presentation line order for a\n",
      "  given report. Together with the statement and report field, presentation\n",
      "  location, order and grouping can be derived.\n",
      "NUMERIC\n",
      "6\n",
      "No\n",
      "*\n",
      "stmt\n",
      "The financial statement location to which the value of\n",
      "  the ‚Äúreport field pertains.\n",
      "ALPHANUMERIC (BS = Balance Sheet, IS = Income\n",
      "  Statement, CF = Cash Flow, EQ = Equity, CI = Comprehensive Income, SI =\n",
      "  Schedule of Investments, UN = Unclassifiable Statement).\n",
      "2\n",
      "No\n",
      "inpth\n",
      "Value was presented ‚Äúparenthetically‚Äù instead of in\n",
      "  columns within the financial statements. For example:\n",
      "Receivables (\n",
      "net of\n",
      "  allowance for bad debts of $200 in 2012\n",
      ") $700\n",
      ".\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n",
      "rfile\n",
      "The type of interactive data file rendered on the EDGAR\n",
      "  web site, H = .htm file, X = .xml file.\n",
      "ALPHANUMERIC\n",
      "1\n",
      "No\n",
      "tag\n",
      "The tag chosen by the filer for this line item.\n",
      "ALPHANUMERIC\n",
      "256\n",
      "No\n",
      "version\n",
      "The taxonomy identifier if the tag is a standard tag,\n",
      "  otherwise adsh.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "plabel\n",
      "The text presented on the line item, also known as a ‚Äúpreferred‚Äù\n",
      "  label.\n",
      "ALPHANUMERIC\n",
      "512\n",
      "No\n",
      "negating\n",
      "Flag to indicate whether the plabel is negating.\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "# ƒê·ªçc l·∫°i v·ªõi encoding th√≠ch h·ª£p\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# D√πng BeautifulSoup ƒë·ªÉ ph√¢n t√≠ch\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "# L·∫•y to√†n b·ªô text\n",
    "plain_text = soup.get_text(separator='\\n', strip=True)\n",
    "print(plain_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74000d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Xu·∫•t vƒÉn b·∫£n s·∫°ch ra output.txt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# T√°ch v√† g·ªôp t·ª´ng ƒëo·∫°n vƒÉn b·∫£n, kh√¥ng ng·∫Øt d√≤ng sai\n",
    "text_lines = list(soup.body.stripped_strings)\n",
    "plain_text = \"\\n\".join(text_lines)\n",
    "\n",
    "# Ghi ra file ƒë·ªÉ d·ªÖ xem to√†n b·ªô\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(plain_text)\n",
    "\n",
    "print(\"‚úÖ Xu·∫•t vƒÉn b·∫£n s·∫°ch ra output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28cac8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö M·ª•c l·ª•c (Contents):\n",
      "- 1Overview.. 1\n",
      "- 2Scope. 2\n",
      "- 3Organization. 2\n",
      "- 4File Formats. 3\n",
      "- 5Table Definitions. 3\n",
      "- 5.1SUB (Submissions)3\n",
      "- 5.2TAG (Tags)7\n",
      "- 5.3NUM (Numbers)7\n",
      "- 5.4PRE (Presentation of Statements)8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoduy\\AppData\\Local\\Temp\\ipykernel_18508\\1000130124.py:11: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  contents_header = soup.find(text=\"Contents\")\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# T√¨m ph·∫ßn t·ª≠ ch·ª©a \"Contents\"\n",
    "contents_header = soup.find(text=\"Contents\")\n",
    "if contents_header:\n",
    "    # T√¨m th·∫ª cha ch·ª©a danh s√°ch li√™n k·∫øt \"Contents\"\n",
    "    contents_section = contents_header.find_parent()\n",
    "    \n",
    "    # Ti·∫øp t·ª•c t√¨m ƒë·∫øn danh s√°ch li√™n k·∫øt k·∫ø b√™n (th∆∞·ªùng l√† <ul> ho·∫∑c <p>/<div> ti·∫øp theo)\n",
    "    links = []\n",
    "    for tag in contents_section.find_all_next(['a', 'p'], limit=20):\n",
    "        if tag.name == \"a\" and tag.get(\"href\", \"\").startswith(\"#\"):\n",
    "            links.append(tag.get_text(strip=True))\n",
    "        elif tag.name == \"p\" and \"Figure\" in tag.get_text():\n",
    "            break  # D·ª´ng khi ƒë·∫øn c√°c ph·∫ßn nh∆∞ \"Figure 1...\"\n",
    "    \n",
    "    print(\"üìö M·ª•c l·ª•c (Contents):\")\n",
    "    for item in links:\n",
    "        print(\"-\", item)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y ph·∫ßn 'Contents'\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175d69ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u m·ª•c l·ª•c v√†o contents_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoduy\\AppData\\Local\\Temp\\ipykernel_18508\\1596589823.py:11: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  contents_header = soup.find(text=\"Contents\")\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# T√¨m ph·∫ßn \"Contents\"\n",
    "contents_header = soup.find(text=\"Contents\")\n",
    "contents_list = []\n",
    "\n",
    "if contents_header:\n",
    "    contents_section = contents_header.find_parent()\n",
    "    for tag in contents_section.find_all_next(['a', 'p'], limit=20):\n",
    "        if tag.name == \"a\" and tag.get(\"href\", \"\").startswith(\"#\"):\n",
    "            contents_list.append(tag.get_text(strip=True))\n",
    "        elif tag.name == \"p\" and \"Figure\" in tag.get_text():\n",
    "            break\n",
    "\n",
    "# L∆∞u ra file .txt\n",
    "with open(\"contents_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"üìö M·ª•c l·ª•c (Contents):\\n\")\n",
    "    for item in contents_list:\n",
    "        f.write(f\"- {item}\\n\")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ l∆∞u m·ª•c l·ª•c v√†o contents_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d840b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ xu·∫•t c√°c ph·∫ßn th√†nh c√¥ng v√†o sections_output.txt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# L·∫•y to√†n b·ªô c√°c ph·∫ßn t·ª≠ theo d√≤ng li√™n t·ª•c (kh√¥ng gi·ªõi h·∫°n v√†o heading c·ª• th·ªÉ)\n",
    "elements = list(soup.body.stripped_strings)\n",
    "\n",
    "sections = {}\n",
    "current_title = None\n",
    "current_content = []\n",
    "\n",
    "for line in elements:\n",
    "    # Ki·ªÉm tra n·∫øu l√† ti√™u ƒë·ªÅ b·∫Øt ƒë·∫ßu b·∫±ng s·ªë ho·∫∑c \"Figure\"\n",
    "    if line.strip().startswith((\"1 \", \"2 \", \"3 \", \"4 \", \"5 \", \"Figure\")):\n",
    "        # L∆∞u ph·∫ßn tr∆∞·ªõc ƒë√≥ n·∫øu c√≥\n",
    "        if current_title:\n",
    "            sections[current_title] = \"\\n\".join(current_content)\n",
    "        # C·∫≠p nh·∫≠t ti√™u ƒë·ªÅ m·ªõi\n",
    "        current_title = line.strip()\n",
    "        current_content = []\n",
    "    else:\n",
    "        # N·ªôi dung thu·ªôc v·ªÅ ti√™u ƒë·ªÅ hi·ªán t·∫°i\n",
    "        current_content.append(line.strip())\n",
    "\n",
    "# L∆∞u ph·∫ßn cu·ªëi c√πng\n",
    "if current_title:\n",
    "    sections[current_title] = \"\\n\".join(current_content)\n",
    "\n",
    "# Ghi ra file txt ƒë·ªÉ quan s√°t\n",
    "with open(\"sections_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for title, content in sections.items():\n",
    "        f.write(f\"{title}\\n{'='*len(title)}\\n{content}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ xu·∫•t c√°c ph·∫ßn th√†nh c√¥ng v√†o sections_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1badc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to output.md\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to convert table to Markdown format\n",
    "def table_to_markdown(table):\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr'):\n",
    "        row = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n",
    "        rows.append(row)\n",
    "    if not rows:\n",
    "        return ''\n",
    "    # Create header row\n",
    "    header_row = '| ' + ' | '.join(rows[0]) + ' |'\n",
    "    # Create separator row\n",
    "    separator_row = '| ' + ' | '.join(['---'] * len(rows[0])) + ' |'\n",
    "    # Create body rows\n",
    "    body_rows = ['| ' + ' | '.join(row) + ' |' for row in rows[1:]]\n",
    "    # Combine all\n",
    "    return '\\n'.join([header_row, separator_row] + body_rows)\n",
    "\n",
    "# HTML content (replace this with reading from a file if needed)\n",
    "# html_content = \"\"\"\n",
    "# (Your provided HTML content here - truncated for brevity; in practice, read from file or use full string)\n",
    "# \"\"\"\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Open file to write Markdown output\n",
    "with open('output.md', 'w', encoding='utf-8') as f:\n",
    "    # Find the main content div\n",
    "    main_div = soup.find('div', class_='WordSection1')\n",
    "    if main_div:\n",
    "        for element in main_div.children:\n",
    "            if element.name == 'h1':\n",
    "                f.write('# ' + element.get_text(strip=True) + '\\n\\n')\n",
    "            elif element.name == 'h2':\n",
    "                f.write('## ' + element.get_text(strip=True) + '\\n\\n')\n",
    "            elif element.name == 'p':\n",
    "                classes = element.get('class', [])\n",
    "                if 'MsoListParagraph' in classes:\n",
    "                    f.write('- ' + element.get_text(strip=True) + '\\n')\n",
    "                elif 'MsoTocHeading' in classes:\n",
    "                    f.write('# ' + element.get_text(strip=True) + '\\n\\n')\n",
    "                elif 'MsoToc1' in classes or 'MsoToc2' in classes or 'MsoTof' in classes:\n",
    "                    f.write(element.get_text(strip=True) + '\\n')\n",
    "                elif 'MsoCaption' in classes:\n",
    "                    f.write('**' + element.get_text(strip=True) + '**\\n\\n')\n",
    "                else:\n",
    "                    f.write(element.get_text(strip=True) + '\\n\\n')\n",
    "            elif element.name == 'table':\n",
    "                markdown_table = table_to_markdown(element)\n",
    "                f.write(markdown_table + '\\n\\n')\n",
    "            elif element.name == 'div' and element.get('align') == 'center':\n",
    "                # Handle centered tables within div\n",
    "                table = element.find('table')\n",
    "                if table:\n",
    "                    markdown_table = table_to_markdown(table)\n",
    "                    f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "print(\"Text extracted and saved to output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b3afc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to output.md\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to convert HTML table to Markdown\n",
    "def table_to_markdown(table):\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr'):\n",
    "        row = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n",
    "        rows.append(row)\n",
    "    if not rows:\n",
    "        return ''\n",
    "    header_row = '| ' + ' | '.join(rows[0]) + ' |'\n",
    "    separator_row = '| ' + ' | '.join(['---'] * len(rows[0])) + ' |'\n",
    "    body_rows = ['| ' + ' | '.join(row) + ' |' for row in rows[1:]]\n",
    "    return '\\n'.join([header_row, separator_row] + body_rows)\n",
    "\n",
    "# Function to process text and handle nested elements like links and code\n",
    "def process_text(element):\n",
    "    text = ''\n",
    "    for child in element.children:\n",
    "        if child.name == 'a' and child.get('href'):\n",
    "            text += f\"[{child.get_text(strip=True)}]({child.get('href')})\"\n",
    "        elif child.name in ['code', 'pre']:\n",
    "            text += f\"\\n```\\n{child.get_text(strip=True)}\\n```\\n\"\n",
    "        else:\n",
    "            text += child.get_text(strip=True) if isinstance(child, BeautifulSoup) else str(child).strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Read HTML file\n",
    "file_path = \"readme.html\"\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Open Markdown file for writing\n",
    "with open('output.md', 'w', encoding='utf-8') as f:\n",
    "    # Find main content div\n",
    "    main_div = soup.find('div', class_='WordSection1')\n",
    "    if main_div:\n",
    "        for element in main_div.children:\n",
    "            if element.name == 'h1':\n",
    "                f.write('# ' + element.get_text(strip=True) + '\\n\\n')\n",
    "            elif element.name == 'h2':\n",
    "                # Handle subheadings (e.g., 5.1, 5.2)\n",
    "                heading_text = element.get_text(strip=True)\n",
    "                if heading_text.startswith(('1', '2', '3', '4', '5')) and '.' in heading_text:\n",
    "                    f.write('## ' + heading_text + '\\n\\n')\n",
    "                else:\n",
    "                    f.write('## ' + heading_text + '\\n\\n')\n",
    "            elif element.name == 'p':\n",
    "                classes = element.get('class', [])\n",
    "                text = process_text(element)\n",
    "                if 'MsoListParagraph' in classes or text.startswith('¬∑'):\n",
    "                    # Clean up bullet points\n",
    "                    text = text.lstrip('¬∑').strip()\n",
    "                    f.write('- ' + text + '\\n')\n",
    "                elif 'MsoCaption' in classes:\n",
    "                    f.write('**' + text + '**\\n\\n')\n",
    "                elif 'MsoTocHeading' in classes:\n",
    "                    f.write('# ' + text + '\\n\\n')\n",
    "                elif any(cls in classes for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    f.write(text + '\\n')\n",
    "                else:\n",
    "                    f.write(text + '\\n\\n')\n",
    "            elif element.name == 'table':\n",
    "                markdown_table = table_to_markdown(element)\n",
    "                f.write(markdown_table + '\\n\\n')\n",
    "            elif element.name == 'div' and element.get('align') == 'center':\n",
    "                table = element.find('table')\n",
    "                if table:\n",
    "                    markdown_table = table_to_markdown(table)\n",
    "                    f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "print(\"Text extracted and saved to output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9615df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_table(table):\n",
    "    \"\"\"Format b·∫£ng th√†nh markdown.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    table_md = ['| ' + ' | '.join(headers) + ' |']\n",
    "    table_md.append('| ' + ' | '.join(['---'] * len(headers)) + ' |')\n",
    "    \n",
    "    for row in rows[1:]:\n",
    "        cells = [clean_text(td.get_text()) for td in row.find_all('td')]\n",
    "        table_md.append('| ' + ' | '.join(cells) + ' |')\n",
    "    \n",
    "    return '\\n'.join(table_md) + '\\n'\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh markdown.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "    title = soup.find('p', class_='MsoTocHeading')\n",
    "    if title:\n",
    "        output.append(f\"# {clean_text(title.get_text())}\")\n",
    "    \n",
    "    # X·ª≠ l√Ω m·ª•c l·ª•c\n",
    "    toc_heading = soup.find('p', class_='MsoTocHeading', string=re.compile('Contents'))\n",
    "    if toc_heading:\n",
    "        output.append(f\"## {clean_text(toc_heading.get_text())}\")\n",
    "        for toc_item in soup.find_all(['p'], class_=['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "            level = 2 if toc_item.get('class')[0] == 'MsoToc1' else 3\n",
    "            text = clean_text(toc_item.get_text())\n",
    "            output.append(f\"{'#' * level} {text}\")\n",
    "    \n",
    "    # X·ª≠ l√Ω c√°c heading (h1, h2, ...)\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        level = int(heading.name[1])\n",
    "        text = clean_text(heading.get_text())\n",
    "        output.append(f\"{'#' * (level + 1)} {text}\")\n",
    "    \n",
    "    # X·ª≠ l√Ω ƒëo·∫°n vƒÉn\n",
    "    for para in soup.find_all(['p'], class_=['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "        text = clean_text(para.get_text())\n",
    "        if para.get('class')[0] == 'MsoListParagraph':\n",
    "            output.append(f\"- {text}\")\n",
    "        elif para.get('class')[0] == 'MsoCaption':\n",
    "            output.append(f\"**{text}**\")\n",
    "        else:\n",
    "            output.append(text)\n",
    "    \n",
    "    # X·ª≠ l√Ω b·∫£ng\n",
    "    for table in soup.find_all('table', class_='MsoNormalTable'):\n",
    "        output.append(format_table(table))\n",
    "    \n",
    "    return '\\n\\n'.join([line for line in output if line])\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file markdown\n",
    "    with open('formatted_financial_statement.md', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def format_tag_report_md(data):\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu b·∫£ng TAG th√†nh ƒë·ªãnh d·∫°ng b√°o c√°o t√†i ch√≠nh markdown.\"\"\"\n",
    "    report = [\n",
    "        f\"# TAG Data Structure Report\",\n",
    "        f\"**Generated Date**: {datetime.now().strftime('%Y-%m-%d')}\",\n",
    "        \"\",\n",
    "        \"## 1. Tag Identification\",\n",
    "        \"- **Tag Name** (tag): The unique identifier for a tag in a specific taxonomy release. (Type: ALPHANUMERIC, Size: 256, Required)\",\n",
    "        \"- **Version** (version): Identifies the taxonomy for standard tags or accession number for custom tags. (Type: ALPHANUMERIC, Size: 20, Required)\",\n",
    "        \"\",\n",
    "        \"## 2. Tag Characteristics\",\n",
    "        \"- **Custom Flag** (custom): Indicates if the tag is custom (1) or standard (0). Redundant with version/adsh. (Type: BOOLEAN, Size: 1, Required)\",\n",
    "        \"- **Abstract Flag** (abstract): Set to 1 if the tag does not represent a numeric fact. (Type: BOOLEAN, Size: 1, Required)\",\n",
    "        \"\",\n",
    "        \"## 3. Data Properties\",\n",
    "        \"- **Data Type** (datatype): Specifies the data type (e.g., monetary) if not abstract; NULL if abstract. (Type: ALPHANUMERIC, Size: 20, Optional)\",\n",
    "        \"- **Instant or Duration** (iord): Indicates if the value is point-in-time ('I') or duration ('D'); NULL if abstract. (Type: ALPHANUMERIC, Size: 1, Required)\",\n",
    "        \"- **Credit/Debit** (crdr): Natural accounting balance (C or D) for monetary tags; NULL if undefined. (Type: ALPHANUMERIC, Size: 1, Optional)\",\n",
    "        \"\",\n",
    "        \"## 4. Tag Documentation\",\n",
    "        \"- **Label Text** (tlabel): Label provided by taxonomy (standard) or filer (custom); NULL if absent. (Type: ALPHANUMERIC, Size: 512, Optional)\",\n",
    "        \"- **Detailed Definition** (doc): Detailed definition from taxonomy (standard) or filer (custom); NULL if absent. (Type: ALPHANUMERIC, Optional)\",\n",
    "        \"\",\n",
    "        \"## Summary\",\n",
    "        \"The TAG data structure defines metadata for XBRL tags, including identification, characteristics, data properties, and documentation. Key fields (tag, version) ensure uniqueness, while optional fields (datatype, crdr, tlabel, doc) provide flexibility for varied use cases.\"\n",
    "    ]\n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "def format_tag_report_json(data):\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu b·∫£ng TAG th√†nh ƒë·ªãnh d·∫°ng JSON.\"\"\"\n",
    "    report = {\n",
    "        \"report_title\": \"TAG Data Structure Report\",\n",
    "        \"generated_date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "        \"sections\": [\n",
    "            {\n",
    "                \"title\": \"Tag Identification\",\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"field\": \"tag\",\n",
    "                        \"description\": \"The unique identifier for a tag in a specific taxonomy release.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 256,\n",
    "                        \"required\": True,\n",
    "                        \"key\": True\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"version\",\n",
    "                        \"description\": \"Identifies the taxonomy for standard tags or accession number for custom tags.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 20,\n",
    "                        \"required\": True,\n",
    "                        \"key\": True\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Tag Characteristics\",\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"field\": \"custom\",\n",
    "                        \"description\": \"Indicates if the tag is custom (1) or standard (0). Redundant with version/adsh.\",\n",
    "                        \"type\": \"BOOLEAN\",\n",
    "                        \"max_size\": 1,\n",
    "                        \"required\": True\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"abstract\",\n",
    "                        \"description\": \"Set to 1 if the tag does not represent a numeric fact.\",\n",
    "                        \"type\": \"BOOLEAN\",\n",
    "                        \"max_size\": 1,\n",
    "                        \"required\": True\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Data Properties\",\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"field\": \"datatype\",\n",
    "                        \"description\": \"Specifies the data type (e.g., monetary) if not abstract; NULL if abstract.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 20,\n",
    "                        \"required\": False\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"iord\",\n",
    "                        \"description\": \"Indicates if the value is point-in-time ('I') or duration ('D'); NULL if abstract.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 1,\n",
    "                        \"required\": True\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"crdr\",\n",
    "                        \"description\": \"Natural accounting balance (C or D) for monetary tags; NULL if undefined.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 1,\n",
    "                        \"required\": False\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Tag Documentation\",\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"field\": \"tlabel\",\n",
    "                        \"description\": \"Label provided by taxonomy (standard) or filer (custom); NULL if absent.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 512,\n",
    "                        \"required\": False\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"doc\",\n",
    "                        \"description\": \"Detailed definition from taxonomy (standard) or filer (custom); NULL if absent.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": None,\n",
    "                        \"required\": False\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"summary\": \"The TAG data structure defines metadata for XBRL tags, including identification, characteristics, data properties, and documentation. Key fields (tag, version) ensure uniqueness, while optional fields (datatype, crdr, tlabel, doc) provide flexibility for varied use cases.\"\n",
    "    }\n",
    "    return report\n",
    "\n",
    "def main():\n",
    "    # D·ªØ li·ªáu b·∫£ng TAG (gi·∫£ l·∫≠p t·ª´ input c·ªßa b·∫°n)\n",
    "    tag_data = [\n",
    "        {\"field\": \"tag\", \"description\": \"The unique identifier (name) for a tag in a specific taxonomy release.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 256, \"may_be_null\": \"No\", \"key\": \"*\"},\n",
    "        {\"field\": \"version\", \"description\": \"For a standard tag, an identifier for the taxonomy; otherwise the accession number where the tag was defined.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 20, \"may_be_null\": \"No\", \"key\": \"*\"},\n",
    "        {\"field\": \"custom\", \"description\": \"1 if tag is custom (version=adsh), 0 if it is standard. Note: This flag is technically redundant with the version and adsh columns.\", \"type\": \"BOOLEAN (1 if true and 0 if false)\", \"max_size\": 1, \"may_be_null\": \"No\", \"key\": \"\"},\n",
    "        {\"field\": \"abstract\", \"description\": \"1 if the tag is not used to represent a numeric fact.\", \"type\": \"BOOLEAN (1 if true and 0 if false)\", \"max_size\": 1, \"may_be_null\": \"No\", \"key\": \"\"},\n",
    "        {\"field\": \"datatype\", \"description\": \"If abstract=1, then NULL, otherwise the data type (e.g., monetary) for the tag.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 20, \"may_be_null\": \"Yes\", \"key\": \"\"},\n",
    "        {\"field\": \"iord\", \"description\": \"If abstract=1, then NULL; otherwise, ‚ÄúI‚Äù if the value is a point-in time, or ‚ÄúD‚Äù if the value is a duration.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 1, \"may_be_null\": \"No\", \"key\": \"\"},\n",
    "        {\"field\": \"crdr\", \"description\": \"If datatype = monetary, then the tag‚Äôs natural accounting balance (debit or credit); if not defined, then NULL.\", \"type\": \"ALPHANUMERIC (‚ÄúC‚Äù or ‚ÄúD‚Äù)\", \"max_size\": 1, \"may_be_null\": \"Yes\", \"key\": \"\"},\n",
    "        {\"field\": \"tlabel\", \"description\": \"If a standard tag, then the label text provided by the taxonomy, otherwise the text provided by the filer. A tag which had neither would have a NULL value here.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 512, \"may_be_null\": \"Yes\", \"key\": \"\"},\n",
    "        {\"field\": \"doc\", \"description\": \"The detailed definition for the tag. If a standard tag, then the text provided by the taxonomy, otherwise the text assigned by the filer. Some tags have neither, and this field is NULL.\", \"type\": \"ALPHANUMERIC\", \"max_size\": \"\", \"may_be_null\": \"Yes\", \"key\": \"\"}\n",
    "    ]\n",
    "\n",
    "    # T·∫°o b√°o c√°o markdown\n",
    "    md_report = format_tag_report_md(tag_data)\n",
    "    with open('tag_data_report.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(md_report)\n",
    "\n",
    "    # T·∫°o b√°o c√°o JSON\n",
    "    json_report = format_tag_report_json(tag_data)\n",
    "    with open('tag_data_report.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "448ef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuy·ªÉn m·ªôt h√†ng c·ªßa b·∫£ng th√†nh m·ªôt d√≤ng vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"X·ª≠ l√Ω b·∫£ng HTML v√† chuy·ªÉn th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ c·ªôt t·ª´ h√†ng ƒë·∫ßu ti√™n\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuy·ªÉn c√°c h√†ng d·ªØ li·ªáu th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n\n",
    "    lines = [f\"{table_title}\", \"=\" * 40, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Ch·ªâ x·ª≠ l√Ω h√†ng c√≥ d·ªØ li·ªáu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(line)\n",
    "    lines.extend([\"\", \"=\" * 40])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "    title = soup.find('p', class_='MsoTocHeading')\n",
    "    if title:\n",
    "        output.append(clean_text(title.get_text()))\n",
    "        output.append(\"=\" * 40)\n",
    "        output.append(\"\")\n",
    "    \n",
    "    # X·ª≠ l√Ω c√°c b·∫£ng\n",
    "    for table in soup.find_all('table', class_='MsoNormalTable'):\n",
    "        # T√¨m ti√™u ƒë·ªÅ b·∫£ng t·ª´ th·∫ª caption g·∫ßn nh·∫•t tr∆∞·ªõc b·∫£ng\n",
    "        caption = table.find_previous('p', class_='MsoCaption')\n",
    "        table_title = clean_text(caption.get_text()) if caption else \"Table\"\n",
    "        table_lines = process_table(table, table_title)\n",
    "        if table_lines:  # Ch·ªâ th√™m n·∫øu b·∫£ng c√≥ d·ªØ li·ªáu\n",
    "            output.extend(table_lines)\n",
    "            output.append(\"\")\n",
    "    \n",
    "    return '\\n'.join([line for line in output if line])\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file text\n",
    "    with open('formatted_tables.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8bf86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuy·ªÉn m·ªôt h√†ng c·ªßa b·∫£ng th√†nh m·ªôt d√≤ng vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"X·ª≠ l√Ω b·∫£ng HTML v√† chuy·ªÉn th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ c·ªôt t·ª´ h√†ng ƒë·∫ßu ti√™n\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuy·ªÉn c√°c h√†ng d·ªØ li·ªáu th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n\n",
    "    lines = [f\"{table_title}\", \"=\" * 40, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Ch·ªâ x·ª≠ l√Ω h√†ng c√≥ d·ªØ li·ªáu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(line)\n",
    "    lines.extend([\"\", \"=\" * 40])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "    title = soup.find('p', class_='MsoTocHeading')\n",
    "    if title:\n",
    "        output.append(clean_text(title.get_text()))\n",
    "        output.append(\"=\" * 40)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # X·ª≠ l√Ω m·ª•c l·ª•c\n",
    "    toc_heading = soup.find('p', class_='MsoTocHeading', string=re.compile('Contents'))\n",
    "    if toc_heading:\n",
    "        output.append(clean_text(toc_heading.get_text()))\n",
    "        output.append(\"-\" * 40)\n",
    "        output.append(\"\")\n",
    "        for toc_item in soup.find_all(['p'], class_=['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "            level = 1 if toc_item.get('class')[0] == 'MsoToc1' else 2\n",
    "            text = clean_text(toc_item.get_text())\n",
    "            output.append(f\"{'  ' * (level - 1)}{text}\")\n",
    "        output.append(\"\")\n",
    "\n",
    "    # X·ª≠ l√Ω c√°c heading (h1, h2, ...)\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        level = int(heading.name[1])\n",
    "        text = clean_text(heading.get_text())\n",
    "        output.append(f\"{'-' * (level + 1)} {text}\")\n",
    "        output.append(\"\")\n",
    "\n",
    "    # X·ª≠ l√Ω ƒëo·∫°n vƒÉn v√† danh s√°ch\n",
    "    for para in soup.find_all(['p'], class_=['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "        text = clean_text(para.get_text())\n",
    "        if para.get('class')[0] == 'MsoListParagraph':\n",
    "            output.append(f\"- {text}\")\n",
    "        elif para.get('class')[0] == 'MsoCaption':\n",
    "            output.append(f\"**{text}**\")\n",
    "        else:\n",
    "            output.append(text)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # X·ª≠ l√Ω c√°c b·∫£ng\n",
    "    for table in soup.find_all('table', class_='MsoNormalTable'):\n",
    "        # T√¨m ti√™u ƒë·ªÅ b·∫£ng t·ª´ th·∫ª caption g·∫ßn nh·∫•t tr∆∞·ªõc b·∫£ng\n",
    "        caption = table.find_previous('p', class_='MsoCaption')\n",
    "        table_title = clean_text(caption.get_text()) if caption else \"Table\"\n",
    "        table_lines = process_table(table, table_title)\n",
    "        if table_lines:  # Ch·ªâ th√™m n·∫øu b·∫£ng c√≥ d·ªØ li·ªáu\n",
    "            output.extend(table_lines)\n",
    "            output.append(\"\")\n",
    "\n",
    "    return '\\n'.join([line for line in output if line])\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file text\n",
    "    with open('formatted_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad95e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuy·ªÉn m·ªôt h√†ng c·ªßa b·∫£ng th√†nh m·ªôt d√≤ng vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"X·ª≠ l√Ω b·∫£ng HTML v√† chuy·ªÉn th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ c·ªôt t·ª´ h√†ng ƒë·∫ßu ti√™n\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuy·ªÉn c√°c h√†ng d·ªØ li·ªáu th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Ch·ªâ x·ª≠ l√Ω h√†ng c√≥ d·ªØ li·ªáu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Th·ª•t l·ªÅ ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "    title = soup.find('p', class_='MsoTocHeading')\n",
    "    if title:\n",
    "        output.append(clean_text(title.get_text()).upper())\n",
    "        output.append(\"=\" * 50)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # X·ª≠ l√Ω m·ª•c l·ª•c\n",
    "    toc_heading = soup.find('p', class_='MsoTocHeading', string=re.compile('Contents'))\n",
    "    if toc_heading:\n",
    "        output.append(\"TABLE OF CONTENTS\")\n",
    "        output.append(\"-\" * 50)\n",
    "        output.append(\"\")\n",
    "        for toc_item in soup.find_all(['p'], class_=['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "            level = 1 if toc_item.get('class')[0] == 'MsoToc1' else 2\n",
    "            text = clean_text(toc_item.get_text())\n",
    "            output.append(f\"{'  ' * (level - 1)}{text}\")\n",
    "        output.append(\"\")\n",
    "        output.append(\"-\" * 50)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # X·ª≠ l√Ω c√°c heading (h1, h2, ...)\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        level = int(heading.name[1])\n",
    "        text = clean_text(heading.get_text())\n",
    "        # Th√™m d·∫•u ph√¢n c√°ch theo c·∫•p ƒë·ªô heading\n",
    "        separator = \"-\" * (level + 1)\n",
    "        output.append(f\"{separator} {text.upper()}\")\n",
    "        output.append(\"\")\n",
    "\n",
    "    # X·ª≠ l√Ω ƒëo·∫°n vƒÉn v√† danh s√°ch\n",
    "    for para in soup.find_all(['p'], class_=['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "        text = clean_text(para.get_text())\n",
    "        if not text:  # B·ªè qua ƒëo·∫°n vƒÉn r·ªóng\n",
    "            continue\n",
    "        if para.get('class')[0] == 'MsoListParagraph':\n",
    "            output.append(f\"  - {text}\")  # Th·ª•t l·ªÅ cho danh s√°ch\n",
    "        elif para.get('class')[0] == 'MsoCaption':\n",
    "            output.append(f\"**{text.upper()}**\")  # Caption in ƒë·∫≠m v√† in hoa\n",
    "        else:\n",
    "            output.append(text)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # X·ª≠ l√Ω c√°c b·∫£ng\n",
    "    for table in soup.find_all('table', class_='MsoNormalTable'):\n",
    "        caption = table.find_previous('p', class_='MsoCaption')\n",
    "        table_title = clean_text(caption.get_text()) if caption else \"Table\"\n",
    "        table_lines = process_table(table, table_title.upper())\n",
    "        if table_lines:  # Ch·ªâ th√™m n·∫øu b·∫£ng c√≥ d·ªØ li·ªáu\n",
    "            output.extend(table_lines)\n",
    "            output.append(\"\")\n",
    "\n",
    "    # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng li√™n ti·∫øp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file text\n",
    "    with open('improved_formatted_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5349c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuy·ªÉn m·ªôt h√†ng c·ªßa b·∫£ng th√†nh m·ªôt d√≤ng vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"X·ª≠ l√Ω b·∫£ng HTML v√† chuy·ªÉn th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ c·ªôt t·ª´ h√†ng ƒë·∫ßu ti√™n\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuy·ªÉn c√°c h√†ng d·ªØ li·ªáu th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Ch·ªâ x·ª≠ l√Ω h√†ng c√≥ d·ªØ li·ªáu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Th·ª•t l·ªÅ ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Duy·ªát qua t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ trong HTML theo th·ª© t·ª± xu·∫•t hi·ªán\n",
    "    for element in soup.find('body').children:\n",
    "        # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω m·ª•c l·ª•c\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')):\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # T√¨m c√°c m·ª•c trong m·ª•c l·ª•c\n",
    "            toc_items = []\n",
    "            current = element.next_sibling\n",
    "            while current and current.name == 'p' and any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                text = clean_text(current.get_text())\n",
    "                # ƒêi·ªÅu ch·ªânh th·ª•t l·ªÅ cho c√°c m·ª•c con\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Th·ª•t l·ªÅ 4 kho·∫£ng c√°ch cho m·ª•c con\n",
    "                toc_items.append(text)\n",
    "                current = current.next_sibling\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω ƒëo·∫°n vƒÉn v√† danh s√°ch\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # B·ªè qua ƒëo·∫°n vƒÉn r·ªóng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      ¬∑ {text}\")  # Th·ª•t l·ªÅ 6 kho·∫£ng c√°ch v√† d√πng k√Ω hi·ªáu ¬∑\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                output.append(f\"Figure {text}\")\n",
    "                output.append(\"\")\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω b·∫£ng (gi·ªØ nguy√™n)\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            caption = element.find_previous('p', class_='MsoCaption')\n",
    "            table_title = clean_text(caption.get_text()) if caption else \"Table\"\n",
    "            table_lines = process_table(element, table_title.upper())\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "\n",
    "    # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng li√™n ti·∫øp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file text\n",
    "    with open('improved_formatted_content_v3.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3441bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuy·ªÉn m·ªôt h√†ng c·ªßa b·∫£ng th√†nh m·ªôt d√≤ng vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"X·ª≠ l√Ω b·∫£ng HTML v√† chuy·ªÉn th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ c·ªôt t·ª´ h√†ng ƒë·∫ßu ti√™n\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuy·ªÉn c√°c h√†ng d·ªØ li·ªáu th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Ch·ªâ x·ª≠ l√Ω h√†ng c√≥ d·ªØ li·ªáu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Th·ª•t l·ªÅ ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # L·∫•y t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c·∫ßn x·ª≠ l√Ω theo th·ª© t·ª± xu·∫•t hi·ªán\n",
    "    elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "    toc_processed = False\n",
    "    table_caption = None\n",
    "\n",
    "    for element in elements:\n",
    "        # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω m·ª•c l·ª•c\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')) and not toc_processed:\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # T√¨m c√°c m·ª•c trong m·ª•c l·ª•c\n",
    "            toc_items = []\n",
    "            current = element\n",
    "            while current:\n",
    "                current = current.find_next('p')\n",
    "                if not current or not any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    break\n",
    "                text = clean_text(current.get_text())\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Th·ª•t l·ªÅ 4 kho·∫£ng c√°ch cho m·ª•c con\n",
    "                toc_items.append(text)\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "            toc_processed = True\n",
    "        \n",
    "        # X·ª≠ l√Ω heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω ƒëo·∫°n vƒÉn, danh s√°ch v√† caption\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # B·ªè qua ƒëo·∫°n vƒÉn r·ªóng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      ¬∑ {text}\")  # Th·ª•t l·ªÅ 6 kho·∫£ng c√°ch v√† d√πng k√Ω hi·ªáu ¬∑\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                table_caption = f\"Figure {text}\"  # L∆∞u caption ƒë·ªÉ d√πng cho b·∫£ng ti·∫øp theo\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω b·∫£ng (gi·ªØ nguy√™n)\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            table_title = table_caption if table_caption else \"Table\"\n",
    "            table_lines = process_table(element, table_title.upper())\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "            table_caption = None  # Reset caption sau khi d√πng\n",
    "\n",
    "    # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng li√™n ti·∫øp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file text\n",
    "    with open('fixed_formatted_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27bb944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuy·ªÉn m·ªôt h√†ng c·ªßa b·∫£ng th√†nh m·ªôt d√≤ng vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"X·ª≠ l√Ω b·∫£ng HTML v√† chuy·ªÉn th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ c·ªôt t·ª´ h√†ng ƒë·∫ßu ti√™n\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuy·ªÉn c√°c h√†ng d·ªØ li·ªáu th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Ch·ªâ x·ª≠ l√Ω h√†ng c√≥ d·ªØ li·ªáu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Th·ª•t l·ªÅ ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # L·∫•y t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c·∫ßn x·ª≠ l√Ω theo th·ª© t·ª± xu·∫•t hi·ªán\n",
    "    elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "    toc_processed = False\n",
    "    table_caption = None\n",
    "    skip_until_table = False  # Bi·∫øn ƒë·ªÉ ki·ªÉm so√°t vi·ªác b·ªè qua n·ªôi dung b·∫£ng\n",
    "\n",
    "    for element in elements:\n",
    "        # B·ªè qua n·ªôi dung n·∫øu ƒëang trong tr·∫°ng th√°i b·ªè qua (cho ƒë·∫øn khi g·∫∑p b·∫£ng)\n",
    "        if skip_until_table and element.name != 'table':\n",
    "            continue\n",
    "\n",
    "        # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω m·ª•c l·ª•c\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')) and not toc_processed:\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # T√¨m c√°c m·ª•c trong m·ª•c l·ª•c\n",
    "            toc_items = []\n",
    "            current = element\n",
    "            while current:\n",
    "                current = current.find_next('p')\n",
    "                if not current or not any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    break\n",
    "                text = clean_text(current.get_text())\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Th·ª•t l·ªÅ 4 kho·∫£ng c√°ch cho m·ª•c con\n",
    "                toc_items.append(text)\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "            toc_processed = True\n",
    "        \n",
    "        # X·ª≠ l√Ω heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω ƒëo·∫°n vƒÉn, danh s√°ch v√† caption\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # B·ªè qua ƒëo·∫°n vƒÉn r·ªóng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      ¬∑ {text}\")  # Th·ª•t l·ªÅ 6 kho·∫£ng c√°ch v√† d√πng k√Ω hi·ªáu ¬∑\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                table_caption = f\"Figure {text}\"  # L∆∞u caption ƒë·ªÉ d√πng cho b·∫£ng ti·∫øp theo\n",
    "                skip_until_table = True  # B·ªè qua c√°c ƒëo·∫°n vƒÉn cho ƒë·∫øn khi g·∫∑p b·∫£ng\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω b·∫£ng\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            table_title = table_caption if table_caption else \"Table\"\n",
    "            table_lines = process_table(element, table_title)\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "            table_caption = None  # Reset caption sau khi d√πng\n",
    "            skip_until_table = False  # K·∫øt th√∫c tr·∫°ng th√°i b·ªè qua\n",
    "\n",
    "    # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng li√™n ti·∫øp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file text\n",
    "    with open('fixed_formatted_content_v2.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7518c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuy·ªÉn m·ªôt h√†ng c·ªßa b·∫£ng th√†nh m·ªôt d√≤ng vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"X·ª≠ l√Ω b·∫£ng HTML v√† chuy·ªÉn th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ c·ªôt t·ª´ h√†ng ƒë·∫ßu ti√™n\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuy·ªÉn c√°c h√†ng d·ªØ li·ªáu th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Ch·ªâ x·ª≠ l√Ω h√†ng c√≥ d·ªØ li·ªáu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Th·ª•t l·ªÅ ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines, headers  # Tr·∫£ v·ªÅ ti√™u ƒë·ªÅ c·ªôt ƒë·ªÉ ki·ªÉm tra n·ªôi dung b·∫£ng\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # L·∫•y t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c·∫ßn x·ª≠ l√Ω theo th·ª© t·ª± xu·∫•t hi·ªán\n",
    "    elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "    toc_processed = False\n",
    "    table_caption = None\n",
    "    skip_until_table = False  # Bi·∫øn ƒë·ªÉ ki·ªÉm so√°t vi·ªác b·ªè qua n·ªôi dung tr∆∞·ªõc b·∫£ng\n",
    "    skip_table_content = False  # Bi·∫øn ƒë·ªÉ ki·ªÉm so√°t vi·ªác b·ªè qua n·ªôi dung b·∫£ng\n",
    "    table_headers = []  # L∆∞u tr·ªØ ti√™u ƒë·ªÅ c·ªôt c·ªßa b·∫£ng hi·ªán t·∫°i\n",
    "\n",
    "    for element in elements:\n",
    "        # B·ªè qua n·ªôi dung n·∫øu ƒëang trong tr·∫°ng th√°i b·ªè qua (cho ƒë·∫øn khi g·∫∑p b·∫£ng)\n",
    "        if skip_until_table and element.name != 'table':\n",
    "            continue\n",
    "\n",
    "        # B·ªè qua n·ªôi dung b·∫£ng n·∫øu ƒëang trong tr·∫°ng th√°i b·ªè qua n·ªôi dung b·∫£ng\n",
    "        if skip_table_content and element.name == 'p' and 'MsoNormal' in element.get('class', []):\n",
    "            text = clean_text(element.get_text())\n",
    "            # N·∫øu ƒëo·∫°n vƒÉn kh√¥ng ch·ª©a ti√™u ƒë·ªÅ c·ªôt ho·∫∑c d·ªØ li·ªáu b·∫£ng, d·ª´ng b·ªè qua\n",
    "            if not any(header.lower() in text.lower() for header in table_headers) and not text.startswith(\"Note:\"):\n",
    "                skip_table_content = False\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω m·ª•c l·ª•c\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')) and not toc_processed:\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # T√¨m c√°c m·ª•c trong m·ª•c l·ª•c\n",
    "            toc_items = []\n",
    "            current = element\n",
    "            while current:\n",
    "                current = current.find_next('p')\n",
    "                if not current or not any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    break\n",
    "                text = clean_text(current.get_text())\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Th·ª•t l·ªÅ 4 kho·∫£ng c√°ch cho m·ª•c con\n",
    "                toc_items.append(text)\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "            toc_processed = True\n",
    "        \n",
    "        # X·ª≠ l√Ω heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "            skip_table_content = False  # Reset tr·∫°ng th√°i b·ªè qua n·ªôi dung b·∫£ng khi g·∫∑p heading\n",
    "        \n",
    "        # X·ª≠ l√Ω ƒëo·∫°n vƒÉn, danh s√°ch v√† caption\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # B·ªè qua ƒëo·∫°n vƒÉn r·ªóng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      ¬∑ {text}\")  # Th·ª•t l·ªÅ 6 kho·∫£ng c√°ch v√† d√πng k√Ω hi·ªáu ¬∑\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                table_caption = f\"Figure {text}\"  # L∆∞u caption ƒë·ªÉ d√πng cho b·∫£ng ti·∫øp theo\n",
    "                skip_until_table = True  # B·ªè qua c√°c ƒëo·∫°n vƒÉn cho ƒë·∫øn khi g·∫∑p b·∫£ng\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω b·∫£ng\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            table_title = table_caption if table_caption else \"Table\"\n",
    "            table_lines, headers = process_table(element, table_title)\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "            table_caption = None  # Reset caption sau khi d√πng\n",
    "            skip_until_table = False  # K·∫øt th√∫c tr·∫°ng th√°i b·ªè qua\n",
    "            skip_table_content = True  # B·∫Øt ƒë·∫ßu b·ªè qua n·ªôi dung b·∫£ng\n",
    "            table_headers = headers  # L∆∞u ti√™u ƒë·ªÅ c·ªôt ƒë·ªÉ ki·ªÉm tra n·ªôi dung b·∫£ng\n",
    "\n",
    "    # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng li√™n ti·∫øp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file text\n",
    "    with open('fixed_formatted_content_v3.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51678f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a, d√≤ng tr·ªëng li√™n ti·∫øp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuy·ªÉn m·ªôt h√†ng c·ªßa b·∫£ng th√†nh m·ªôt d√≤ng vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"X·ª≠ l√Ω b·∫£ng HTML v√† chuy·ªÉn th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return [], [], []\n",
    "    \n",
    "    # X√°c ƒë·ªãnh ti√™u ƒë·ªÅ c·ªôt t·ª´ h√†ng ƒë·∫ßu ti√™n\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return [], [], []\n",
    "    \n",
    "    # L∆∞u tr·ªØ d·ªØ li·ªáu h√†ng ƒë·ªÉ ki·ªÉm tra sau n√†y\n",
    "    table_data = []\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:\n",
    "            row_data = [clean_text(cell.get_text()) for cell in cells]\n",
    "            table_data.extend(row_data)\n",
    "    \n",
    "    # Chuy·ªÉn c√°c h√†ng d·ªØ li·ªáu th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Ch·ªâ x·ª≠ l√Ω h√†ng c√≥ d·ªØ li·ªáu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Th·ª•t l·ªÅ ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines, headers, table_data\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Tr√≠ch xu·∫•t v√† format n·ªôi dung HTML th√†nh danh s√°ch c√°c d√≤ng vƒÉn b·∫£n.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # L·∫•y t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c·∫ßn x·ª≠ l√Ω theo th·ª© t·ª± xu·∫•t hi·ªán\n",
    "    elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "    toc_processed = False\n",
    "    table_caption = None\n",
    "    skip_until_table = False  # Bi·∫øn ƒë·ªÉ ki·ªÉm so√°t vi·ªác b·ªè qua n·ªôi dung tr∆∞·ªõc b·∫£ng\n",
    "    skip_table_content = False  # Bi·∫øn ƒë·ªÉ ki·ªÉm so√°t vi·ªác b·ªè qua n·ªôi dung b·∫£ng\n",
    "    table_headers = []  # L∆∞u tr·ªØ ti√™u ƒë·ªÅ c·ªôt c·ªßa b·∫£ng hi·ªán t·∫°i\n",
    "    table_data = []  # L∆∞u tr·ªØ d·ªØ li·ªáu h√†ng c·ªßa b·∫£ng hi·ªán t·∫°i\n",
    "\n",
    "    for element in elements:\n",
    "        # B·ªè qua n·ªôi dung n·∫øu ƒëang trong tr·∫°ng th√°i b·ªè qua (cho ƒë·∫øn khi g·∫∑p b·∫£ng)\n",
    "        if skip_until_table and element.name != 'table':\n",
    "            continue\n",
    "\n",
    "        # B·ªè qua n·ªôi dung b·∫£ng n·∫øu ƒëang trong tr·∫°ng th√°i b·ªè qua n·ªôi dung b·∫£ng\n",
    "        if skip_table_content and element.name == 'p' and 'MsoNormal' in element.get('class', []):\n",
    "            text = clean_text(element.get_text())\n",
    "            # N·∫øu ƒëo·∫°n vƒÉn kh√¥ng ch·ª©a ti√™u ƒë·ªÅ c·ªôt, d·ªØ li·ªáu h√†ng, v√† kh√¥ng n·∫±m trong n·ªôi dung b·∫£ng, d·ª´ng b·ªè qua\n",
    "            if (not any(header.lower() in text.lower() for header in table_headers) and \n",
    "                not any(data.lower() in text.lower() for data in table_data) and \n",
    "                not text.startswith(\"Note:\")):\n",
    "                skip_table_content = False\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # X·ª≠ l√Ω ti√™u ƒë·ªÅ ch√≠nh\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω m·ª•c l·ª•c\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')) and not toc_processed:\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # T√¨m c√°c m·ª•c trong m·ª•c l·ª•c\n",
    "            toc_items = []\n",
    "            current = element\n",
    "            while current:\n",
    "                current = current.find_next('p')\n",
    "                if not current or not any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    break\n",
    "                text = clean_text(current.get_text())\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Th·ª•t l·ªÅ 4 kho·∫£ng c√°ch cho m·ª•c con\n",
    "                toc_items.append(text)\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "            toc_processed = True\n",
    "        \n",
    "        # X·ª≠ l√Ω heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "            skip_table_content = False  # Reset tr·∫°ng th√°i b·ªè qua n·ªôi dung b·∫£ng khi g·∫∑p heading\n",
    "        \n",
    "        # X·ª≠ l√Ω ƒëo·∫°n vƒÉn, danh s√°ch v√† caption\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # B·ªè qua ƒëo·∫°n vƒÉn r·ªóng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      ¬∑ {text}\")  # Th·ª•t l·ªÅ 6 kho·∫£ng c√°ch v√† d√πng k√Ω hi·ªáu ¬∑\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                table_caption = f\"Figure {text}\"  # L∆∞u caption ƒë·ªÉ d√πng cho b·∫£ng ti·∫øp theo\n",
    "                skip_until_table = True  # B·ªè qua c√°c ƒëo·∫°n vƒÉn cho ƒë·∫øn khi g·∫∑p b·∫£ng\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # X·ª≠ l√Ω b·∫£ng\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            table_title = table_caption if table_caption else \"Table\"\n",
    "            table_lines, headers, data = process_table(element, table_title)\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "            table_caption = None  # Reset caption sau khi d√πng\n",
    "            skip_until_table = False  # K·∫øt th√∫c tr·∫°ng th√°i b·ªè qua\n",
    "            skip_table_content = True  # B·∫Øt ƒë·∫ßu b·ªè qua n·ªôi dung b·∫£ng\n",
    "            table_headers = headers  # L∆∞u ti√™u ƒë·ªÅ c·ªôt\n",
    "            table_data = data  # L∆∞u d·ªØ li·ªáu h√†ng\n",
    "\n",
    "    # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng li√™n ti·∫øp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # ƒê·ªçc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t v√† format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # L∆∞u v√†o file text\n",
    "    with open('fixed_formatted_content_v4.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45e9b720",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_split_text\u001b[39m(file_path):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"ƒê·ªçc file vƒÉn b·∫£n v√† chia th√†nh c√°c ƒëo·∫°n nh·ªè.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_and_split_text(file_path):\n",
    "    \"\"\"ƒê·ªçc file vƒÉn b·∫£n v√† chia th√†nh c√°c ƒëo·∫°n nh·ªè.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n d·ª±a tr√™n ti√™u ƒë·ªÅ ho·∫∑c c√°c ƒëo·∫°n vƒÉn l·ªõn\n",
    "        sections = re.split(r'\\n\\s*(?=(?:--|---|Figure)\\s+.*?\\n)', text)\n",
    "        documents = []\n",
    "        for i, section in enumerate(sections):\n",
    "            section = section.strip()\n",
    "            if section:\n",
    "                # T·∫°o Document t·ª´ m·ªói ƒëo·∫°n\n",
    "                doc = Document(\n",
    "                    page_content=section,\n",
    "                    metadata={\n",
    "                        \"section_id\": i,\n",
    "                        \"source\": file_path\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        print(f\"‚úÖ Loaded and split text into {len(documents)} sections.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load and split text: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a41f0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_chroma\n",
      "  Downloading langchain_chroma-0.2.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: langchain-core>=0.3.52 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain_chroma) (0.3.58)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain_chroma) (1.26.4)\n",
      "Collecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain_chroma)\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.8.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading chroma_hnswlib-0.7.6.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi>=0.95.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.11.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading onnxruntime-1.21.1-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading mmh3-5.1.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (13.7.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain-core>=0.3.52->langchain_chroma) (0.3.42)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain-core>=0.3.52->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain-core>=0.3.52->langchain_chroma) (24.1)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.6)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.52->langchain_chroma) (2.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.3.52->langchain_chroma) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.3.52->langchain_chroma) (0.23.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.25.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.13.2)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (7.0.1)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.15.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading watchfiles-1.0.5-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.3.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.8)\n",
      "Downloading langchain_chroma-0.2.3-py3-none-any.whl (11 kB)\n",
      "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "   ---------------------------------------- 0.0/611.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 611.1/611.1 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.6/4.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.6/2.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 7.9 MB/s eta 0:00:00\n",
      "Downloading mmh3-5.1.0-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Downloading onnxruntime-1.21.1-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.3 MB 7.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/12.3 MB 7.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.3/12.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.4/12.3 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n",
      "Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n",
      "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.3-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading watchfiles-1.0.5-cp312-cp312-win_amd64.whl (291 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib, pypika\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=2988c6c7b9b56ceedcaf991ddb0962889c7d911c88a53948f7eae512986b4b91\n",
      "  Stored in directory: c:\\users\\hoduy\\appdata\\local\\pip\\cache\\wheels\\d5\\3d\\69\\8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Failed to build chroma-hnswlib\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Building wheel for chroma-hnswlib (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'hnswlib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for chroma-hnswlib\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (chroma-hnswlib)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849a8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded and split text into 15 sections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoduy\\AppData\\Local\\Temp\\ipykernel_35148\\3786724872.py:47: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ ƒê√£ th√™m 15 t√†i li·ªáu v√†o vector DB.\n",
      "‚úÖ Created vector DB with 15 documents at d:\\hk2_nam3\\langraph\\day4\\kiennguyen\\financial_data_db.\n",
      "‚úÖ Loaded existing Vector DB with 15 documents.\n",
      "‚úÖ Retriever initialized with similarity_score_threshold.\n",
      "---RETRIEVAL FROM VECTOR DB---\n",
      "üìÅ Retrieved 3 documents:\n",
      "Document 1:\n",
      "Figure Figure 1. Data relationships\n",
      "==================================================\n",
      "\n",
      "  Dataset: NUM has Columns referencing other datasets: adsh has Referenced dataset: SUB has Referenced columns: ...\n",
      "Metadata: {'section_id': 4, 'source': 'fixed_formatted_content_v4.txt'}\n",
      "\n",
      "Document 2:\n",
      "--- 5.3 NUM (Numbers)\n",
      "\n",
      "The NUM data set contains numeric data, one row per data point as rendered by the Commission on the primary financial statements. The source for the table is the ‚Äúas filed‚Äù XBRL...\n",
      "Metadata: {'source': 'fixed_formatted_content_v4.txt', 'section_id': 11}\n",
      "\n",
      "Document 3:\n",
      "-- 3 Organization\n",
      "\n",
      "Note that this data set represents quarterly and annual uncorrected and ‚Äúas filed‚Äù EDGAR document submissions containing multiple reporting periods (including amendments of prior su...\n",
      "Metadata: {'source': 'fixed_formatted_content_v4.txt', 'section_id': 3}\n",
      "\n",
      "‚úÖ Retrieved 3 documents.\n",
      "Documents retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "# from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "# T·∫£i API key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng\n",
    "# load_dotenv()\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh nh√∫ng\n",
    "openai_embed_model = OpenAIEmbeddings(\n",
    "    model='text-embedding-3-small',\n",
    "    api_key=\"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\"\n",
    ")\n",
    "\n",
    "def load_and_split_text(file_path):\n",
    "    \"\"\"ƒê·ªçc file vƒÉn b·∫£n v√† chia th√†nh c√°c ƒëo·∫°n nh·ªè.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n d·ª±a tr√™n ti√™u ƒë·ªÅ ho·∫∑c c√°c ƒëo·∫°n vƒÉn l·ªõn\n",
    "        sections = re.split(r'\\n\\s*(?=(?:--|---|Figure)\\s+.*?\\n)', text)\n",
    "        documents = []\n",
    "        for i, section in enumerate(sections):\n",
    "            section = section.strip()\n",
    "            if section:\n",
    "                # T·∫°o Document t·ª´ m·ªói ƒëo·∫°n\n",
    "                doc = Document(\n",
    "                    page_content=section,\n",
    "                    metadata={\n",
    "                        \"section_id\": i,\n",
    "                        \"source\": file_path\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        print(f\"‚úÖ Loaded and split text into {len(documents)} sections.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load and split text: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_vector_store(documents, persist_directory):\n",
    "    \"\"\"T·∫°o v√† l∆∞u tr·ªØ vector database t·ª´ c√°c t√†i li·ªáu.\"\"\"\n",
    "    try:\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        # X√≥a d·ªØ li·ªáu c≈© n·∫øu c√≥\n",
    "        chroma_db.delete_collection()\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        # Th√™m t√†i li·ªáu v√†o vector database\n",
    "        chroma_db.add_documents(documents)\n",
    "        doc_count = chroma_db._collection.count()\n",
    "        print(f\"üìÑ ƒê√£ th√™m {len(documents)} t√†i li·ªáu v√†o vector DB.\")\n",
    "        print(f\"‚úÖ Created vector DB with {doc_count} documents at {persist_directory}.\")\n",
    "        return chroma_db\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create vector DB: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_vector_store_and_retriever(persist_directory):\n",
    "    \"\"\"T·∫£i vector database v√† kh·ªüi t·∫°o retriever.\"\"\"\n",
    "    try:\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        doc_count = chroma_db._collection.count()\n",
    "        if doc_count > 0:\n",
    "            print(f\"‚úÖ Loaded existing Vector DB with {doc_count} documents.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No documents found in the vector DB.\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            similarity_threshold_retriever = chroma_db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\",\n",
    "                search_kwargs={\"k\": 3, \"score_threshold\": 0.3}\n",
    "            )\n",
    "            print(\"‚úÖ Retriever initialized with similarity_score_threshold.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to init threshold-based retriever: {e}\")\n",
    "            similarity_threshold_retriever = chroma_db.as_retriever(\n",
    "                search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "            )\n",
    "            print(\"üîÅ Fallback to regular similarity retriever.\")\n",
    "        return chroma_db, similarity_threshold_retriever\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load Chroma DB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def retrieve(question, chroma_db, retriever):\n",
    "    \"\"\"Truy xu·∫•t t√†i li·ªáu t·ª´ vector database d·ª±a tr√™n c√¢u h·ªèi.\"\"\"\n",
    "    print(\"---RETRIEVAL FROM VECTOR DB---\")\n",
    "    documents = []\n",
    "    if retriever:\n",
    "        try:\n",
    "            documents = retriever.invoke(question)\n",
    "            if len(documents) == 0:\n",
    "                print(\"‚ö†Ô∏è No relevant documents retrieved.\")\n",
    "            else:\n",
    "                print(f\"üìÅ Retrieved {len(documents)} documents:\")\n",
    "                for i, doc in enumerate(documents):\n",
    "                    print(f\"Document {i+1}:\")\n",
    "                    print(doc.page_content[:200] + \"...\")  # Hi·ªÉn th·ªã 200 k√Ω t·ª± ƒë·∫ßu ti√™n\n",
    "                    print(f\"Metadata: {doc.metadata}\")\n",
    "                    print(\"\")\n",
    "                print(f\"‚úÖ Retrieved {len(documents)} documents.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå No retriever available.\")\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def main():\n",
    "    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file vƒÉn b·∫£n ƒë√£ x·ª≠ l√Ω\n",
    "    file_path = \"fixed_formatted_content_v4.txt\"\n",
    "    # ƒê∆∞·ªùng d·∫´n ƒë·ªÉ l∆∞u vector database\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "    persist_directory = os.path.abspath(os.path.join(BASE_DIR, \"financial_data_db\"))\n",
    "\n",
    "    # B∆∞·ªõc 1: ƒê·ªçc v√† chia nh·ªè vƒÉn b·∫£n\n",
    "    documents = load_and_split_text(file_path)\n",
    "    if not documents:\n",
    "        print(\"‚ùå No documents to process. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # B∆∞·ªõc 2: T·∫°o vector database\n",
    "    chroma_db = create_vector_store(documents, persist_directory)\n",
    "    if chroma_db is None:\n",
    "        print(\"‚ùå Failed to create vector store. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # B∆∞·ªõc 3: T·∫£i vector database v√† kh·ªüi t·∫°o retriever\n",
    "    chroma_db, retriever = get_vector_store_and_retriever(persist_directory)\n",
    "    if chroma_db is None or retriever is None:\n",
    "        print(\"‚ùå Failed to initialize vector store or retriever. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # B∆∞·ªõc 4: Th·ª≠ truy xu·∫•t v·ªõi m·ªôt c√¢u h·ªèi\n",
    "    question = \"What is the relationship between the data sets NUM and SUB?\"\n",
    "    result = retrieve(question, chroma_db, retriever)\n",
    "    # in cau tra l∆°i \n",
    "    if result[\"documents\"]:\n",
    "        print(\"Documents retrieved successfully.\")\n",
    "    else:\n",
    "        print(\"No documents retrieved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad283954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: Financial Statement Data Sets\n",
      "\n",
      "Contents\n",
      "docs: 1 Overview.. 1\n",
      "docs: 2 Scope. 2\n",
      "docs: 3 Organization. 2\n",
      "docs: 4 File Formats. 3\n",
      "docs: 5 Table Definitions. 3\n",
      "    5.1 SUB (Submissions) 3\n",
      "    5.2 TAG (Tags) 7\n",
      "    5.3 NUM (Numbers) 7\n",
      "    5.4 PRE (Presentation of Statements) 8\n",
      "docs: -- 1 Overview\n",
      "\n",
      "The following data sets provide information extracted from XBRL submissions filed with the Commission in a flattened data format to assist users in more easily consuming the data for analysis. The data is sourced from selected information found in the XBRL tagged financial statements submitted by filers to the Commission. These data sets currently include quarterly and annual numeric data rendered by the Commission in the primary financial statements submitted by filers. Certain additional fields (e.g. Standard Industrial Classification (SIC)) used in the Commission‚Äôs EDGAR system are also included to help in supporting the use of the data. The information has been taken directly from submissions created by each registrant, and the data is ‚Äúas filed‚Äù by the registrant. The information will be updated quarterly. Data contained in documents filed after the last business day of the quarter will be included in the next quarterly posting.\n",
      "\n",
      "DISCLAIMER: The Financial Statement Data Sets contain information derived from structured data filed with the Commission by individual registrants as well as Commission-generated filing identifiers. Because the data sets are derived from information provided by individual registrants, we cannot guarantee the accuracy of the data sets. In addition, it is possible inaccuracies or other errors were introduced into the data sets during the process of extracting the data and compiling the data sets. Finally, the data sets do not reflect all available information, including certain metadata associated with Commission filings. The data sets are intended to assist the public in analyzing data contained in Commission filings; however, they are not a substitute for such filings. Investors should review the full Commission filings before making any investment decision.\n",
      "\n",
      "The data extracted from the XBRL submissions is organized into four data sets containing information about submissions, numbers, taxonomy tags, and presentation. Each data set consists of rows and columns and is provided as a tab-delimited TXT format file. The data sets are as follows:\n",
      "\n",
      "      ¬∑ ¬∑ SUB ‚Äì Submission data set; this includes one record for each XBRL submission with amounts rendered by the Commission in the primary financial statements. The set includes fields of information pertinent to the submission and the filing entity. Information is extracted from the SEC‚Äôs EDGAR system and the filings submitted to the SEC by registrants.\n",
      "\n",
      "      ¬∑ ¬∑ NUM ‚Äì Number data set; this includes one row for each distinct amount appearing on the primary financial statements rendered by the Commission from each submission included in the SUB data set.\n",
      "\n",
      "      ¬∑ ¬∑ TAG ‚Äì Tag data set; includes defining information about each numerical tag. Information includes tag descriptions (documentation labels), taxonomy version information and other tag attributes.\n",
      "\n",
      "      ¬∑ ¬∑ PRE ‚Äì Presentation data set; this provides information about how the tags and numbers were presented in the primary financial statements as rendered by the Commission.\n",
      "docs: -- 2 Scope\n",
      "\n",
      "The scope of the data in the financial statement data sets consists of:\n",
      "\n",
      "      ¬∑ ¬∑ Numeric data on the primary financial statements as rendered by the Commission (Balance Sheet, Income Statement, Cash Flows, Changes in Equity, and Comprehensive Income) and page footnotes on those statements;\n",
      "\n",
      "      ¬∑ ¬∑ From XBRL submissions which include financial statements rendered by the Commission (e.g., 10-K, 10-Q, 20-F, 40-F);\n",
      "\n",
      "      ¬∑ ¬∑ Submitted from 4/15/2009 through the ‚ÄúData Cutoff Date‚Äù inclusive (there is a file named 2009q1.zip on the SEC website that contains data sets with column headings only and no rows, merely so that all years prior to this year will consist of four zip files).\n",
      "\n",
      "All numeric data is ‚Äúas filed.‚Äù\n",
      "docs: -- 3 Organization\n",
      "\n",
      "Note that this data set represents quarterly and annual uncorrected and ‚Äúas filed‚Äù EDGAR document submissions containing multiple reporting periods (including amendments of prior submissions). Data in this submitted form may contain redundancies, inconsistencies, and discrepancies relative to other publication formats. There are four data sets.\n",
      "\n",
      "      ¬∑ 1. SUB identifies all the EDGAR submissions with amounts rendered by the Commission on the primary financial statements in the data set, with each row having the unique (primary) key adsh, a 20 character EDGAR Accession Number with dashes in positions 11 and 14.\n",
      "\n",
      "      ¬∑ 2. TAG is a data set of all numerical tags used in the submissions, both standard and custom. A unique key of each row is a combination of these fields:\n",
      "\n",
      "      ¬∑ 1) tag ‚Äì tag used by the filer\n",
      "\n",
      "      ¬∑ 2) version ‚Äì if a standard tag, the taxonomy of origin, otherwise equal to adsh.\n",
      "\n",
      "      ¬∑ 3. NUM is a data set of all numeric XBRL facts presented on the primary financial statements as rendered by the Comission. A unique key of each row is a combination of the following fields:\n",
      "\n",
      "      ¬∑ 1) adsh- EDGAR accession number\n",
      "\n",
      "      ¬∑ 2) tag ‚Äì tag used by the filer\n",
      "\n",
      "      ¬∑ 3) version ‚Äì if a standard tag, the taxonomy of origin, otherwise equal to adsh.\n",
      "\n",
      "      ¬∑ 4) ddate - period end date\n",
      "\n",
      "      ¬∑ 5) qtrs - duration in number of quarters\n",
      "\n",
      "      ¬∑ 6) uom - unit of measure\n",
      "\n",
      "      ¬∑ 7) segments ‚Äì XBRL tags used to represent axis and member reporting\n",
      "\n",
      "      ¬∑ 8) coreg - coregistrant of the parent company registrant (if applicable)\n",
      "\n",
      "      ¬∑ 4. PRE is a data set that provides the text assigned by the filer to each line item in the primary financial statements, the order in which the line item appeared, and the tag assigned to it. A unique key of each row is a combination of the following fields:\n",
      "\n",
      "      ¬∑ 1) adsh ‚Äì EDGAR accession number\n",
      "\n",
      "      ¬∑ 2) report ‚Äì sequential number of report within the statements\n",
      "\n",
      "      ¬∑ 3) line ‚Äì sequential number of line within a report.\n",
      "\n",
      "The relationship of the data sets is as shown in Figure 1. The Accession Number (adsh) found in the NUM data set can be used to retrieve information about the submission in SUB. Each row of data in NUM was tagged by the filer using a tag. Information about the tag used can be found in TAG. Each row of data in NUM appears on one or more lines of reports detailed in PRE.\n",
      "docs: Figure Figure 1. Data relationships\n",
      "==================================================\n",
      "\n",
      "  Dataset: NUM has Columns referencing other datasets: adsh has Referenced dataset: SUB has Referenced columns: adsh\n",
      "  Dataset: tag, version has Columns referencing other datasets: TAG has Referenced dataset: tag, version\n",
      "  Dataset: PRE has Columns referencing other datasets: adsh has Referenced dataset: SUB has Referenced columns: adsh\n",
      "  Dataset: tag, version has Columns referencing other datasets: TAG has Referenced dataset: tag, version\n",
      "  Dataset: adsh, tag, version has Columns referencing other datasets: NUM has Referenced dataset: adsh, tag, version\n",
      "\n",
      "==================================================\n",
      "\n",
      "Note: The SEC website folder http://www.sec.gov/Archives/edgar/data/{cik}/{accession}/ will always contain all the files for a given submission, where {accession} is the adsh with the ‚Äò-‚Äòcharacters removed.\n",
      "docs: -- 4 File Formats\n",
      "\n",
      "Each of the four data sets is provided in a single encoding, as follows:\n",
      "\n",
      "Tab Delimited Value (.txt): utf-8, tab-delimited, \\n- terminated lines, with the first line containing the column names in lowercase.\n",
      "docs: -- 5 Table Definitions\n",
      "\n",
      "The columns in the figures below (figures 2 ‚Äì 5) provide the following information: field name, description, source (SUB file only), data format, maximum field size, an indication of whether or not the field may be NULL (yes or no), and key.\n",
      "\n",
      "The Source column in the SUB file has two possible values:\n",
      "\n",
      "      ¬∑ ¬∑ EDGAR indicates that the source of the data is the filer‚Äôs EDGAR submission header.\n",
      "\n",
      "      ¬∑ ¬∑ XBRL indicates that the source of the data is the filer‚Äôs XBRL submission.\n",
      "\n",
      "The Key column indicates whether the field is part of a unique index on the data. There are two possible values for this column:\n",
      "\n",
      "      ¬∑ ¬∑ ‚Äú*‚Äù ‚Äì Indicates the field is part of a unique key for the row.\n",
      "\n",
      "      ¬∑ ¬∑ Empty (nothing in column) ‚Äì the column is a function of all or some of a unique key.\n",
      "docs: --- 5.1 SUB (Submissions)\n",
      "\n",
      "The submissions data set contains summary information about an entire EDGAR submission. Some fields were sourced directly from EDGAR submission information, while other columns of data were sourced from the XBRL submission. Note: EDGAR derived fields represent the most recent EDGAR assignment as of a given filing‚Äôs submission date and do not necessarily represent the most current assignments.\n",
      "docs: Figure Figure 2. Fields in the SUB data set\n",
      "==================================================\n",
      "\n",
      "  Field Name: adsh has Field Description: Accession Number. The 20-character string formed from the 18-digit number assigned by the SEC to each EDGAR submission. has Source: EDGAR has Format: ALPHANUMERIC (nnnnnnnnnn-nn-nnnnnn) has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: cik has Field Description: Central Index Key (CIK). Ten digit number assigned by the SEC to each registrant that submits filings. has Source: EDGAR has Format: NUMERIC has Max Size: 10 has May be NULL: No has Key: \n",
      "  Field Name: name has Field Description: Name of registrant. This corresponds to the name of the legal entity as recorded in EDGAR as of the filing date. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 150 has May be NULL: No has Key: \n",
      "  Field Name: sic has Field Description: Standard Industrial Classification (SIC). Four digit code assigned by the SEC as of the filing date, indicating the registrant‚Äôs type of business. has Source: EDGAR has Format: NUMERIC has Max Size: 4 has May be NULL: Yes has Key: \n",
      "  Field Name: countryba has Field Description: The ISO 3166-1 country of the registrant's business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: stprba has Field Description: The state or province of the registrant‚Äôs business address, if field countryba is US or CA. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: cityba has Field Description: The city of the registrant's business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 30 has May be NULL: Yes has Key: \n",
      "  Field Name: zipba has Field Description: The zip code of the registrant‚Äôs business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 10 has May be NULL: Yes has Key: \n",
      "  Field Name: bas1 has Field Description: The first line of the street of the registrant‚Äôs business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 40 has May be NULL: Yes has Key: \n",
      "  Field Name: bas2 has Field Description: The second line of the street of the registrant‚Äôs business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 40 has May be NULL: Yes has Key: \n",
      "  Field Name: baph has Field Description: The phone number of the registrant‚Äôs business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 20 has May be NULL: Yes has Key: \n",
      "  Field Name: countryma has Field Description: The ISO 3166-1 country of the registrant's mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: stprma has Field Description: The state or province of the registrant‚Äôs mailing address, if field countryma is US or CA. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: cityma has Field Description: The city of the registrant's mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 30 has May be NULL: Yes has Key: \n",
      "  Field Name: zipma has Field Description: The zip code of the registrant‚Äôs mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 10 has May be NULL: Yes has Key: \n",
      "  Field Name: mas1 has Field Description: The first line of the street of the registrant‚Äôs mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 40 has May be NULL: Yes has Key: \n",
      "  Field Name: mas2 has Field Description: The second line of the street of the registrant‚Äôs mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 40 has May be NULL: Yes has Key: \n",
      "  Field Name: countryinc has Field Description: The ISO 3166-1 country of incorporation for the registrant. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 3 has May be NULL: Yes has Key: \n",
      "  Field Name: stprinc has Field Description: The state or province of incorporation for the registrant, if countryinc is US or CA. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: ein has Field Description: Employee Identification Number, 9 digit identification number assigned by the Internal Revenue Service to business entities operating in the United States. has Source: EDGAR has Format: NUMERIC has Max Size: 10 has May be NULL: Yes has Key: \n",
      "  Field Name: former has Field Description: Most recent former name of the registrant, if any. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 150 has May be NULL: Yes has Key: \n",
      "  Field Name: changed has Field Description: Date of change from the former name, if any. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 8 has May be NULL: Yes has Key: \n",
      "  Field Name: afs has Field Description: Filer status with the SEC at the time of submission: 1-LAF=Large Accelerated, 2-ACC=Accelerated, 3-SRA=Smaller Reporting Accelerated, 4-NON=Non-Accelerated, 5-SML=Smaller Reporting Filer, NULL=not assigned. has Source: XBRL has Format: ALPHANUMERIC has Max Size: 5 has May be NULL: Yes has Key: \n",
      "  Field Name: wksi has Field Description: Well Known Seasoned Issuer (WKSI). An issuer that meets specific SEC requirements at some point during a 60-day period preceding the date the issuer satisfies its obligation to update its shelf registration statement. has Source: XBRL has Format: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: fye has Field Description: Fiscal Year End Date, rounded to nearest month-end. has Source: XBRL has Format: ALPHANUMERIC (mmdd) has Max Size: 4 has May be NULL: Yes has Key: \n",
      "  Field Name: form has Field Description: The submission type of the registrant‚Äôs filing. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 10 has May be NULL: No has Key: \n",
      "  Field Name: period has Field Description: Balance Sheet Date, rounded to nearest month-end. has Source: XBRL has Format: DATE (yyyymmdd) has Max Size: 8 has May be NULL: No has Key: \n",
      "  Field Name: fy has Field Description: Fiscal Year Focus (as defined in the EDGAR XBRL Guide Ch. 3.1.8). has Source: XBRL has Format: YEAR (yyyy) has Max Size: 4 has May be NULL: Yes has Key: \n",
      "  Field Name: fp has Field Description: Fiscal Period Focus (as defined in the EDGAR XBRL Guide Ch. 3.1.8) within Fiscal Year. has Source: XBRL has Format: ALPHANUMERIC (FY, Q1, Q2, Q3, Q4) has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: filed has Field Description: The date of the registrant‚Äôs filing with the Commission. has Source: EDGAR has Format: DATE (yyyymmdd) has Max Size: 8 has May be NULL: No has Key: \n",
      "  Field Name: accepted has Field Description: The acceptance date and time of the registrant‚Äôs filing with the Commission. has Source: EDGAR has Format: DATETIME (yyyy‚Äëmm‚Äëdd hh:mm:ss) has Max Size: 19 has May be NULL: No has Key: \n",
      "  Field Name: prevrpt has Field Description: Previous Report ‚ÄìTRUE indicates that the submission information was subsequently amended. has Source: EDGAR has Format: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: detail has Field Description: TRUE indicates that the XBRL submission contains quantitative disclosures within the footnotes and schedules at the required detail level (e.g., each amount). has Source: XBRL has Format: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: instance has Field Description: The name of the submitted XBRL Instance Document. The name often begins with the company ticker symbol. has Source: EDGAR has Format: ALPHANUMERIC (e.g. abcd‚Äëyyyymmdd.xml) has Max Size: 40 has May be NULL: No has Key: \n",
      "  Field Name: nciks has Field Description: Number of Central Index Keys (CIK) of registrants (i.e., business units) included in the consolidating entity‚Äôs submitted filing. has Source: EDGAR has Format: NUMERIC has Max Size: 4 has May be NULL: No has Key: \n",
      "  Field Name: aciks has Field Description: Additional CIKs of co-registrants included in a consolidating entity‚Äôs EDGAR submission, separated by spaces. If there are no other co-registrants (i.e., nciks=1), the value of aciks is NULL. For a very small number of filers, the entire list of co-registrants is too long to fit in the field. Where this is the case, users should refer to the complete submission file for all CIK information. has Source: EDGAR has Format: ALPHANUMERIC (space delimited) has Max Size: 120 has May be NULL: Yes has Key: \n",
      "\n",
      "==================================================\n",
      "docs: --- 5.2 TAG (Tags)\n",
      "\n",
      "The TAG data set contains the standard taxonomy tags and the custom taxonomy tags defined in the submissions. The source is the ‚Äúas filed‚Äù XBRL filer submissions. The standard tags are derived from taxonomies in https://www.sec.gov/data-research/standard-taxonomies.\n",
      "docs: Figure Figure 3. Fields in the TAG data set\n",
      "==================================================\n",
      "\n",
      "  Field Name: tag has Field Description: The unique identifier (name) for a tag in a specific taxonomy release. has Field Type: ALPHANUMERIC has Max Size: 256 has May be NULL: No has Key: *\n",
      "  Field Name: version has Field Description: For a standard tag, an identifier for the taxonomy; otherwise the accession number where the tag was defined. has Field Type: ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: custom has Field Description: 1 if tag is custom (version=adsh), 0 if it is standard. Note: This flag is technically redundant with the version and adsh columns. has Field Type: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: abstract has Field Description: 1 if the tag is not used to represent a numeric fact. has Field Type: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: datatype has Field Description: If abstract=1, then NULL, otherwise the data type (e.g., monetary) for the tag. has Field Type: ALPHANUMERIC has Max Size: 20 has May be NULL: Yes has Key: \n",
      "  Field Name: iord has Field Description: If abstract=1, then NULL; otherwise, ‚ÄúI‚Äù if the value is a point-in time, or ‚ÄúD‚Äù if the value is a duration. has Field Type: ALPHANUMERIC has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: crdr has Field Description: If datatype = monetary, then the tag‚Äôs natural accounting balance (debit or credit); if not defined, then NULL. has Field Type: ALPHANUMERIC (‚ÄúC‚Äù or ‚ÄúD‚Äù) has Max Size: 1 has May be NULL: Yes has Key: \n",
      "  Field Name: tlabel has Field Description: If a standard tag, then the label text provided by the taxonomy, otherwise the text provided by the filer. A tag which had neither would have a NULL value here. has Field Type: ALPHANUMERIC has Max Size: 512 has May be NULL: Yes has Key: \n",
      "  Field Name: doc has Field Description: The detailed definition for the tag. If a standard tag, then the text provided by the taxonomy, otherwise the text assigned by the filer. Some tags have neither, and this field is NULL. has Field Type: ALPHANUMERIC has Max Size:  has May be NULL: Yes has Key: \n",
      "\n",
      "==================================================\n",
      "docs: --- 5.3 NUM (Numbers)\n",
      "\n",
      "The NUM data set contains numeric data, one row per data point as rendered by the Commission on the primary financial statements. The source for the table is the ‚Äúas filed‚Äù XBRL filer submissions.\n",
      "docs: Figure Figure 4. Fields in the NUM data set\n",
      "==================================================\n",
      "\n",
      "  Field Name: adsh has Field Description: Accession Number. The 20-character string formed from the 18-digit number assigned by the SEC to each EDGAR submission. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: tag has Field Description: The unique identifier (name) for a tag in a specific taxonomy release. has Field Type (format): ALPHANUMERIC has Max Size: 256 has May be NULL: No has Key: *\n",
      "  Field Name: version has Field Description: For a standard tag, an identifier for the taxonomy; otherwise the accession number where the tag was defined. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: ddate has Field Description: The end date for the data value, rounded to the nearest month end. has Field Type (format): DATE (yyyymmdd) has Max Size: 8 has May be NULL: No has Key: *\n",
      "  Field Name: qtrs has Field Description: The count of the number of quarters represented by the data value, rounded to the nearest whole number. ‚Äú0‚Äù indicates it is a point-in-time value. has Field Type (format): NUMERIC has Max Size: 8 has May be NULL: No has Key: *\n",
      "  Field Name: uom has Field Description: The unit of measure for the value. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: segments has Field Description: Tags used to represent axis and member reporting. has Field Type (format): ALPHANUMERIC has Max Size: 1024 has May be NULL: Yes has Key: *\n",
      "  Field Name: coreg has Field Description: If specified, indicates a specific co-registrant, the parent company, or other entity (e.g., guarantor). NULL indicates the consolidated entity. has Field Type (format): ALPHANUMERIC has Max Size: 256 has May be NULL: Yes has Key: *\n",
      "  Field Name: value has Field Description: The value. This is not scaled, it is as found in the Interactive Data file, but is limited to four digits to the right of the decimal point. has Field Type (format): NUMERIC(28,4) has Max Size: 16 has May be NULL: Yes has Key: \n",
      "  Field Name: footnote has Field Description: The text of any superscripted footnotes on the value, as shown on the statement page, truncated to 512 characters, or if there is no footnote, then this field will be blank. has Field Type (format): ALPHANUMERIC has Max Size: 512 has May be NULL: Yes has Key: \n",
      "\n",
      "==================================================\n",
      "docs: --- 5.4 PRE (Presentation of Statements)\n",
      "\n",
      "The PRE data set contains one row for each line of the financial statements tagged by the filer. The source for the data set is the ‚Äúas filed‚Äù XBRL filer submissions. Note that there may be more than one row per entry in NUM because the same tag can appear in more than one statement (the tag NetIncome, for example can appear in both the Income Statement and Cash Flows in a single financial statement, and the tag Cash may appear in both the Balance Sheet and Cash Flows).\n",
      "docs: Figure Figure 5. Fields in the PRE data set\n",
      "==================================================\n",
      "\n",
      "  Field Name: adsh has Field Description: Accession Number. The 20-character string formed from the 18-digit number assigned by the SEC to each EDGAR submission. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: report has Field Description: Represents the report grouping. This field corresponds to the statement (stmt) field, which indicates the type of statement. The numeric value refers to the ‚ÄúR file‚Äù as posted on the EDGAR Web site. has Field Type (format): NUMERIC has Max Size: 6 has May be NULL: No has Key: *\n",
      "  Field Name: line has Field Description: Represents the tag‚Äôs presentation line order for a given report. Together with the statement and report field, presentation location, order and grouping can be derived. has Field Type (format): NUMERIC has Max Size: 6 has May be NULL: No has Key: *\n",
      "  Field Name: stmt has Field Description: The financial statement location to which the value of the ‚Äúreport field pertains. has Field Type (format): ALPHANUMERIC (BS = Balance Sheet, IS = Income Statement, CF = Cash Flow, EQ = Equity, CI = Comprehensive Income, SI = Schedule of Investments, UN = Unclassifiable Statement). has Max Size: 2 has May be NULL: No has Key: \n",
      "  Field Name: inpth has Field Description: Value was presented ‚Äúparenthetically‚Äù instead of in columns within the financial statements. For example: Receivables (net of allowance for bad debts of $200 in 2012) $700. has Field Type (format): BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: rfile has Field Description: The type of interactive data file rendered on the EDGAR web site, H = .htm file, X = .xml file. has Field Type (format): ALPHANUMERIC has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: tag has Field Description: The tag chosen by the filer for this line item. has Field Type (format): ALPHANUMERIC has Max Size: 256 has May be NULL: No has Key: \n",
      "  Field Name: version has Field Description: The taxonomy identifier if the tag is a standard tag, otherwise adsh. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: \n",
      "  Field Name: plabel has Field Description: The text presented on the line item, also known as a ‚Äúpreferred‚Äù label. has Field Type (format): ALPHANUMERIC has Max Size: 512 has May be NULL: No has Key: \n",
      "  Field Name: negating has Field Description: Flag to indicate whether the plabel is negating. has Field Type (format): BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "\n",
      "==================================================\n",
      "‚úÖ Loaded and split text into 20 sections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoduy\\AppData\\Local\\Temp\\ipykernel_3720\\1026261183.py:48: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ ƒê√£ th√™m 20 t√†i li·ªáu v√†o vector DB.\n",
      "‚úÖ Created vector DB with 20 documents at d:\\hk2_nam3\\langraph\\day4\\kiennguyen\\financial_data_db.\n",
      "‚úÖ Loaded existing Vector DB with 20 documents.\n",
      "‚úÖ Retriever initialized with similarity_score_threshold.\n",
      "---RETRIEVAL FROM VECTOR DB---\n",
      "üìÅ Retrieved 3 documents:\n",
      "Document 1:\n",
      "Figure Figure 1. Data relationships\n",
      "==================================================\n",
      "\n",
      "  Dataset: NUM has Columns referencing other datasets: adsh has Referenced dataset: SUB has Referenced columns: ...\n",
      "Metadata: {'source': 'fixed_formatted_content_v4.txt', 'section_id': 9}\n",
      "\n",
      "Document 2:\n",
      "--- 5.3 NUM (Numbers)\n",
      "\n",
      "The NUM data set contains numeric data, one row per data point as rendered by the Commission on the primary financial statements. The source for the table is the ‚Äúas filed‚Äù XBRL...\n",
      "Metadata: {'source': 'fixed_formatted_content_v4.txt', 'section_id': 16}\n",
      "\n",
      "Document 3:\n",
      "-- 3 Organization\n",
      "\n",
      "Note that this data set represents quarterly and annual uncorrected and ‚Äúas filed‚Äù EDGAR document submissions containing multiple reporting periods (including amendments of prior su...\n",
      "Metadata: {'section_id': 8, 'source': 'fixed_formatted_content_v4.txt'}\n",
      "\n",
      "\n",
      "‚úÖ Tr·∫£ l·ªùi t·ªïng h·ª£p:\n",
      "The NUM data set references the SUB data set through the 'adsh' column. The Accession Number (adsh) found in the NUM data set can be used to retrieve information about the submission in SUB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "OPENAI_API_KEY=\"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\"\n",
    "\n",
    "\n",
    "# C·∫•u h√¨nh API key OpenAI\n",
    "openai_embed_model = OpenAIEmbeddings(\n",
    "    model='text-embedding-3-small',\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "def load_and_split_text(file_path):\n",
    "    \"\"\"ƒê·ªçc file vƒÉn b·∫£n v√† chia th√†nh c√°c ƒëo·∫°n nh·ªè.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n theo Figure, Section\n",
    "        sections = re.split(r'\\n\\s*(?=(?:--|---|Figure|\\d+)\\s+.*?\\n)', text)\n",
    "        documents = []\n",
    "        for i, section in enumerate(sections):\n",
    "            section = section.strip()\n",
    "            print(\"docs:\",section)\n",
    "            if section:\n",
    "                doc = Document(\n",
    "                    page_content=section,\n",
    "                    metadata={\n",
    "                        \"section_id\": i,\n",
    "                        \"source\": file_path\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        print(f\"‚úÖ Loaded and split text into {len(documents)} sections.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load and split text: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_vector_store(documents, persist_directory):\n",
    "    \"\"\"T·∫°o v√† l∆∞u tr·ªØ vector database t·ª´ c√°c t√†i li·ªáu.\"\"\"\n",
    "    try:\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        chroma_db.delete_collection()\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        chroma_db.add_documents(documents)\n",
    "        doc_count = chroma_db._collection.count()\n",
    "        print(f\"üìÑ ƒê√£ th√™m {len(documents)} t√†i li·ªáu v√†o vector DB.\")\n",
    "        print(f\"‚úÖ Created vector DB with {doc_count} documents at {persist_directory}.\")\n",
    "        return chroma_db\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create vector DB: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_vector_store_and_retriever(persist_directory):\n",
    "    \"\"\"T·∫£i vector database v√† kh·ªüi t·∫°o retriever.\"\"\"\n",
    "    try:\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        doc_count = chroma_db._collection.count()\n",
    "        if doc_count > 0:\n",
    "            print(f\"‚úÖ Loaded existing Vector DB with {doc_count} documents.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No documents found in the vector DB.\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            retriever = chroma_db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\",\n",
    "                search_kwargs={\"k\": 3, \"score_threshold\": 0.3}\n",
    "            )\n",
    "            print(\"‚úÖ Retriever initialized with similarity_score_threshold.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Fallback: {e}\")\n",
    "            retriever = chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "            print(\"üîÅ Fallback to regular similarity retriever.\")\n",
    "        return chroma_db, retriever\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load Chroma DB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def retrieve_and_answer(question, chroma_db, retriever):\n",
    "    \"\"\"Truy xu·∫•t t√†i li·ªáu v√† t·∫°o c√¢u tr·∫£ l·ªùi t·ªïng h·ª£p.\"\"\"\n",
    "    print(\"---RETRIEVAL FROM VECTOR DB---\")\n",
    "    if not retriever:\n",
    "        print(\"‚ùå No retriever available.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        documents = retriever.invoke(question)\n",
    "        if not documents:\n",
    "            print(\"‚ö†Ô∏è No relevant documents retrieved.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìÅ Retrieved {len(documents)} documents:\")\n",
    "        for i, doc in enumerate(documents):\n",
    "            print(f\"Document {i+1}:\")\n",
    "            print(doc.page_content[:200] + \"...\")\n",
    "            print(f\"Metadata: {doc.metadata}\\n\")\n",
    "\n",
    "        # G·ªôp n·ªôi dung ƒë·ªÉ t·∫°o prompt\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # LLM ƒë·ªÉ tr·∫£ l·ªùi\n",
    "        llm = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "Tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a tr√™n n·ªôi dung t√†i li·ªáu b√™n d∆∞·ªõi:\n",
    "\n",
    "C√¢u h·ªèi: {question}\n",
    "\n",
    "T√†i li·ªáu:\n",
    "{context}\n",
    "\n",
    "Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng v√† ch√≠nh x√°c:\n",
    "\"\"\")\n",
    "        chain = prompt | llm\n",
    "        answer = chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "        print(\"\\n‚úÖ Tr·∫£ l·ªùi t·ªïng h·ª£p:\")\n",
    "        print(answer.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during retrieval or answer generation: {e}\")\n",
    "\n",
    "def main():\n",
    "    file_path = \"fixed_formatted_content_v4.txt\"\n",
    "    BASE_DIR = os.getcwd()\n",
    "    persist_directory = os.path.abspath(os.path.join(BASE_DIR, \"financial_data_db\"))\n",
    "\n",
    "    documents = load_and_split_text(file_path)\n",
    "    if not documents:\n",
    "        return\n",
    "\n",
    "    chroma_db = create_vector_store(documents, persist_directory)\n",
    "    if chroma_db is None:\n",
    "        return\n",
    "\n",
    "    chroma_db, retriever = get_vector_store_and_retriever(persist_directory)\n",
    "    if chroma_db is None or retriever is None:\n",
    "        return\n",
    "\n",
    "    question = \"What is the relationship between the data sets NUM and SUB?\"\n",
    "    retrieve_and_answer(question, chroma_db, retriever)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "746f7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Vector DB contains: 20 documents\n",
      "\n",
      "‚úÖ Tr·∫£ l·ªùi:\n",
      "T√™n tr∆∞·ªùng ch·ª©a m√£ zip c·ªßa ƒë·ªãa ch·ªâ kinh doanh c·ªßa ng∆∞·ªùi ƒëƒÉng k√Ω l√† \"zipba\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "# Embedding model\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model='text-embedding-3-small',\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "def get_retriever(persist_directory):\n",
    "    chroma_db = Chroma(\n",
    "        collection_name='financial_data_db',\n",
    "        embedding_function=embedding_model,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    print(\"üìÑ Vector DB contains:\", chroma_db._collection.count(), \"documents\")\n",
    "    \n",
    "    try:\n",
    "        retriever = chroma_db.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\"k\": 5, \"score_threshold\": 0.1}\n",
    "        )\n",
    "    except:\n",
    "        retriever = chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def ask_question(question, retriever):\n",
    "    documents = retriever.invoke(question)\n",
    "    if not documents:\n",
    "        print(\"‚ö†Ô∏è No relevant documents found.\")\n",
    "        return\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a tr√™n n·ªôi dung t√†i li·ªáu b√™n d∆∞·ªõi:\n",
    "\n",
    "C√¢u h·ªèi: {question}\n",
    "\n",
    "T√†i li·ªáu:\n",
    "{context}\n",
    "\n",
    "Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng v√† ch√≠nh x√°c:\n",
    "\"\"\")\n",
    "    chain = prompt | llm\n",
    "    answer = chain.invoke({\"question\": question, \"context\": context})\n",
    "    print(\"\\n‚úÖ Tr·∫£ l·ªùi:\")\n",
    "    print(answer.content)\n",
    "\n",
    "def main():\n",
    "    persist_dir = os.path.abspath(os.path.join(os.getcwd(), \"financial_data_db\"))\n",
    "    retriever = get_retriever(persist_dir)\n",
    "    \n",
    "    # üëâ Nh·∫≠p c√¢u h·ªèi t·∫°i ƒë√¢y\n",
    "    # question = \"How many records does the sub include per XBRL submission?\"\n",
    "    question = \"The zip code of the registrant's business address l√† g√¨\"\n",
    "    ask_question(question, retriever)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b833c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
