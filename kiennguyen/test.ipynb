{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e015c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Statement Data Sets\n",
      "Financial Statement Data\n",
      "Sets\n",
      "Contents\n",
      "1\n",
      "Overview\n",
      ".. 1\n",
      "2\n",
      "Scope\n",
      ". 2\n",
      "3\n",
      "Organization\n",
      ". 2\n",
      "4\n",
      "File Formats\n",
      ". 3\n",
      "5\n",
      "Table Definitions\n",
      ". 3\n",
      "5.1\n",
      "SUB (Submissions)\n",
      "3\n",
      "5.2\n",
      "TAG (Tags)\n",
      "7\n",
      "5.3\n",
      "NUM (Numbers)\n",
      "7\n",
      "5.4\n",
      "PRE (Presentation of Statements)\n",
      "8\n",
      "Figure 1. Data relationships\n",
      ". 3\n",
      "Figure 2. Fields in the SUB data set\n",
      "4\n",
      "Figure 3. Fields in the TAG data set\n",
      "7\n",
      "Figure 4. Fields in the NUM data set\n",
      "8\n",
      "Figure 5. Fields in the PRE data set\n",
      "8\n",
      "1\n",
      "Overview\n",
      "The following data sets provide information extracted from XBRL\n",
      "submissions filed with the Commission in a flattened data format to assist\n",
      "users in more easily consuming the data for analysis. The data is sourced from\n",
      "selected information found in the XBRL tagged financial statements submitted by\n",
      "filers to the Commission.  These data sets currently include quarterly and\n",
      "annual numeric data rendered by the Commission in the primary financial\n",
      "statements submitted by filers. Certain additional fields (e.g. Standard\n",
      "Industrial Classification (SIC)) used in the Commission’s EDGAR system are also\n",
      "included to help in supporting the use of the data.  The information has\n",
      "been taken directly from submissions created by each registrant, and the data\n",
      "is “as filed” by the registrant.  The information will be updated\n",
      "quarterly. Data contained in documents filed after the last business day of the\n",
      "quarter will be included in the next quarterly posting.\n",
      "DISCLAIMER: The Financial Statement Data Sets contain\n",
      "information derived from structured data filed with the Commission by\n",
      "individual registrants as well as Commission-generated filing identifiers.\n",
      "Because the data sets are derived from information provided by individual\n",
      "registrants, we cannot guarantee the accuracy of the data sets. In addition, it\n",
      "is possible inaccuracies or other errors were introduced into the data sets\n",
      "during the process of extracting the data and compiling the data sets. Finally,\n",
      "the data sets do not reflect all available information, including certain\n",
      "metadata associated with Commission filings. The data sets are intended to\n",
      "assist the public in analyzing data contained in Commission filings; however,\n",
      "they are not a substitute for such filings. Investors should review the full\n",
      "Commission filings before making any investment decision.\n",
      "The data extracted from the XBRL submissions is organized\n",
      "into four data sets containing information about submissions, numbers, taxonomy\n",
      "tags, and presentation.  Each data set consists of rows and columns and is\n",
      "provided as a tab-delimited TXT format file.  The data sets are as\n",
      "follows:\n",
      "·\n",
      "SUB\n",
      "– Submission data set; this includes one record for each XBRL\n",
      "submission with amounts rendered by the Commission in the primary financial\n",
      "statements. The set includes fields of information pertinent to the submission\n",
      "and the filing entity. Information is extracted from the SEC’s EDGAR system and\n",
      "the filings submitted to the SEC by registrants.\n",
      "·\n",
      "NUM\n",
      "– Number data set; this includes one row for each distinct\n",
      "amount appearing on the primary financial statements rendered by the Commission\n",
      "from each submission included in the SUB data set.\n",
      "·\n",
      "TAG\n",
      "– Tag data set; includes defining information about each numerical\n",
      "tag.  Information includes tag descriptions (documentation labels),\n",
      "taxonomy version information and other tag attributes.\n",
      "·\n",
      "PRE\n",
      "– Presentation data set; this provides information about how\n",
      "the tags and numbers were presented in the primary financial statements as\n",
      "rendered by the Commission.\n",
      "2\n",
      "Scope\n",
      "The scope of the data in the financial statement data sets\n",
      "consists of:\n",
      "·\n",
      "Numeric data on the primary financial statements as rendered by the\n",
      "Commission (Balance Sheet, Income Statement, Cash Flows, Changes in Equity, and\n",
      "Comprehensive Income) and page footnotes on those statements;\n",
      "·\n",
      "From XBRL  submissions which include financial statements rendered by\n",
      "the Commission  (e.g., 10-K, 10-Q, 20-F, 40-F);\n",
      "·\n",
      "Submitted from 4/15/2009 through the “Data Cutoff Date” inclusive (there\n",
      "is a file named 2009q1.zip on the SEC website that contains data sets with\n",
      "column headings only and no rows, merely so that all years prior to this year\n",
      "will consist of four zip files).\n",
      "All numeric data is “as filed.”\n",
      "3\n",
      "Organization\n",
      "Note that this data set represents quarterly and annual\n",
      "uncorrected and “as filed” EDGAR document submissions containing multiple\n",
      "reporting periods (including amendments of prior submissions). Data in this\n",
      "submitted form may contain redundancies, inconsistencies, and discrepancies\n",
      "relative to other publication formats. There are four data sets.\n",
      "1.\n",
      "SUB\n",
      "identifies all the EDGAR submissions with amounts rendered by the Commission on\n",
      "the primary financial statements in the data set, with each row having the\n",
      "unique (primary) key\n",
      "adsh,\n",
      "a 20 character EDGAR Accession Number with\n",
      "dashes in positions 11 and 14.\n",
      "2.\n",
      "TAG\n",
      "is a data set of all numerical tags used in the submissions, both standard and\n",
      "custom.  A unique key of each row is a combination of these fields:\n",
      "1)\n",
      "tag –\n",
      "tag used by the filer\n",
      "2)\n",
      "version\n",
      "– if a standard tag, the taxonomy of origin, otherwise\n",
      "equal to adsh.\n",
      "3.\n",
      "NUM\n",
      "is a data set of all numeric XBRL facts presented on the primary financial\n",
      "statements as rendered by the Comission. A unique key of each row is a\n",
      "combination of the following fields:\n",
      "1)\n",
      "adsh\n",
      "- EDGAR accession number\n",
      "2)\n",
      "tag –\n",
      "tag used by the filer\n",
      "3)\n",
      "version\n",
      "– if a standard tag, the taxonomy of origin, otherwise\n",
      "equal to adsh.\n",
      "4)\n",
      "ddate\n",
      "- period end date\n",
      "5)\n",
      "qtrs\n",
      "- duration in number of quarters\n",
      "6)\n",
      "uom -\n",
      "unit of measure\n",
      "7)\n",
      "segments\n",
      "– XBRL tags used to represent axis and member reporting\n",
      "8)\n",
      "coreg -\n",
      "coregistrant of the parent company registrant (if\n",
      "applicable)\n",
      "4.\n",
      "PRE\n",
      "is a data set that provides the text assigned by the filer to each line item in\n",
      "the primary financial statements, the order in which the line item appeared,\n",
      "and the tag assigned to it.  A unique key of each row is a combination of\n",
      "the following fields:\n",
      "1)\n",
      "adsh –\n",
      "EDGAR accession number\n",
      "2)\n",
      "report\n",
      "– sequential number of report within the statements\n",
      "3)\n",
      "line\n",
      "– sequential number of line within a report.\n",
      "The relationship of the data sets is as shown in Figure 1.\n",
      "The Accession Number\n",
      "(adsh)\n",
      "found in the NUM data set can be used to\n",
      "retrieve information about the submission in SUB.  Each row of data in NUM\n",
      "was tagged by the filer using a tag. Information about the tag used can be\n",
      "found in  TAG.  Each row of data in NUM appears on one or more lines\n",
      "of reports detailed in PRE.\n",
      "Figure 1. Data relationships\n",
      "Dataset\n",
      "Columns referencing other datasets\n",
      "Referenced dataset\n",
      "Referenced columns\n",
      "NUM\n",
      "adsh\n",
      "SUB\n",
      "adsh\n",
      "tag, version\n",
      "TAG\n",
      "tag, version\n",
      "PRE\n",
      "adsh\n",
      "SUB\n",
      "adsh\n",
      "tag, version\n",
      "TAG\n",
      "tag, version\n",
      "adsh, tag, version\n",
      "NUM\n",
      "adsh, tag, version\n",
      "Note: The SEC website folder http://www.sec.gov/Archives/edgar/data/{\n",
      "cik\n",
      "}/{\n",
      "accession\n",
      "}/\n",
      "will always contain all the files for a given submission, where {\n",
      "accession\n",
      "}\n",
      "is the\n",
      "adsh\n",
      "with the ‘-‘characters removed.\n",
      "4\n",
      "File Formats\n",
      "Each of the four data sets is\n",
      "provided in a single encoding, as follows:\n",
      "Tab Delimited Value (.txt): utf-8,\n",
      "tab-delimited, \\n- terminated lines, with the first line containing the column\n",
      "names in lowercase.\n",
      "5\n",
      "Table Definitions\n",
      "The columns in the figures below (figures 2 – 5) provide the\n",
      "following information: field name, description, source (SUB file only), data\n",
      "format, maximum field size, an indication of whether or not the field may be\n",
      "NULL (yes or no), and key.\n",
      "The Source column in the SUB file has two possible values:\n",
      "·\n",
      "EDGAR indicates that the source of the data is the filer’s EDGAR\n",
      "submission header.\n",
      "·\n",
      "XBRL indicates that the source of the data is the filer’s XBRL\n",
      "submission.\n",
      "The Key column indicates whether the field is part of a\n",
      "unique index on the data.  There are two possible values for this column:\n",
      "·\n",
      "“*”\n",
      "– Indicates\n",
      "the field is part of a unique key for the row.\n",
      "·\n",
      "Empty (nothing in column) – the column is a function of all or some of a\n",
      "unique key.\n",
      "5.1\n",
      "SUB\n",
      "(Submissions)\n",
      "The submissions data set contains summary information about\n",
      "an entire EDGAR submission. Some fields were sourced directly from EDGAR\n",
      "submission information, while other columns of data were sourced from the XBRL\n",
      "submission. Note: EDGAR derived fields represent the most recent EDGAR\n",
      "assignment as of a given filing’s submission date and do not necessarily\n",
      "represent the most current assignments.\n",
      "Figure\n",
      "2. Fields in the SUB data set\n",
      "Field Name\n",
      "Field Description\n",
      "Source\n",
      "Format\n",
      "Max Size\n",
      "May be NULL\n",
      "Key\n",
      "adsh\n",
      "Accession Number. The 20-character string formed from\n",
      "  the 18-digit number assigned by the SEC to each EDGAR submission.\n",
      "EDGAR\n",
      "ALPHANUMERIC (nnnnnnnnnn-nn-nnnnnn)\n",
      "20\n",
      "No\n",
      "*\n",
      "cik\n",
      "Central Index Key (CIK). Ten digit number assigned by\n",
      "  the SEC to each registrant that submits filings.\n",
      "EDGAR\n",
      "NUMERIC\n",
      "10\n",
      "No\n",
      "name\n",
      "Name of registrant. This corresponds to the name of the\n",
      "  legal entity as recorded in EDGAR as of the filing date.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "150\n",
      "No\n",
      "sic\n",
      "Standard Industrial Classification (SIC). Four digit\n",
      "  code assigned by the SEC as of the filing date, indicating the registrant’s\n",
      "  type of business.\n",
      "EDGAR\n",
      "NUMERIC\n",
      "4\n",
      "Yes\n",
      "countryba\n",
      "The ISO 3166-1 country of the registrant's business\n",
      "  address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "stprba\n",
      "The state or province of the registrant’s business\n",
      "  address, if field countryba is US or CA.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "cityba\n",
      "The city of the registrant's business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "30\n",
      "Yes\n",
      "zipba\n",
      "The zip code of the registrant’s business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "10\n",
      "Yes\n",
      "bas1\n",
      "The first line of the street of the registrant’s\n",
      "  business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "40\n",
      "Yes\n",
      "bas2\n",
      "The second line of the street of the registrant’s\n",
      "  business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "40\n",
      "Yes\n",
      "baph\n",
      "The phone number of the registrant’s business address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "20\n",
      "Yes\n",
      "countryma\n",
      "The ISO 3166-1 country of the registrant's mailing\n",
      "  address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "stprma\n",
      "The state or province of the registrant’s mailing\n",
      "  address, if field countryma is US or CA.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "cityma\n",
      "The city of the registrant's mailing address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "30\n",
      "Yes\n",
      "zipma\n",
      "The zip code of the registrant’s mailing address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "10\n",
      "Yes\n",
      "mas1\n",
      "The first line of the street of the registrant’s\n",
      "  mailing address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "40\n",
      "Yes\n",
      "mas2\n",
      "The second line of the street of the registrant’s\n",
      "  mailing address.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "40\n",
      "Yes\n",
      "countryinc\n",
      "The ISO 3166-1 country of incorporation for the\n",
      "  registrant.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "3\n",
      "Yes\n",
      "stprinc\n",
      "The state or province of incorporation for the\n",
      "  registrant, if countryinc is US or CA.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "2\n",
      "Yes\n",
      "ein\n",
      "Employee Identification Number, 9 digit identification\n",
      "  number assigned by the Internal Revenue Service to business entities\n",
      "  operating in the United States.\n",
      "EDGAR\n",
      "NUMERIC\n",
      "10\n",
      "Yes\n",
      "former\n",
      "Most recent former name of the registrant, if any.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "150\n",
      "Yes\n",
      "changed\n",
      "Date of change from the former name, if any.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "8\n",
      "Yes\n",
      "afs\n",
      "Filer status with the SEC at the time of submission:\n",
      "1-LAF=Large Accelerated,\n",
      "2-ACC=Accelerated,\n",
      "3-SRA=Smaller Reporting Accelerated,\n",
      "4-NON=Non-Accelerated,\n",
      "5-SML=Smaller Reporting Filer,\n",
      "NULL=not assigned.\n",
      "XBRL\n",
      "ALPHANUMERIC\n",
      "5\n",
      "Yes\n",
      "wksi\n",
      "Well Known Seasoned Issuer (WKSI). An issuer that meets\n",
      "  specific SEC requirements at some point during a 60-day period preceding the\n",
      "  date the issuer satisfies its obligation to update its shelf registration\n",
      "  statement.\n",
      "XBRL\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n",
      "fye\n",
      "Fiscal Year End Date, rounded to nearest month-end.\n",
      "XBRL\n",
      "ALPHANUMERIC (mmdd)\n",
      "4\n",
      "Yes\n",
      "form\n",
      "The submission type of the registrant’s filing.\n",
      "EDGAR\n",
      "ALPHANUMERIC\n",
      "10\n",
      "No\n",
      "period\n",
      "Balance Sheet Date, rounded to nearest month-end.\n",
      "XBRL\n",
      "DATE (yyyymmdd)\n",
      "8\n",
      "No\n",
      "fy\n",
      "Fiscal Year Focus (as defined in the EDGAR XBRL Guide Ch.\n",
      "  3.1.8).\n",
      "XBRL\n",
      "YEAR (yyyy)\n",
      "4\n",
      "Yes\n",
      "fp\n",
      "Fiscal Period Focus (as defined in the EDGAR XBRL Guide\n",
      "  Ch. 3.1.8) within Fiscal Year.\n",
      "XBRL\n",
      "ALPHANUMERIC (FY, Q1, Q2, Q3, Q4)\n",
      "2\n",
      "Yes\n",
      "filed\n",
      "The date of the registrant’s filing with the\n",
      "  Commission.\n",
      "EDGAR\n",
      "DATE (yyyymmdd)\n",
      "8\n",
      "No\n",
      "accepted\n",
      "The acceptance date and time of the registrant’s filing\n",
      "  with the Commission.\n",
      "EDGAR\n",
      "DATETIME (yyyy‑mm‑dd hh:mm:ss)\n",
      "19\n",
      "No\n",
      "prevrpt\n",
      "Previous Report –TRUE indicates that the submission\n",
      "  information was subsequently amended.\n",
      "EDGAR\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n",
      "detail\n",
      "TRUE indicates that the XBRL submission contains\n",
      "  quantitative disclosures within the footnotes and schedules at the required\n",
      "  detail level (e.g., each amount).\n",
      "XBRL\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n",
      "instance\n",
      "The name of the submitted XBRL Instance Document. The\n",
      "  name often begins with the company ticker symbol.\n",
      "EDGAR\n",
      "ALPHANUMERIC (e.g. abcd‑yyyymmdd.xml)\n",
      "40\n",
      "No\n",
      "nciks\n",
      "Number of Central Index Keys (CIK) of registrants\n",
      "  (i.e., business units) included in the consolidating entity’s submitted\n",
      "  filing.\n",
      "EDGAR\n",
      "NUMERIC\n",
      "4\n",
      "No\n",
      "aciks\n",
      "Additional CIKs of co-registrants included in a\n",
      "  consolidating entity’s EDGAR submission, separated by spaces. If there are no\n",
      "  other co-registrants (i.e., nciks=1), the value of aciks is NULL.  For a\n",
      "  very small number of filers, the entire list of co-registrants is too long to\n",
      "  fit in the field.  Where this is the case, users should refer to the\n",
      "  complete submission file for all CIK information.\n",
      "EDGAR\n",
      "ALPHANUMERIC (space delimited)\n",
      "120\n",
      "Yes\n",
      "Note: To access the complete submission files for a given\n",
      "filing, please see the\n",
      "SEC EDGAR website\n",
      ". \n",
      "The SEC website folder http://www.sec.gov/Archives/edgar/data/{\n",
      "cik\n",
      "}/{\n",
      "accession\n",
      "}/\n",
      "will always contain all the files for a given submission.  To assemble the\n",
      "folder address to any filing referenced in the SUB data set, simply substitute\n",
      "{\n",
      "cik\n",
      "} with the\n",
      "cik\n",
      "field and replace {\n",
      "accession\n",
      "} with the\n",
      "adsh\n",
      "field (after removing the dash character).  The following sample SQL Query\n",
      "provides an example of how to generate a list of addresses for filings\n",
      "contained in the SUB data set:\n",
      "·\n",
      "select name,form,period,\n",
      "'http://www.sec.gov/Archives/edgar/data/' + ltrim(str(\n",
      "cik\n",
      ",10))+'/' +\n",
      "replace(\n",
      "adsh\n",
      ",'-','')+'/'+\n",
      "instance\n",
      "as url from SUB sub order by\n",
      "period desc, name\n",
      "5.2\n",
      "TAG (Tags)\n",
      "The TAG data set contains the standard taxonomy tags and the\n",
      "custom taxonomy tags defined in the submissions.  The source is the “as\n",
      "filed” XBRL filer submissions.  The standard tags are derived from\n",
      "taxonomies in\n",
      "https://www.sec.gov/data-research/standard-taxonomies\n",
      ".\n",
      "Figure\n",
      "3. Fields in the TAG data set\n",
      "Field Name\n",
      "Field Description\n",
      "Field Type\n",
      "Max Size\n",
      "May be NULL\n",
      "Key\n",
      "tag\n",
      "The unique identifier (name) for a tag in a specific\n",
      "  taxonomy release.\n",
      "ALPHANUMERIC\n",
      "256\n",
      "No\n",
      "*\n",
      "version\n",
      "For a standard tag, an identifier for the taxonomy;\n",
      "  otherwise the accession number where the tag was defined.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "custom\n",
      "1 if tag is custom (version=adsh), 0 if it is standard.\n",
      "Note: This flag is technically redundant with the  version and adsh\n",
      "  columns.\n",
      "BOOLEAN (1 if true and 0 if false)\n",
      "1\n",
      "No\n",
      "abstract\n",
      "1 if the tag is not used to represent a numeric fact.\n",
      "BOOLEAN (1 if true and 0 if false)\n",
      "1\n",
      "No\n",
      "datatype\n",
      "If abstract=1, then NULL, otherwise the data type\n",
      "  (e.g., monetary) for the tag.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "Yes\n",
      "iord\n",
      "If abstract=1, then NULL; otherwise, “I” if the value\n",
      "  is a point-in time, or “D” if the value is a duration.\n",
      "ALPHANUMERIC\n",
      "1\n",
      "No\n",
      "crdr\n",
      "If datatype = monetary, then the tag’s natural\n",
      "  accounting balance (debit or credit); if not defined, then NULL.\n",
      "ALPHANUMERIC (“C” or “D”)\n",
      "1\n",
      "Yes\n",
      "tlabel\n",
      "If a standard tag, then the label text provided by the\n",
      "  taxonomy, otherwise the text provided by the filer.  A tag which had\n",
      "  neither would have a NULL value here.\n",
      "ALPHANUMERIC\n",
      "512\n",
      "Yes\n",
      "doc\n",
      "The detailed definition for the tag. If a standard tag,\n",
      "  then the text provided by the taxonomy, otherwise the text assigned by the\n",
      "  filer.  Some tags have neither, and this field is NULL.\n",
      "ALPHANUMERIC\n",
      "Yes\n",
      "5.3\n",
      "NUM (Numbers)\n",
      "The NUM data set contains numeric data, one row per data\n",
      "point as rendered by the Commission on the primary financial statements. The\n",
      "source for the table is the “as filed” XBRL filer submissions.\n",
      "Figure 4. Fields in the NUM data set\n",
      "Field Name\n",
      "Field Description\n",
      "Field Type (format)\n",
      "Max Size\n",
      "May be NULL\n",
      "Key\n",
      "adsh\n",
      "Accession Number. The 20-character string formed from\n",
      "  the 18-digit number assigned by the SEC to each EDGAR submission.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "tag\n",
      "The unique identifier (name) for a tag in a specific\n",
      "  taxonomy release.\n",
      "ALPHANUMERIC\n",
      "256\n",
      "No\n",
      "*\n",
      "version\n",
      "For a standard tag, an identifier for the taxonomy;\n",
      "  otherwise the accession number where the tag was defined.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "ddate\n",
      "The end date for the data value, rounded to the nearest\n",
      "  month end.\n",
      "DATE (yyyymmdd)\n",
      "8\n",
      "No\n",
      "*\n",
      "qtrs\n",
      "The count of the number of quarters represented by the\n",
      "  data value, rounded to the nearest whole number. “0” indicates it is a\n",
      "  point-in-time value.\n",
      "NUMERIC\n",
      "8\n",
      "No\n",
      "*\n",
      "uom\n",
      "The unit of measure for the value.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "segments\n",
      "Tags used to represent axis and member reporting.\n",
      "ALPHANUMERIC\n",
      "1024\n",
      "Yes\n",
      "*\n",
      "coreg\n",
      "If specified, indicates a specific co-registrant, the\n",
      "  parent company, or other entity (e.g., guarantor).  NULL indicates the consolidated\n",
      "  entity.\n",
      "ALPHANUMERIC\n",
      "256\n",
      "Yes\n",
      "*\n",
      "value\n",
      "The value. This is not scaled, it is as found in the\n",
      "  Interactive Data file, but is limited to four digits to the right of the\n",
      "  decimal point.\n",
      "NUMERIC(28,4)\n",
      "16\n",
      "Yes\n",
      "footnote\n",
      "The text of any superscripted footnotes on the value,\n",
      "  as shown on the statement page, truncated to 512 characters, or if there is\n",
      "  no footnote, then this field will be blank.\n",
      "ALPHANUMERIC\n",
      "512\n",
      "Yes\n",
      "5.4\n",
      "PRE\n",
      "(Presentation of Statements)\n",
      "The PRE data set contains one row for each line of the\n",
      "financial statements tagged by the filer.  The source for the data set is\n",
      "the “as filed” XBRL filer submissions.   Note that there may be more\n",
      "than one row per entry in NUM because the same tag can appear in more than one\n",
      "statement (the tag NetIncome, for example can appear in both the Income\n",
      "Statement and Cash Flows in a single financial statement, and the tag Cash may\n",
      "appear in both the Balance Sheet and Cash Flows).\n",
      "Figure 5. Fields in the PRE data set\n",
      "Field Name\n",
      "Field Description\n",
      "Field Type (format)\n",
      "Max Size\n",
      "May be NULL\n",
      "Key\n",
      "adsh\n",
      "Accession Number. The\n",
      "  20-character string formed from the 18-digit number assigned by the SEC to\n",
      "  each EDGAR submission.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "*\n",
      "report\n",
      "Represents the report grouping. This field corresponds\n",
      "  to the statement (stmt) field, which indicates the type of statement. The\n",
      "  numeric value refers to the “R file” as posted on the EDGAR Web site.\n",
      "NUMERIC\n",
      "6\n",
      "No\n",
      "*\n",
      "line\n",
      "Represents the tag’s presentation line order for a\n",
      "  given report. Together with the statement and report field, presentation\n",
      "  location, order and grouping can be derived.\n",
      "NUMERIC\n",
      "6\n",
      "No\n",
      "*\n",
      "stmt\n",
      "The financial statement location to which the value of\n",
      "  the “report field pertains.\n",
      "ALPHANUMERIC (BS = Balance Sheet, IS = Income\n",
      "  Statement, CF = Cash Flow, EQ = Equity, CI = Comprehensive Income, SI =\n",
      "  Schedule of Investments, UN = Unclassifiable Statement).\n",
      "2\n",
      "No\n",
      "inpth\n",
      "Value was presented “parenthetically” instead of in\n",
      "  columns within the financial statements. For example:\n",
      "Receivables (\n",
      "net of\n",
      "  allowance for bad debts of $200 in 2012\n",
      ") $700\n",
      ".\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n",
      "rfile\n",
      "The type of interactive data file rendered on the EDGAR\n",
      "  web site, H = .htm file, X = .xml file.\n",
      "ALPHANUMERIC\n",
      "1\n",
      "No\n",
      "tag\n",
      "The tag chosen by the filer for this line item.\n",
      "ALPHANUMERIC\n",
      "256\n",
      "No\n",
      "version\n",
      "The taxonomy identifier if the tag is a standard tag,\n",
      "  otherwise adsh.\n",
      "ALPHANUMERIC\n",
      "20\n",
      "No\n",
      "plabel\n",
      "The text presented on the line item, also known as a “preferred”\n",
      "  label.\n",
      "ALPHANUMERIC\n",
      "512\n",
      "No\n",
      "negating\n",
      "Flag to indicate whether the plabel is negating.\n",
      "BOOLEAN (\n",
      "1 if true and 0 if false\n",
      ")\n",
      "1\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "# Đọc lại với encoding thích hợp\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Dùng BeautifulSoup để phân tích\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "# Lấy toàn bộ text\n",
    "plain_text = soup.get_text(separator='\\n', strip=True)\n",
    "print(plain_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74000d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Xuất văn bản sạch ra output.txt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Tách và gộp từng đoạn văn bản, không ngắt dòng sai\n",
    "text_lines = list(soup.body.stripped_strings)\n",
    "plain_text = \"\\n\".join(text_lines)\n",
    "\n",
    "# Ghi ra file để dễ xem toàn bộ\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(plain_text)\n",
    "\n",
    "print(\"✅ Xuất văn bản sạch ra output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28cac8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Mục lục (Contents):\n",
      "- 1Overview.. 1\n",
      "- 2Scope. 2\n",
      "- 3Organization. 2\n",
      "- 4File Formats. 3\n",
      "- 5Table Definitions. 3\n",
      "- 5.1SUB (Submissions)3\n",
      "- 5.2TAG (Tags)7\n",
      "- 5.3NUM (Numbers)7\n",
      "- 5.4PRE (Presentation of Statements)8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoduy\\AppData\\Local\\Temp\\ipykernel_18508\\1000130124.py:11: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  contents_header = soup.find(text=\"Contents\")\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Tìm phần tử chứa \"Contents\"\n",
    "contents_header = soup.find(text=\"Contents\")\n",
    "if contents_header:\n",
    "    # Tìm thẻ cha chứa danh sách liên kết \"Contents\"\n",
    "    contents_section = contents_header.find_parent()\n",
    "    \n",
    "    # Tiếp tục tìm đến danh sách liên kết kế bên (thường là <ul> hoặc <p>/<div> tiếp theo)\n",
    "    links = []\n",
    "    for tag in contents_section.find_all_next(['a', 'p'], limit=20):\n",
    "        if tag.name == \"a\" and tag.get(\"href\", \"\").startswith(\"#\"):\n",
    "            links.append(tag.get_text(strip=True))\n",
    "        elif tag.name == \"p\" and \"Figure\" in tag.get_text():\n",
    "            break  # Dừng khi đến các phần như \"Figure 1...\"\n",
    "    \n",
    "    print(\"📚 Mục lục (Contents):\")\n",
    "    for item in links:\n",
    "        print(\"-\", item)\n",
    "\n",
    "else:\n",
    "    print(\"❌ Không tìm thấy phần 'Contents'\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175d69ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã lưu mục lục vào contents_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoduy\\AppData\\Local\\Temp\\ipykernel_18508\\1596589823.py:11: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  contents_header = soup.find(text=\"Contents\")\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Tìm phần \"Contents\"\n",
    "contents_header = soup.find(text=\"Contents\")\n",
    "contents_list = []\n",
    "\n",
    "if contents_header:\n",
    "    contents_section = contents_header.find_parent()\n",
    "    for tag in contents_section.find_all_next(['a', 'p'], limit=20):\n",
    "        if tag.name == \"a\" and tag.get(\"href\", \"\").startswith(\"#\"):\n",
    "            contents_list.append(tag.get_text(strip=True))\n",
    "        elif tag.name == \"p\" and \"Figure\" in tag.get_text():\n",
    "            break\n",
    "\n",
    "# Lưu ra file .txt\n",
    "with open(\"contents_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"📚 Mục lục (Contents):\\n\")\n",
    "    for item in contents_list:\n",
    "        f.write(f\"- {item}\\n\")\n",
    "\n",
    "print(\"✅ Đã lưu mục lục vào contents_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d840b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã xuất các phần thành công vào sections_output.txt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Lấy toàn bộ các phần tử theo dòng liên tục (không giới hạn vào heading cụ thể)\n",
    "elements = list(soup.body.stripped_strings)\n",
    "\n",
    "sections = {}\n",
    "current_title = None\n",
    "current_content = []\n",
    "\n",
    "for line in elements:\n",
    "    # Kiểm tra nếu là tiêu đề bắt đầu bằng số hoặc \"Figure\"\n",
    "    if line.strip().startswith((\"1 \", \"2 \", \"3 \", \"4 \", \"5 \", \"Figure\")):\n",
    "        # Lưu phần trước đó nếu có\n",
    "        if current_title:\n",
    "            sections[current_title] = \"\\n\".join(current_content)\n",
    "        # Cập nhật tiêu đề mới\n",
    "        current_title = line.strip()\n",
    "        current_content = []\n",
    "    else:\n",
    "        # Nội dung thuộc về tiêu đề hiện tại\n",
    "        current_content.append(line.strip())\n",
    "\n",
    "# Lưu phần cuối cùng\n",
    "if current_title:\n",
    "    sections[current_title] = \"\\n\".join(current_content)\n",
    "\n",
    "# Ghi ra file txt để quan sát\n",
    "with open(\"sections_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for title, content in sections.items():\n",
    "        f.write(f\"{title}\\n{'='*len(title)}\\n{content}\\n\\n\")\n",
    "\n",
    "print(\"✅ Đã xuất các phần thành công vào sections_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1badc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to output.md\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to convert table to Markdown format\n",
    "def table_to_markdown(table):\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr'):\n",
    "        row = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n",
    "        rows.append(row)\n",
    "    if not rows:\n",
    "        return ''\n",
    "    # Create header row\n",
    "    header_row = '| ' + ' | '.join(rows[0]) + ' |'\n",
    "    # Create separator row\n",
    "    separator_row = '| ' + ' | '.join(['---'] * len(rows[0])) + ' |'\n",
    "    # Create body rows\n",
    "    body_rows = ['| ' + ' | '.join(row) + ' |' for row in rows[1:]]\n",
    "    # Combine all\n",
    "    return '\\n'.join([header_row, separator_row] + body_rows)\n",
    "\n",
    "# HTML content (replace this with reading from a file if needed)\n",
    "# html_content = \"\"\"\n",
    "# (Your provided HTML content here - truncated for brevity; in practice, read from file or use full string)\n",
    "# \"\"\"\n",
    "file_path = \"readme.htm\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Open file to write Markdown output\n",
    "with open('output.md', 'w', encoding='utf-8') as f:\n",
    "    # Find the main content div\n",
    "    main_div = soup.find('div', class_='WordSection1')\n",
    "    if main_div:\n",
    "        for element in main_div.children:\n",
    "            if element.name == 'h1':\n",
    "                f.write('# ' + element.get_text(strip=True) + '\\n\\n')\n",
    "            elif element.name == 'h2':\n",
    "                f.write('## ' + element.get_text(strip=True) + '\\n\\n')\n",
    "            elif element.name == 'p':\n",
    "                classes = element.get('class', [])\n",
    "                if 'MsoListParagraph' in classes:\n",
    "                    f.write('- ' + element.get_text(strip=True) + '\\n')\n",
    "                elif 'MsoTocHeading' in classes:\n",
    "                    f.write('# ' + element.get_text(strip=True) + '\\n\\n')\n",
    "                elif 'MsoToc1' in classes or 'MsoToc2' in classes or 'MsoTof' in classes:\n",
    "                    f.write(element.get_text(strip=True) + '\\n')\n",
    "                elif 'MsoCaption' in classes:\n",
    "                    f.write('**' + element.get_text(strip=True) + '**\\n\\n')\n",
    "                else:\n",
    "                    f.write(element.get_text(strip=True) + '\\n\\n')\n",
    "            elif element.name == 'table':\n",
    "                markdown_table = table_to_markdown(element)\n",
    "                f.write(markdown_table + '\\n\\n')\n",
    "            elif element.name == 'div' and element.get('align') == 'center':\n",
    "                # Handle centered tables within div\n",
    "                table = element.find('table')\n",
    "                if table:\n",
    "                    markdown_table = table_to_markdown(table)\n",
    "                    f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "print(\"Text extracted and saved to output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b3afc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to output.md\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to convert HTML table to Markdown\n",
    "def table_to_markdown(table):\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr'):\n",
    "        row = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n",
    "        rows.append(row)\n",
    "    if not rows:\n",
    "        return ''\n",
    "    header_row = '| ' + ' | '.join(rows[0]) + ' |'\n",
    "    separator_row = '| ' + ' | '.join(['---'] * len(rows[0])) + ' |'\n",
    "    body_rows = ['| ' + ' | '.join(row) + ' |' for row in rows[1:]]\n",
    "    return '\\n'.join([header_row, separator_row] + body_rows)\n",
    "\n",
    "# Function to process text and handle nested elements like links and code\n",
    "def process_text(element):\n",
    "    text = ''\n",
    "    for child in element.children:\n",
    "        if child.name == 'a' and child.get('href'):\n",
    "            text += f\"[{child.get_text(strip=True)}]({child.get('href')})\"\n",
    "        elif child.name in ['code', 'pre']:\n",
    "            text += f\"\\n```\\n{child.get_text(strip=True)}\\n```\\n\"\n",
    "        else:\n",
    "            text += child.get_text(strip=True) if isinstance(child, BeautifulSoup) else str(child).strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Read HTML file\n",
    "file_path = \"readme.html\"\n",
    "with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Open Markdown file for writing\n",
    "with open('output.md', 'w', encoding='utf-8') as f:\n",
    "    # Find main content div\n",
    "    main_div = soup.find('div', class_='WordSection1')\n",
    "    if main_div:\n",
    "        for element in main_div.children:\n",
    "            if element.name == 'h1':\n",
    "                f.write('# ' + element.get_text(strip=True) + '\\n\\n')\n",
    "            elif element.name == 'h2':\n",
    "                # Handle subheadings (e.g., 5.1, 5.2)\n",
    "                heading_text = element.get_text(strip=True)\n",
    "                if heading_text.startswith(('1', '2', '3', '4', '5')) and '.' in heading_text:\n",
    "                    f.write('## ' + heading_text + '\\n\\n')\n",
    "                else:\n",
    "                    f.write('## ' + heading_text + '\\n\\n')\n",
    "            elif element.name == 'p':\n",
    "                classes = element.get('class', [])\n",
    "                text = process_text(element)\n",
    "                if 'MsoListParagraph' in classes or text.startswith('·'):\n",
    "                    # Clean up bullet points\n",
    "                    text = text.lstrip('·').strip()\n",
    "                    f.write('- ' + text + '\\n')\n",
    "                elif 'MsoCaption' in classes:\n",
    "                    f.write('**' + text + '**\\n\\n')\n",
    "                elif 'MsoTocHeading' in classes:\n",
    "                    f.write('# ' + text + '\\n\\n')\n",
    "                elif any(cls in classes for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    f.write(text + '\\n')\n",
    "                else:\n",
    "                    f.write(text + '\\n\\n')\n",
    "            elif element.name == 'table':\n",
    "                markdown_table = table_to_markdown(element)\n",
    "                f.write(markdown_table + '\\n\\n')\n",
    "            elif element.name == 'div' and element.get('align') == 'center':\n",
    "                table = element.find('table')\n",
    "                if table:\n",
    "                    markdown_table = table_to_markdown(table)\n",
    "                    f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "print(\"Text extracted and saved to output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9615df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_table(table):\n",
    "    \"\"\"Format bảng thành markdown.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    table_md = ['| ' + ' | '.join(headers) + ' |']\n",
    "    table_md.append('| ' + ' | '.join(['---'] * len(headers)) + ' |')\n",
    "    \n",
    "    for row in rows[1:]:\n",
    "        cells = [clean_text(td.get_text()) for td in row.find_all('td')]\n",
    "        table_md.append('| ' + ' | '.join(cells) + ' |')\n",
    "    \n",
    "    return '\\n'.join(table_md) + '\\n'\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành markdown.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Xử lý tiêu đề chính\n",
    "    title = soup.find('p', class_='MsoTocHeading')\n",
    "    if title:\n",
    "        output.append(f\"# {clean_text(title.get_text())}\")\n",
    "    \n",
    "    # Xử lý mục lục\n",
    "    toc_heading = soup.find('p', class_='MsoTocHeading', string=re.compile('Contents'))\n",
    "    if toc_heading:\n",
    "        output.append(f\"## {clean_text(toc_heading.get_text())}\")\n",
    "        for toc_item in soup.find_all(['p'], class_=['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "            level = 2 if toc_item.get('class')[0] == 'MsoToc1' else 3\n",
    "            text = clean_text(toc_item.get_text())\n",
    "            output.append(f\"{'#' * level} {text}\")\n",
    "    \n",
    "    # Xử lý các heading (h1, h2, ...)\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        level = int(heading.name[1])\n",
    "        text = clean_text(heading.get_text())\n",
    "        output.append(f\"{'#' * (level + 1)} {text}\")\n",
    "    \n",
    "    # Xử lý đoạn văn\n",
    "    for para in soup.find_all(['p'], class_=['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "        text = clean_text(para.get_text())\n",
    "        if para.get('class')[0] == 'MsoListParagraph':\n",
    "            output.append(f\"- {text}\")\n",
    "        elif para.get('class')[0] == 'MsoCaption':\n",
    "            output.append(f\"**{text}**\")\n",
    "        else:\n",
    "            output.append(text)\n",
    "    \n",
    "    # Xử lý bảng\n",
    "    for table in soup.find_all('table', class_='MsoNormalTable'):\n",
    "        output.append(format_table(table))\n",
    "    \n",
    "    return '\\n\\n'.join([line for line in output if line])\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file markdown\n",
    "    with open('formatted_financial_statement.md', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def format_tag_report_md(data):\n",
    "    \"\"\"Chuyển đổi dữ liệu bảng TAG thành định dạng báo cáo tài chính markdown.\"\"\"\n",
    "    report = [\n",
    "        f\"# TAG Data Structure Report\",\n",
    "        f\"**Generated Date**: {datetime.now().strftime('%Y-%m-%d')}\",\n",
    "        \"\",\n",
    "        \"## 1. Tag Identification\",\n",
    "        \"- **Tag Name** (tag): The unique identifier for a tag in a specific taxonomy release. (Type: ALPHANUMERIC, Size: 256, Required)\",\n",
    "        \"- **Version** (version): Identifies the taxonomy for standard tags or accession number for custom tags. (Type: ALPHANUMERIC, Size: 20, Required)\",\n",
    "        \"\",\n",
    "        \"## 2. Tag Characteristics\",\n",
    "        \"- **Custom Flag** (custom): Indicates if the tag is custom (1) or standard (0). Redundant with version/adsh. (Type: BOOLEAN, Size: 1, Required)\",\n",
    "        \"- **Abstract Flag** (abstract): Set to 1 if the tag does not represent a numeric fact. (Type: BOOLEAN, Size: 1, Required)\",\n",
    "        \"\",\n",
    "        \"## 3. Data Properties\",\n",
    "        \"- **Data Type** (datatype): Specifies the data type (e.g., monetary) if not abstract; NULL if abstract. (Type: ALPHANUMERIC, Size: 20, Optional)\",\n",
    "        \"- **Instant or Duration** (iord): Indicates if the value is point-in-time ('I') or duration ('D'); NULL if abstract. (Type: ALPHANUMERIC, Size: 1, Required)\",\n",
    "        \"- **Credit/Debit** (crdr): Natural accounting balance (C or D) for monetary tags; NULL if undefined. (Type: ALPHANUMERIC, Size: 1, Optional)\",\n",
    "        \"\",\n",
    "        \"## 4. Tag Documentation\",\n",
    "        \"- **Label Text** (tlabel): Label provided by taxonomy (standard) or filer (custom); NULL if absent. (Type: ALPHANUMERIC, Size: 512, Optional)\",\n",
    "        \"- **Detailed Definition** (doc): Detailed definition from taxonomy (standard) or filer (custom); NULL if absent. (Type: ALPHANUMERIC, Optional)\",\n",
    "        \"\",\n",
    "        \"## Summary\",\n",
    "        \"The TAG data structure defines metadata for XBRL tags, including identification, characteristics, data properties, and documentation. Key fields (tag, version) ensure uniqueness, while optional fields (datatype, crdr, tlabel, doc) provide flexibility for varied use cases.\"\n",
    "    ]\n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "def format_tag_report_json(data):\n",
    "    \"\"\"Chuyển đổi dữ liệu bảng TAG thành định dạng JSON.\"\"\"\n",
    "    report = {\n",
    "        \"report_title\": \"TAG Data Structure Report\",\n",
    "        \"generated_date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "        \"sections\": [\n",
    "            {\n",
    "                \"title\": \"Tag Identification\",\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"field\": \"tag\",\n",
    "                        \"description\": \"The unique identifier for a tag in a specific taxonomy release.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 256,\n",
    "                        \"required\": True,\n",
    "                        \"key\": True\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"version\",\n",
    "                        \"description\": \"Identifies the taxonomy for standard tags or accession number for custom tags.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 20,\n",
    "                        \"required\": True,\n",
    "                        \"key\": True\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Tag Characteristics\",\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"field\": \"custom\",\n",
    "                        \"description\": \"Indicates if the tag is custom (1) or standard (0). Redundant with version/adsh.\",\n",
    "                        \"type\": \"BOOLEAN\",\n",
    "                        \"max_size\": 1,\n",
    "                        \"required\": True\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"abstract\",\n",
    "                        \"description\": \"Set to 1 if the tag does not represent a numeric fact.\",\n",
    "                        \"type\": \"BOOLEAN\",\n",
    "                        \"max_size\": 1,\n",
    "                        \"required\": True\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Data Properties\",\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"field\": \"datatype\",\n",
    "                        \"description\": \"Specifies the data type (e.g., monetary) if not abstract; NULL if abstract.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 20,\n",
    "                        \"required\": False\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"iord\",\n",
    "                        \"description\": \"Indicates if the value is point-in-time ('I') or duration ('D'); NULL if abstract.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 1,\n",
    "                        \"required\": True\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"crdr\",\n",
    "                        \"description\": \"Natural accounting balance (C or D) for monetary tags; NULL if undefined.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 1,\n",
    "                        \"required\": False\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Tag Documentation\",\n",
    "                \"items\": [\n",
    "                    {\n",
    "                        \"field\": \"tlabel\",\n",
    "                        \"description\": \"Label provided by taxonomy (standard) or filer (custom); NULL if absent.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": 512,\n",
    "                        \"required\": False\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"doc\",\n",
    "                        \"description\": \"Detailed definition from taxonomy (standard) or filer (custom); NULL if absent.\",\n",
    "                        \"type\": \"ALPHANUMERIC\",\n",
    "                        \"max_size\": None,\n",
    "                        \"required\": False\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"summary\": \"The TAG data structure defines metadata for XBRL tags, including identification, characteristics, data properties, and documentation. Key fields (tag, version) ensure uniqueness, while optional fields (datatype, crdr, tlabel, doc) provide flexibility for varied use cases.\"\n",
    "    }\n",
    "    return report\n",
    "\n",
    "def main():\n",
    "    # Dữ liệu bảng TAG (giả lập từ input của bạn)\n",
    "    tag_data = [\n",
    "        {\"field\": \"tag\", \"description\": \"The unique identifier (name) for a tag in a specific taxonomy release.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 256, \"may_be_null\": \"No\", \"key\": \"*\"},\n",
    "        {\"field\": \"version\", \"description\": \"For a standard tag, an identifier for the taxonomy; otherwise the accession number where the tag was defined.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 20, \"may_be_null\": \"No\", \"key\": \"*\"},\n",
    "        {\"field\": \"custom\", \"description\": \"1 if tag is custom (version=adsh), 0 if it is standard. Note: This flag is technically redundant with the version and adsh columns.\", \"type\": \"BOOLEAN (1 if true and 0 if false)\", \"max_size\": 1, \"may_be_null\": \"No\", \"key\": \"\"},\n",
    "        {\"field\": \"abstract\", \"description\": \"1 if the tag is not used to represent a numeric fact.\", \"type\": \"BOOLEAN (1 if true and 0 if false)\", \"max_size\": 1, \"may_be_null\": \"No\", \"key\": \"\"},\n",
    "        {\"field\": \"datatype\", \"description\": \"If abstract=1, then NULL, otherwise the data type (e.g., monetary) for the tag.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 20, \"may_be_null\": \"Yes\", \"key\": \"\"},\n",
    "        {\"field\": \"iord\", \"description\": \"If abstract=1, then NULL; otherwise, “I” if the value is a point-in time, or “D” if the value is a duration.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 1, \"may_be_null\": \"No\", \"key\": \"\"},\n",
    "        {\"field\": \"crdr\", \"description\": \"If datatype = monetary, then the tag’s natural accounting balance (debit or credit); if not defined, then NULL.\", \"type\": \"ALPHANUMERIC (“C” or “D”)\", \"max_size\": 1, \"may_be_null\": \"Yes\", \"key\": \"\"},\n",
    "        {\"field\": \"tlabel\", \"description\": \"If a standard tag, then the label text provided by the taxonomy, otherwise the text provided by the filer. A tag which had neither would have a NULL value here.\", \"type\": \"ALPHANUMERIC\", \"max_size\": 512, \"may_be_null\": \"Yes\", \"key\": \"\"},\n",
    "        {\"field\": \"doc\", \"description\": \"The detailed definition for the tag. If a standard tag, then the text provided by the taxonomy, otherwise the text assigned by the filer. Some tags have neither, and this field is NULL.\", \"type\": \"ALPHANUMERIC\", \"max_size\": \"\", \"may_be_null\": \"Yes\", \"key\": \"\"}\n",
    "    ]\n",
    "\n",
    "    # Tạo báo cáo markdown\n",
    "    md_report = format_tag_report_md(tag_data)\n",
    "    with open('tag_data_report.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(md_report)\n",
    "\n",
    "    # Tạo báo cáo JSON\n",
    "    json_report = format_tag_report_json(tag_data)\n",
    "    with open('tag_data_report.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "448ef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuyển một hàng của bảng thành một dòng văn bản theo định dạng yêu cầu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"Xử lý bảng HTML và chuyển thành danh sách các dòng văn bản.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # Xác định tiêu đề cột từ hàng đầu tiên\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuyển các hàng dữ liệu thành danh sách các dòng văn bản\n",
    "    lines = [f\"{table_title}\", \"=\" * 40, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Chỉ xử lý hàng có dữ liệu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(line)\n",
    "    lines.extend([\"\", \"=\" * 40])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành danh sách các dòng văn bản.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Xử lý tiêu đề chính\n",
    "    title = soup.find('p', class_='MsoTocHeading')\n",
    "    if title:\n",
    "        output.append(clean_text(title.get_text()))\n",
    "        output.append(\"=\" * 40)\n",
    "        output.append(\"\")\n",
    "    \n",
    "    # Xử lý các bảng\n",
    "    for table in soup.find_all('table', class_='MsoNormalTable'):\n",
    "        # Tìm tiêu đề bảng từ thẻ caption gần nhất trước bảng\n",
    "        caption = table.find_previous('p', class_='MsoCaption')\n",
    "        table_title = clean_text(caption.get_text()) if caption else \"Table\"\n",
    "        table_lines = process_table(table, table_title)\n",
    "        if table_lines:  # Chỉ thêm nếu bảng có dữ liệu\n",
    "            output.extend(table_lines)\n",
    "            output.append(\"\")\n",
    "    \n",
    "    return '\\n'.join([line for line in output if line])\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file text\n",
    "    with open('formatted_tables.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8bf86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuyển một hàng của bảng thành một dòng văn bản theo định dạng yêu cầu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"Xử lý bảng HTML và chuyển thành danh sách các dòng văn bản.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # Xác định tiêu đề cột từ hàng đầu tiên\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuyển các hàng dữ liệu thành danh sách các dòng văn bản\n",
    "    lines = [f\"{table_title}\", \"=\" * 40, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Chỉ xử lý hàng có dữ liệu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(line)\n",
    "    lines.extend([\"\", \"=\" * 40])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành danh sách các dòng văn bản.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Xử lý tiêu đề chính\n",
    "    title = soup.find('p', class_='MsoTocHeading')\n",
    "    if title:\n",
    "        output.append(clean_text(title.get_text()))\n",
    "        output.append(\"=\" * 40)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Xử lý mục lục\n",
    "    toc_heading = soup.find('p', class_='MsoTocHeading', string=re.compile('Contents'))\n",
    "    if toc_heading:\n",
    "        output.append(clean_text(toc_heading.get_text()))\n",
    "        output.append(\"-\" * 40)\n",
    "        output.append(\"\")\n",
    "        for toc_item in soup.find_all(['p'], class_=['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "            level = 1 if toc_item.get('class')[0] == 'MsoToc1' else 2\n",
    "            text = clean_text(toc_item.get_text())\n",
    "            output.append(f\"{'  ' * (level - 1)}{text}\")\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Xử lý các heading (h1, h2, ...)\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        level = int(heading.name[1])\n",
    "        text = clean_text(heading.get_text())\n",
    "        output.append(f\"{'-' * (level + 1)} {text}\")\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Xử lý đoạn văn và danh sách\n",
    "    for para in soup.find_all(['p'], class_=['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "        text = clean_text(para.get_text())\n",
    "        if para.get('class')[0] == 'MsoListParagraph':\n",
    "            output.append(f\"- {text}\")\n",
    "        elif para.get('class')[0] == 'MsoCaption':\n",
    "            output.append(f\"**{text}**\")\n",
    "        else:\n",
    "            output.append(text)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Xử lý các bảng\n",
    "    for table in soup.find_all('table', class_='MsoNormalTable'):\n",
    "        # Tìm tiêu đề bảng từ thẻ caption gần nhất trước bảng\n",
    "        caption = table.find_previous('p', class_='MsoCaption')\n",
    "        table_title = clean_text(caption.get_text()) if caption else \"Table\"\n",
    "        table_lines = process_table(table, table_title)\n",
    "        if table_lines:  # Chỉ thêm nếu bảng có dữ liệu\n",
    "            output.extend(table_lines)\n",
    "            output.append(\"\")\n",
    "\n",
    "    return '\\n'.join([line for line in output if line])\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file text\n",
    "    with open('formatted_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad95e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuyển một hàng của bảng thành một dòng văn bản theo định dạng yêu cầu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"Xử lý bảng HTML và chuyển thành danh sách các dòng văn bản.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # Xác định tiêu đề cột từ hàng đầu tiên\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuyển các hàng dữ liệu thành danh sách các dòng văn bản\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Chỉ xử lý hàng có dữ liệu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Thụt lề để dễ đọc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành danh sách các dòng văn bản.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Xử lý tiêu đề chính\n",
    "    title = soup.find('p', class_='MsoTocHeading')\n",
    "    if title:\n",
    "        output.append(clean_text(title.get_text()).upper())\n",
    "        output.append(\"=\" * 50)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Xử lý mục lục\n",
    "    toc_heading = soup.find('p', class_='MsoTocHeading', string=re.compile('Contents'))\n",
    "    if toc_heading:\n",
    "        output.append(\"TABLE OF CONTENTS\")\n",
    "        output.append(\"-\" * 50)\n",
    "        output.append(\"\")\n",
    "        for toc_item in soup.find_all(['p'], class_=['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "            level = 1 if toc_item.get('class')[0] == 'MsoToc1' else 2\n",
    "            text = clean_text(toc_item.get_text())\n",
    "            output.append(f\"{'  ' * (level - 1)}{text}\")\n",
    "        output.append(\"\")\n",
    "        output.append(\"-\" * 50)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Xử lý các heading (h1, h2, ...)\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        level = int(heading.name[1])\n",
    "        text = clean_text(heading.get_text())\n",
    "        # Thêm dấu phân cách theo cấp độ heading\n",
    "        separator = \"-\" * (level + 1)\n",
    "        output.append(f\"{separator} {text.upper()}\")\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Xử lý đoạn văn và danh sách\n",
    "    for para in soup.find_all(['p'], class_=['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "        text = clean_text(para.get_text())\n",
    "        if not text:  # Bỏ qua đoạn văn rỗng\n",
    "            continue\n",
    "        if para.get('class')[0] == 'MsoListParagraph':\n",
    "            output.append(f\"  - {text}\")  # Thụt lề cho danh sách\n",
    "        elif para.get('class')[0] == 'MsoCaption':\n",
    "            output.append(f\"**{text.upper()}**\")  # Caption in đậm và in hoa\n",
    "        else:\n",
    "            output.append(text)\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Xử lý các bảng\n",
    "    for table in soup.find_all('table', class_='MsoNormalTable'):\n",
    "        caption = table.find_previous('p', class_='MsoCaption')\n",
    "        table_title = clean_text(caption.get_text()) if caption else \"Table\"\n",
    "        table_lines = process_table(table, table_title.upper())\n",
    "        if table_lines:  # Chỉ thêm nếu bảng có dữ liệu\n",
    "            output.extend(table_lines)\n",
    "            output.append(\"\")\n",
    "\n",
    "    # Loại bỏ các dòng trống liên tiếp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file text\n",
    "    with open('improved_formatted_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5349c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuyển một hàng của bảng thành một dòng văn bản theo định dạng yêu cầu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"Xử lý bảng HTML và chuyển thành danh sách các dòng văn bản.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # Xác định tiêu đề cột từ hàng đầu tiên\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuyển các hàng dữ liệu thành danh sách các dòng văn bản\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Chỉ xử lý hàng có dữ liệu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Thụt lề để dễ đọc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành danh sách các dòng văn bản.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Duyệt qua tất cả các phần tử trong HTML theo thứ tự xuất hiện\n",
    "    for element in soup.find('body').children:\n",
    "        # Xử lý tiêu đề chính\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý mục lục\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')):\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # Tìm các mục trong mục lục\n",
    "            toc_items = []\n",
    "            current = element.next_sibling\n",
    "            while current and current.name == 'p' and any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                text = clean_text(current.get_text())\n",
    "                # Điều chỉnh thụt lề cho các mục con\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Thụt lề 4 khoảng cách cho mục con\n",
    "                toc_items.append(text)\n",
    "                current = current.next_sibling\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý đoạn văn và danh sách\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # Bỏ qua đoạn văn rỗng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      · {text}\")  # Thụt lề 6 khoảng cách và dùng ký hiệu ·\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                output.append(f\"Figure {text}\")\n",
    "                output.append(\"\")\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý bảng (giữ nguyên)\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            caption = element.find_previous('p', class_='MsoCaption')\n",
    "            table_title = clean_text(caption.get_text()) if caption else \"Table\"\n",
    "            table_lines = process_table(element, table_title.upper())\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "\n",
    "    # Loại bỏ các dòng trống liên tiếp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file text\n",
    "    with open('improved_formatted_content_v3.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3441bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuyển một hàng của bảng thành một dòng văn bản theo định dạng yêu cầu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"Xử lý bảng HTML và chuyển thành danh sách các dòng văn bản.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # Xác định tiêu đề cột từ hàng đầu tiên\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuyển các hàng dữ liệu thành danh sách các dòng văn bản\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Chỉ xử lý hàng có dữ liệu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Thụt lề để dễ đọc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành danh sách các dòng văn bản.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Lấy tất cả các phần tử cần xử lý theo thứ tự xuất hiện\n",
    "    elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "    toc_processed = False\n",
    "    table_caption = None\n",
    "\n",
    "    for element in elements:\n",
    "        # Xử lý tiêu đề chính\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý mục lục\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')) and not toc_processed:\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # Tìm các mục trong mục lục\n",
    "            toc_items = []\n",
    "            current = element\n",
    "            while current:\n",
    "                current = current.find_next('p')\n",
    "                if not current or not any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    break\n",
    "                text = clean_text(current.get_text())\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Thụt lề 4 khoảng cách cho mục con\n",
    "                toc_items.append(text)\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "            toc_processed = True\n",
    "        \n",
    "        # Xử lý heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý đoạn văn, danh sách và caption\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # Bỏ qua đoạn văn rỗng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      · {text}\")  # Thụt lề 6 khoảng cách và dùng ký hiệu ·\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                table_caption = f\"Figure {text}\"  # Lưu caption để dùng cho bảng tiếp theo\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý bảng (giữ nguyên)\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            table_title = table_caption if table_caption else \"Table\"\n",
    "            table_lines = process_table(element, table_title.upper())\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "            table_caption = None  # Reset caption sau khi dùng\n",
    "\n",
    "    # Loại bỏ các dòng trống liên tiếp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file text\n",
    "    with open('fixed_formatted_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27bb944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuyển một hàng của bảng thành một dòng văn bản theo định dạng yêu cầu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"Xử lý bảng HTML và chuyển thành danh sách các dòng văn bản.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # Xác định tiêu đề cột từ hàng đầu tiên\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuyển các hàng dữ liệu thành danh sách các dòng văn bản\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Chỉ xử lý hàng có dữ liệu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Thụt lề để dễ đọc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành danh sách các dòng văn bản.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Lấy tất cả các phần tử cần xử lý theo thứ tự xuất hiện\n",
    "    elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "    toc_processed = False\n",
    "    table_caption = None\n",
    "    skip_until_table = False  # Biến để kiểm soát việc bỏ qua nội dung bảng\n",
    "\n",
    "    for element in elements:\n",
    "        # Bỏ qua nội dung nếu đang trong trạng thái bỏ qua (cho đến khi gặp bảng)\n",
    "        if skip_until_table and element.name != 'table':\n",
    "            continue\n",
    "\n",
    "        # Xử lý tiêu đề chính\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý mục lục\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')) and not toc_processed:\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # Tìm các mục trong mục lục\n",
    "            toc_items = []\n",
    "            current = element\n",
    "            while current:\n",
    "                current = current.find_next('p')\n",
    "                if not current or not any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    break\n",
    "                text = clean_text(current.get_text())\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Thụt lề 4 khoảng cách cho mục con\n",
    "                toc_items.append(text)\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "            toc_processed = True\n",
    "        \n",
    "        # Xử lý heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý đoạn văn, danh sách và caption\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # Bỏ qua đoạn văn rỗng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      · {text}\")  # Thụt lề 6 khoảng cách và dùng ký hiệu ·\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                table_caption = f\"Figure {text}\"  # Lưu caption để dùng cho bảng tiếp theo\n",
    "                skip_until_table = True  # Bỏ qua các đoạn văn cho đến khi gặp bảng\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý bảng\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            table_title = table_caption if table_caption else \"Table\"\n",
    "            table_lines = process_table(element, table_title)\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "            table_caption = None  # Reset caption sau khi dùng\n",
    "            skip_until_table = False  # Kết thúc trạng thái bỏ qua\n",
    "\n",
    "    # Loại bỏ các dòng trống liên tiếp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file text\n",
    "    with open('fixed_formatted_content_v2.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7518c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuyển một hàng của bảng thành một dòng văn bản theo định dạng yêu cầu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"Xử lý bảng HTML và chuyển thành danh sách các dòng văn bản.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return []\n",
    "    \n",
    "    # Xác định tiêu đề cột từ hàng đầu tiên\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return []\n",
    "    \n",
    "    # Chuyển các hàng dữ liệu thành danh sách các dòng văn bản\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Chỉ xử lý hàng có dữ liệu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Thụt lề để dễ đọc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines, headers  # Trả về tiêu đề cột để kiểm tra nội dung bảng\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành danh sách các dòng văn bản.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Lấy tất cả các phần tử cần xử lý theo thứ tự xuất hiện\n",
    "    elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "    toc_processed = False\n",
    "    table_caption = None\n",
    "    skip_until_table = False  # Biến để kiểm soát việc bỏ qua nội dung trước bảng\n",
    "    skip_table_content = False  # Biến để kiểm soát việc bỏ qua nội dung bảng\n",
    "    table_headers = []  # Lưu trữ tiêu đề cột của bảng hiện tại\n",
    "\n",
    "    for element in elements:\n",
    "        # Bỏ qua nội dung nếu đang trong trạng thái bỏ qua (cho đến khi gặp bảng)\n",
    "        if skip_until_table and element.name != 'table':\n",
    "            continue\n",
    "\n",
    "        # Bỏ qua nội dung bảng nếu đang trong trạng thái bỏ qua nội dung bảng\n",
    "        if skip_table_content and element.name == 'p' and 'MsoNormal' in element.get('class', []):\n",
    "            text = clean_text(element.get_text())\n",
    "            # Nếu đoạn văn không chứa tiêu đề cột hoặc dữ liệu bảng, dừng bỏ qua\n",
    "            if not any(header.lower() in text.lower() for header in table_headers) and not text.startswith(\"Note:\"):\n",
    "                skip_table_content = False\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Xử lý tiêu đề chính\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý mục lục\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')) and not toc_processed:\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # Tìm các mục trong mục lục\n",
    "            toc_items = []\n",
    "            current = element\n",
    "            while current:\n",
    "                current = current.find_next('p')\n",
    "                if not current or not any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    break\n",
    "                text = clean_text(current.get_text())\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Thụt lề 4 khoảng cách cho mục con\n",
    "                toc_items.append(text)\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "            toc_processed = True\n",
    "        \n",
    "        # Xử lý heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "            skip_table_content = False  # Reset trạng thái bỏ qua nội dung bảng khi gặp heading\n",
    "        \n",
    "        # Xử lý đoạn văn, danh sách và caption\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # Bỏ qua đoạn văn rỗng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      · {text}\")  # Thụt lề 6 khoảng cách và dùng ký hiệu ·\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                table_caption = f\"Figure {text}\"  # Lưu caption để dùng cho bảng tiếp theo\n",
    "                skip_until_table = True  # Bỏ qua các đoạn văn cho đến khi gặp bảng\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý bảng\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            table_title = table_caption if table_caption else \"Table\"\n",
    "            table_lines, headers = process_table(element, table_title)\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "            table_caption = None  # Reset caption sau khi dùng\n",
    "            skip_until_table = False  # Kết thúc trạng thái bỏ qua\n",
    "            skip_table_content = True  # Bắt đầu bỏ qua nội dung bảng\n",
    "            table_headers = headers  # Lưu tiêu đề cột để kiểm tra nội dung bảng\n",
    "\n",
    "    # Loại bỏ các dòng trống liên tiếp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file text\n",
    "    with open('fixed_formatted_content_v3.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51678f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch text: loại bỏ khoảng trắng thừa, dòng trống liên tiếp.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def format_row_to_line(row, headers):\n",
    "    \"\"\"Chuyển một hàng của bảng thành một dòng văn bản theo định dạng yêu cầu.\"\"\"\n",
    "    parts = []\n",
    "    for header, cell in zip(headers, row):\n",
    "        cell_value = clean_text(cell.get_text()) if cell else \"Not specified\"\n",
    "        parts.append(f\"{header}: {cell_value}\")\n",
    "    return \" has \".join(parts)\n",
    "\n",
    "def process_table(table, table_title):\n",
    "    \"\"\"Xử lý bảng HTML và chuyển thành danh sách các dòng văn bản.\"\"\"\n",
    "    rows = table.find_all('tr')\n",
    "    if not rows:\n",
    "        return [], [], []\n",
    "    \n",
    "    # Xác định tiêu đề cột từ hàng đầu tiên\n",
    "    headers = [clean_text(th.get_text()) for th in rows[0].find_all(['th', 'td'])]\n",
    "    if not headers:\n",
    "        return [], [], []\n",
    "    \n",
    "    # Lưu trữ dữ liệu hàng để kiểm tra sau này\n",
    "    table_data = []\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:\n",
    "            row_data = [clean_text(cell.get_text()) for cell in cells]\n",
    "            table_data.extend(row_data)\n",
    "    \n",
    "    # Chuyển các hàng dữ liệu thành danh sách các dòng văn bản\n",
    "    lines = [f\"{table_title}\", \"=\" * 50, \"\"]\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:  # Chỉ xử lý hàng có dữ liệu\n",
    "            line = format_row_to_line(cells, headers)\n",
    "            lines.append(f\"  {line}\")  # Thụt lề để dễ đọc\n",
    "    lines.extend([\"\", \"=\" * 50])\n",
    "    return lines, headers, table_data\n",
    "\n",
    "def extract_and_format_html(html_content):\n",
    "    \"\"\"Trích xuất và format nội dung HTML thành danh sách các dòng văn bản.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    output = []\n",
    "    \n",
    "    # Lấy tất cả các phần tử cần xử lý theo thứ tự xuất hiện\n",
    "    elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "    toc_processed = False\n",
    "    table_caption = None\n",
    "    skip_until_table = False  # Biến để kiểm soát việc bỏ qua nội dung trước bảng\n",
    "    skip_table_content = False  # Biến để kiểm soát việc bỏ qua nội dung bảng\n",
    "    table_headers = []  # Lưu trữ tiêu đề cột của bảng hiện tại\n",
    "    table_data = []  # Lưu trữ dữ liệu hàng của bảng hiện tại\n",
    "\n",
    "    for element in elements:\n",
    "        # Bỏ qua nội dung nếu đang trong trạng thái bỏ qua (cho đến khi gặp bảng)\n",
    "        if skip_until_table and element.name != 'table':\n",
    "            continue\n",
    "\n",
    "        # Bỏ qua nội dung bảng nếu đang trong trạng thái bỏ qua nội dung bảng\n",
    "        if skip_table_content and element.name == 'p' and 'MsoNormal' in element.get('class', []):\n",
    "            text = clean_text(element.get_text())\n",
    "            # Nếu đoạn văn không chứa tiêu đề cột, dữ liệu hàng, và không nằm trong nội dung bảng, dừng bỏ qua\n",
    "            if (not any(header.lower() in text.lower() for header in table_headers) and \n",
    "                not any(data.lower() in text.lower() for data in table_data) and \n",
    "                not text.startswith(\"Note:\")):\n",
    "                skip_table_content = False\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Xử lý tiêu đề chính\n",
    "        if element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and not element.find(string=re.compile('Contents')):\n",
    "            text = clean_text(element.get_text())\n",
    "            output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý mục lục\n",
    "        elif element.name == 'p' and 'MsoTocHeading' in element.get('class', []) and element.find(string=re.compile('Contents')) and not toc_processed:\n",
    "            output.append(\"Contents\")\n",
    "            output.append(\"\")\n",
    "            # Tìm các mục trong mục lục\n",
    "            toc_items = []\n",
    "            current = element\n",
    "            while current:\n",
    "                current = current.find_next('p')\n",
    "                if not current or not any(cls in current.get('class', []) for cls in ['MsoToc1', 'MsoToc2', 'MsoTof']):\n",
    "                    break\n",
    "                text = clean_text(current.get_text())\n",
    "                if 'MsoToc2' in current.get('class', []):\n",
    "                    text = f\"    {text}\"  # Thụt lề 4 khoảng cách cho mục con\n",
    "                toc_items.append(text)\n",
    "            \n",
    "            for item in toc_items:\n",
    "                output.append(item)\n",
    "            output.append(\"\")\n",
    "            toc_processed = True\n",
    "        \n",
    "        # Xử lý heading (h1, h2, ...)\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            level = int(element.name[1])\n",
    "            text = clean_text(element.get_text())\n",
    "            separator = \"-\" * (level + 1)\n",
    "            output.append(f\"{separator} {text}\")\n",
    "            output.append(\"\")\n",
    "            skip_table_content = False  # Reset trạng thái bỏ qua nội dung bảng khi gặp heading\n",
    "        \n",
    "        # Xử lý đoạn văn, danh sách và caption\n",
    "        elif element.name == 'p' and any(cls in element.get('class', []) for cls in ['MsoNormal', 'MsoListParagraph', 'MsoCaption']):\n",
    "            text = clean_text(element.get_text())\n",
    "            if not text:  # Bỏ qua đoạn văn rỗng\n",
    "                continue\n",
    "            if 'MsoListParagraph' in element.get('class', []):\n",
    "                output.append(f\"      · {text}\")  # Thụt lề 6 khoảng cách và dùng ký hiệu ·\n",
    "            elif 'MsoCaption' in element.get('class', []):\n",
    "                table_caption = f\"Figure {text}\"  # Lưu caption để dùng cho bảng tiếp theo\n",
    "                skip_until_table = True  # Bỏ qua các đoạn văn cho đến khi gặp bảng\n",
    "            else:\n",
    "                output.append(text)\n",
    "            output.append(\"\")\n",
    "        \n",
    "        # Xử lý bảng\n",
    "        elif element.name == 'table' and 'MsoNormalTable' in element.get('class', []):\n",
    "            table_title = table_caption if table_caption else \"Table\"\n",
    "            table_lines, headers, data = process_table(element, table_title)\n",
    "            if table_lines:\n",
    "                output.extend(table_lines)\n",
    "                output.append(\"\")\n",
    "            table_caption = None  # Reset caption sau khi dùng\n",
    "            skip_until_table = False  # Kết thúc trạng thái bỏ qua\n",
    "            skip_table_content = True  # Bắt đầu bỏ qua nội dung bảng\n",
    "            table_headers = headers  # Lưu tiêu đề cột\n",
    "            table_data = data  # Lưu dữ liệu hàng\n",
    "\n",
    "    # Loại bỏ các dòng trống liên tiếp\n",
    "    final_output = []\n",
    "    prev_line_empty = False\n",
    "    for line in output:\n",
    "        if not line.strip():\n",
    "            if not prev_line_empty:\n",
    "                final_output.append(line)\n",
    "                prev_line_empty = True\n",
    "        else:\n",
    "            final_output.append(line)\n",
    "            prev_line_empty = False\n",
    "\n",
    "    return '\\n'.join(final_output)\n",
    "\n",
    "def main():\n",
    "    # Đọc file HTML\n",
    "    with open('readme.html', 'r', encoding='windows-1252') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Trích xuất và format\n",
    "    formatted_text = extract_and_format_html(html_content)\n",
    "    \n",
    "    # Lưu vào file text\n",
    "    with open('fixed_formatted_content_v4.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45e9b720",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_split_text\u001b[39m(file_path):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Đọc file văn bản và chia thành các đoạn nhỏ.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_and_split_text(file_path):\n",
    "    \"\"\"Đọc file văn bản và chia thành các đoạn nhỏ.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Chia văn bản thành các đoạn dựa trên tiêu đề hoặc các đoạn văn lớn\n",
    "        sections = re.split(r'\\n\\s*(?=(?:--|---|Figure)\\s+.*?\\n)', text)\n",
    "        documents = []\n",
    "        for i, section in enumerate(sections):\n",
    "            section = section.strip()\n",
    "            if section:\n",
    "                # Tạo Document từ mỗi đoạn\n",
    "                doc = Document(\n",
    "                    page_content=section,\n",
    "                    metadata={\n",
    "                        \"section_id\": i,\n",
    "                        \"source\": file_path\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        print(f\"✅ Loaded and split text into {len(documents)} sections.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load and split text: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a41f0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_chroma\n",
      "  Downloading langchain_chroma-0.2.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: langchain-core>=0.3.52 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain_chroma) (0.3.58)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain_chroma) (1.26.4)\n",
      "Collecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain_chroma)\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.8.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading chroma_hnswlib-0.7.6.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi>=0.95.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.11.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading onnxruntime-1.21.1-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading mmh3-5.1.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (13.7.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain-core>=0.3.52->langchain_chroma) (0.3.42)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain-core>=0.3.52->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langchain-core>=0.3.52->langchain_chroma) (24.1)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.6)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.52->langchain_chroma) (2.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.3.52->langchain_chroma) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.3.52->langchain_chroma) (0.23.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.25.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.13.2)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (7.0.1)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.15.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading watchfiles-1.0.5-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.3.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hoduy\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.8)\n",
      "Downloading langchain_chroma-0.2.3-py3-none-any.whl (11 kB)\n",
      "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "   ---------------------------------------- 0.0/611.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 611.1/611.1 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.6/4.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.6/2.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 7.9 MB/s eta 0:00:00\n",
      "Downloading mmh3-5.1.0-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Downloading onnxruntime-1.21.1-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.3 MB 7.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/12.3 MB 7.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.3/12.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.4/12.3 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n",
      "Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n",
      "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.3-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading watchfiles-1.0.5-cp312-cp312-win_amd64.whl (291 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib, pypika\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=2988c6c7b9b56ceedcaf991ddb0962889c7d911c88a53948f7eae512986b4b91\n",
      "  Stored in directory: c:\\users\\hoduy\\appdata\\local\\pip\\cache\\wheels\\d5\\3d\\69\\8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Failed to build chroma-hnswlib\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for chroma-hnswlib (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'hnswlib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for chroma-hnswlib\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (chroma-hnswlib)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849a8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded and split text into 15 sections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoduy\\AppData\\Local\\Temp\\ipykernel_35148\\3786724872.py:47: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Đã thêm 15 tài liệu vào vector DB.\n",
      "✅ Created vector DB with 15 documents at d:\\hk2_nam3\\langraph\\day4\\kiennguyen\\financial_data_db.\n",
      "✅ Loaded existing Vector DB with 15 documents.\n",
      "✅ Retriever initialized with similarity_score_threshold.\n",
      "---RETRIEVAL FROM VECTOR DB---\n",
      "📁 Retrieved 3 documents:\n",
      "Document 1:\n",
      "Figure Figure 1. Data relationships\n",
      "==================================================\n",
      "\n",
      "  Dataset: NUM has Columns referencing other datasets: adsh has Referenced dataset: SUB has Referenced columns: ...\n",
      "Metadata: {'section_id': 4, 'source': 'fixed_formatted_content_v4.txt'}\n",
      "\n",
      "Document 2:\n",
      "--- 5.3 NUM (Numbers)\n",
      "\n",
      "The NUM data set contains numeric data, one row per data point as rendered by the Commission on the primary financial statements. The source for the table is the “as filed” XBRL...\n",
      "Metadata: {'source': 'fixed_formatted_content_v4.txt', 'section_id': 11}\n",
      "\n",
      "Document 3:\n",
      "-- 3 Organization\n",
      "\n",
      "Note that this data set represents quarterly and annual uncorrected and “as filed” EDGAR document submissions containing multiple reporting periods (including amendments of prior su...\n",
      "Metadata: {'source': 'fixed_formatted_content_v4.txt', 'section_id': 3}\n",
      "\n",
      "✅ Retrieved 3 documents.\n",
      "Documents retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "# from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "# Tải API key từ biến môi trường\n",
    "# load_dotenv()\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Khởi tạo mô hình nhúng\n",
    "openai_embed_model = OpenAIEmbeddings(\n",
    "    model='text-embedding-3-small',\n",
    "    api_key=\"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\"\n",
    ")\n",
    "\n",
    "def load_and_split_text(file_path):\n",
    "    \"\"\"Đọc file văn bản và chia thành các đoạn nhỏ.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Chia văn bản thành các đoạn dựa trên tiêu đề hoặc các đoạn văn lớn\n",
    "        sections = re.split(r'\\n\\s*(?=(?:--|---|Figure)\\s+.*?\\n)', text)\n",
    "        documents = []\n",
    "        for i, section in enumerate(sections):\n",
    "            section = section.strip()\n",
    "            if section:\n",
    "                # Tạo Document từ mỗi đoạn\n",
    "                doc = Document(\n",
    "                    page_content=section,\n",
    "                    metadata={\n",
    "                        \"section_id\": i,\n",
    "                        \"source\": file_path\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        print(f\"✅ Loaded and split text into {len(documents)} sections.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load and split text: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_vector_store(documents, persist_directory):\n",
    "    \"\"\"Tạo và lưu trữ vector database từ các tài liệu.\"\"\"\n",
    "    try:\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        # Xóa dữ liệu cũ nếu có\n",
    "        chroma_db.delete_collection()\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        # Thêm tài liệu vào vector database\n",
    "        chroma_db.add_documents(documents)\n",
    "        doc_count = chroma_db._collection.count()\n",
    "        print(f\"📄 Đã thêm {len(documents)} tài liệu vào vector DB.\")\n",
    "        print(f\"✅ Created vector DB with {doc_count} documents at {persist_directory}.\")\n",
    "        return chroma_db\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create vector DB: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_vector_store_and_retriever(persist_directory):\n",
    "    \"\"\"Tải vector database và khởi tạo retriever.\"\"\"\n",
    "    try:\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        doc_count = chroma_db._collection.count()\n",
    "        if doc_count > 0:\n",
    "            print(f\"✅ Loaded existing Vector DB with {doc_count} documents.\")\n",
    "        else:\n",
    "            print(\"⚠️ No documents found in the vector DB.\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            similarity_threshold_retriever = chroma_db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\",\n",
    "                search_kwargs={\"k\": 3, \"score_threshold\": 0.3}\n",
    "            )\n",
    "            print(\"✅ Retriever initialized with similarity_score_threshold.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to init threshold-based retriever: {e}\")\n",
    "            similarity_threshold_retriever = chroma_db.as_retriever(\n",
    "                search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "            )\n",
    "            print(\"🔁 Fallback to regular similarity retriever.\")\n",
    "        return chroma_db, similarity_threshold_retriever\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load Chroma DB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def retrieve(question, chroma_db, retriever):\n",
    "    \"\"\"Truy xuất tài liệu từ vector database dựa trên câu hỏi.\"\"\"\n",
    "    print(\"---RETRIEVAL FROM VECTOR DB---\")\n",
    "    documents = []\n",
    "    if retriever:\n",
    "        try:\n",
    "            documents = retriever.invoke(question)\n",
    "            if len(documents) == 0:\n",
    "                print(\"⚠️ No relevant documents retrieved.\")\n",
    "            else:\n",
    "                print(f\"📁 Retrieved {len(documents)} documents:\")\n",
    "                for i, doc in enumerate(documents):\n",
    "                    print(f\"Document {i+1}:\")\n",
    "                    print(doc.page_content[:200] + \"...\")  # Hiển thị 200 ký tự đầu tiên\n",
    "                    print(f\"Metadata: {doc.metadata}\")\n",
    "                    print(\"\")\n",
    "                print(f\"✅ Retrieved {len(documents)} documents.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "    else:\n",
    "        print(\"❌ No retriever available.\")\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def main():\n",
    "    # Đường dẫn đến file văn bản đã xử lý\n",
    "    file_path = \"fixed_formatted_content_v4.txt\"\n",
    "    # Đường dẫn để lưu vector database\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "    persist_directory = os.path.abspath(os.path.join(BASE_DIR, \"financial_data_db\"))\n",
    "\n",
    "    # Bước 1: Đọc và chia nhỏ văn bản\n",
    "    documents = load_and_split_text(file_path)\n",
    "    if not documents:\n",
    "        print(\"❌ No documents to process. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # Bước 2: Tạo vector database\n",
    "    chroma_db = create_vector_store(documents, persist_directory)\n",
    "    if chroma_db is None:\n",
    "        print(\"❌ Failed to create vector store. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # Bước 3: Tải vector database và khởi tạo retriever\n",
    "    chroma_db, retriever = get_vector_store_and_retriever(persist_directory)\n",
    "    if chroma_db is None or retriever is None:\n",
    "        print(\"❌ Failed to initialize vector store or retriever. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # Bước 4: Thử truy xuất với một câu hỏi\n",
    "    question = \"What is the relationship between the data sets NUM and SUB?\"\n",
    "    result = retrieve(question, chroma_db, retriever)\n",
    "    # in cau tra lơi \n",
    "    if result[\"documents\"]:\n",
    "        print(\"Documents retrieved successfully.\")\n",
    "    else:\n",
    "        print(\"No documents retrieved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad283954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: Financial Statement Data Sets\n",
      "\n",
      "Contents\n",
      "docs: 1 Overview.. 1\n",
      "docs: 2 Scope. 2\n",
      "docs: 3 Organization. 2\n",
      "docs: 4 File Formats. 3\n",
      "docs: 5 Table Definitions. 3\n",
      "    5.1 SUB (Submissions) 3\n",
      "    5.2 TAG (Tags) 7\n",
      "    5.3 NUM (Numbers) 7\n",
      "    5.4 PRE (Presentation of Statements) 8\n",
      "docs: -- 1 Overview\n",
      "\n",
      "The following data sets provide information extracted from XBRL submissions filed with the Commission in a flattened data format to assist users in more easily consuming the data for analysis. The data is sourced from selected information found in the XBRL tagged financial statements submitted by filers to the Commission. These data sets currently include quarterly and annual numeric data rendered by the Commission in the primary financial statements submitted by filers. Certain additional fields (e.g. Standard Industrial Classification (SIC)) used in the Commission’s EDGAR system are also included to help in supporting the use of the data. The information has been taken directly from submissions created by each registrant, and the data is “as filed” by the registrant. The information will be updated quarterly. Data contained in documents filed after the last business day of the quarter will be included in the next quarterly posting.\n",
      "\n",
      "DISCLAIMER: The Financial Statement Data Sets contain information derived from structured data filed with the Commission by individual registrants as well as Commission-generated filing identifiers. Because the data sets are derived from information provided by individual registrants, we cannot guarantee the accuracy of the data sets. In addition, it is possible inaccuracies or other errors were introduced into the data sets during the process of extracting the data and compiling the data sets. Finally, the data sets do not reflect all available information, including certain metadata associated with Commission filings. The data sets are intended to assist the public in analyzing data contained in Commission filings; however, they are not a substitute for such filings. Investors should review the full Commission filings before making any investment decision.\n",
      "\n",
      "The data extracted from the XBRL submissions is organized into four data sets containing information about submissions, numbers, taxonomy tags, and presentation. Each data set consists of rows and columns and is provided as a tab-delimited TXT format file. The data sets are as follows:\n",
      "\n",
      "      · · SUB – Submission data set; this includes one record for each XBRL submission with amounts rendered by the Commission in the primary financial statements. The set includes fields of information pertinent to the submission and the filing entity. Information is extracted from the SEC’s EDGAR system and the filings submitted to the SEC by registrants.\n",
      "\n",
      "      · · NUM – Number data set; this includes one row for each distinct amount appearing on the primary financial statements rendered by the Commission from each submission included in the SUB data set.\n",
      "\n",
      "      · · TAG – Tag data set; includes defining information about each numerical tag. Information includes tag descriptions (documentation labels), taxonomy version information and other tag attributes.\n",
      "\n",
      "      · · PRE – Presentation data set; this provides information about how the tags and numbers were presented in the primary financial statements as rendered by the Commission.\n",
      "docs: -- 2 Scope\n",
      "\n",
      "The scope of the data in the financial statement data sets consists of:\n",
      "\n",
      "      · · Numeric data on the primary financial statements as rendered by the Commission (Balance Sheet, Income Statement, Cash Flows, Changes in Equity, and Comprehensive Income) and page footnotes on those statements;\n",
      "\n",
      "      · · From XBRL submissions which include financial statements rendered by the Commission (e.g., 10-K, 10-Q, 20-F, 40-F);\n",
      "\n",
      "      · · Submitted from 4/15/2009 through the “Data Cutoff Date” inclusive (there is a file named 2009q1.zip on the SEC website that contains data sets with column headings only and no rows, merely so that all years prior to this year will consist of four zip files).\n",
      "\n",
      "All numeric data is “as filed.”\n",
      "docs: -- 3 Organization\n",
      "\n",
      "Note that this data set represents quarterly and annual uncorrected and “as filed” EDGAR document submissions containing multiple reporting periods (including amendments of prior submissions). Data in this submitted form may contain redundancies, inconsistencies, and discrepancies relative to other publication formats. There are four data sets.\n",
      "\n",
      "      · 1. SUB identifies all the EDGAR submissions with amounts rendered by the Commission on the primary financial statements in the data set, with each row having the unique (primary) key adsh, a 20 character EDGAR Accession Number with dashes in positions 11 and 14.\n",
      "\n",
      "      · 2. TAG is a data set of all numerical tags used in the submissions, both standard and custom. A unique key of each row is a combination of these fields:\n",
      "\n",
      "      · 1) tag – tag used by the filer\n",
      "\n",
      "      · 2) version – if a standard tag, the taxonomy of origin, otherwise equal to adsh.\n",
      "\n",
      "      · 3. NUM is a data set of all numeric XBRL facts presented on the primary financial statements as rendered by the Comission. A unique key of each row is a combination of the following fields:\n",
      "\n",
      "      · 1) adsh- EDGAR accession number\n",
      "\n",
      "      · 2) tag – tag used by the filer\n",
      "\n",
      "      · 3) version – if a standard tag, the taxonomy of origin, otherwise equal to adsh.\n",
      "\n",
      "      · 4) ddate - period end date\n",
      "\n",
      "      · 5) qtrs - duration in number of quarters\n",
      "\n",
      "      · 6) uom - unit of measure\n",
      "\n",
      "      · 7) segments – XBRL tags used to represent axis and member reporting\n",
      "\n",
      "      · 8) coreg - coregistrant of the parent company registrant (if applicable)\n",
      "\n",
      "      · 4. PRE is a data set that provides the text assigned by the filer to each line item in the primary financial statements, the order in which the line item appeared, and the tag assigned to it. A unique key of each row is a combination of the following fields:\n",
      "\n",
      "      · 1) adsh – EDGAR accession number\n",
      "\n",
      "      · 2) report – sequential number of report within the statements\n",
      "\n",
      "      · 3) line – sequential number of line within a report.\n",
      "\n",
      "The relationship of the data sets is as shown in Figure 1. The Accession Number (adsh) found in the NUM data set can be used to retrieve information about the submission in SUB. Each row of data in NUM was tagged by the filer using a tag. Information about the tag used can be found in TAG. Each row of data in NUM appears on one or more lines of reports detailed in PRE.\n",
      "docs: Figure Figure 1. Data relationships\n",
      "==================================================\n",
      "\n",
      "  Dataset: NUM has Columns referencing other datasets: adsh has Referenced dataset: SUB has Referenced columns: adsh\n",
      "  Dataset: tag, version has Columns referencing other datasets: TAG has Referenced dataset: tag, version\n",
      "  Dataset: PRE has Columns referencing other datasets: adsh has Referenced dataset: SUB has Referenced columns: adsh\n",
      "  Dataset: tag, version has Columns referencing other datasets: TAG has Referenced dataset: tag, version\n",
      "  Dataset: adsh, tag, version has Columns referencing other datasets: NUM has Referenced dataset: adsh, tag, version\n",
      "\n",
      "==================================================\n",
      "\n",
      "Note: The SEC website folder http://www.sec.gov/Archives/edgar/data/{cik}/{accession}/ will always contain all the files for a given submission, where {accession} is the adsh with the ‘-‘characters removed.\n",
      "docs: -- 4 File Formats\n",
      "\n",
      "Each of the four data sets is provided in a single encoding, as follows:\n",
      "\n",
      "Tab Delimited Value (.txt): utf-8, tab-delimited, \\n- terminated lines, with the first line containing the column names in lowercase.\n",
      "docs: -- 5 Table Definitions\n",
      "\n",
      "The columns in the figures below (figures 2 – 5) provide the following information: field name, description, source (SUB file only), data format, maximum field size, an indication of whether or not the field may be NULL (yes or no), and key.\n",
      "\n",
      "The Source column in the SUB file has two possible values:\n",
      "\n",
      "      · · EDGAR indicates that the source of the data is the filer’s EDGAR submission header.\n",
      "\n",
      "      · · XBRL indicates that the source of the data is the filer’s XBRL submission.\n",
      "\n",
      "The Key column indicates whether the field is part of a unique index on the data. There are two possible values for this column:\n",
      "\n",
      "      · · “*” – Indicates the field is part of a unique key for the row.\n",
      "\n",
      "      · · Empty (nothing in column) – the column is a function of all or some of a unique key.\n",
      "docs: --- 5.1 SUB (Submissions)\n",
      "\n",
      "The submissions data set contains summary information about an entire EDGAR submission. Some fields were sourced directly from EDGAR submission information, while other columns of data were sourced from the XBRL submission. Note: EDGAR derived fields represent the most recent EDGAR assignment as of a given filing’s submission date and do not necessarily represent the most current assignments.\n",
      "docs: Figure Figure 2. Fields in the SUB data set\n",
      "==================================================\n",
      "\n",
      "  Field Name: adsh has Field Description: Accession Number. The 20-character string formed from the 18-digit number assigned by the SEC to each EDGAR submission. has Source: EDGAR has Format: ALPHANUMERIC (nnnnnnnnnn-nn-nnnnnn) has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: cik has Field Description: Central Index Key (CIK). Ten digit number assigned by the SEC to each registrant that submits filings. has Source: EDGAR has Format: NUMERIC has Max Size: 10 has May be NULL: No has Key: \n",
      "  Field Name: name has Field Description: Name of registrant. This corresponds to the name of the legal entity as recorded in EDGAR as of the filing date. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 150 has May be NULL: No has Key: \n",
      "  Field Name: sic has Field Description: Standard Industrial Classification (SIC). Four digit code assigned by the SEC as of the filing date, indicating the registrant’s type of business. has Source: EDGAR has Format: NUMERIC has Max Size: 4 has May be NULL: Yes has Key: \n",
      "  Field Name: countryba has Field Description: The ISO 3166-1 country of the registrant's business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: stprba has Field Description: The state or province of the registrant’s business address, if field countryba is US or CA. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: cityba has Field Description: The city of the registrant's business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 30 has May be NULL: Yes has Key: \n",
      "  Field Name: zipba has Field Description: The zip code of the registrant’s business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 10 has May be NULL: Yes has Key: \n",
      "  Field Name: bas1 has Field Description: The first line of the street of the registrant’s business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 40 has May be NULL: Yes has Key: \n",
      "  Field Name: bas2 has Field Description: The second line of the street of the registrant’s business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 40 has May be NULL: Yes has Key: \n",
      "  Field Name: baph has Field Description: The phone number of the registrant’s business address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 20 has May be NULL: Yes has Key: \n",
      "  Field Name: countryma has Field Description: The ISO 3166-1 country of the registrant's mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: stprma has Field Description: The state or province of the registrant’s mailing address, if field countryma is US or CA. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: cityma has Field Description: The city of the registrant's mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 30 has May be NULL: Yes has Key: \n",
      "  Field Name: zipma has Field Description: The zip code of the registrant’s mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 10 has May be NULL: Yes has Key: \n",
      "  Field Name: mas1 has Field Description: The first line of the street of the registrant’s mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 40 has May be NULL: Yes has Key: \n",
      "  Field Name: mas2 has Field Description: The second line of the street of the registrant’s mailing address. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 40 has May be NULL: Yes has Key: \n",
      "  Field Name: countryinc has Field Description: The ISO 3166-1 country of incorporation for the registrant. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 3 has May be NULL: Yes has Key: \n",
      "  Field Name: stprinc has Field Description: The state or province of incorporation for the registrant, if countryinc is US or CA. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: ein has Field Description: Employee Identification Number, 9 digit identification number assigned by the Internal Revenue Service to business entities operating in the United States. has Source: EDGAR has Format: NUMERIC has Max Size: 10 has May be NULL: Yes has Key: \n",
      "  Field Name: former has Field Description: Most recent former name of the registrant, if any. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 150 has May be NULL: Yes has Key: \n",
      "  Field Name: changed has Field Description: Date of change from the former name, if any. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 8 has May be NULL: Yes has Key: \n",
      "  Field Name: afs has Field Description: Filer status with the SEC at the time of submission: 1-LAF=Large Accelerated, 2-ACC=Accelerated, 3-SRA=Smaller Reporting Accelerated, 4-NON=Non-Accelerated, 5-SML=Smaller Reporting Filer, NULL=not assigned. has Source: XBRL has Format: ALPHANUMERIC has Max Size: 5 has May be NULL: Yes has Key: \n",
      "  Field Name: wksi has Field Description: Well Known Seasoned Issuer (WKSI). An issuer that meets specific SEC requirements at some point during a 60-day period preceding the date the issuer satisfies its obligation to update its shelf registration statement. has Source: XBRL has Format: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: fye has Field Description: Fiscal Year End Date, rounded to nearest month-end. has Source: XBRL has Format: ALPHANUMERIC (mmdd) has Max Size: 4 has May be NULL: Yes has Key: \n",
      "  Field Name: form has Field Description: The submission type of the registrant’s filing. has Source: EDGAR has Format: ALPHANUMERIC has Max Size: 10 has May be NULL: No has Key: \n",
      "  Field Name: period has Field Description: Balance Sheet Date, rounded to nearest month-end. has Source: XBRL has Format: DATE (yyyymmdd) has Max Size: 8 has May be NULL: No has Key: \n",
      "  Field Name: fy has Field Description: Fiscal Year Focus (as defined in the EDGAR XBRL Guide Ch. 3.1.8). has Source: XBRL has Format: YEAR (yyyy) has Max Size: 4 has May be NULL: Yes has Key: \n",
      "  Field Name: fp has Field Description: Fiscal Period Focus (as defined in the EDGAR XBRL Guide Ch. 3.1.8) within Fiscal Year. has Source: XBRL has Format: ALPHANUMERIC (FY, Q1, Q2, Q3, Q4) has Max Size: 2 has May be NULL: Yes has Key: \n",
      "  Field Name: filed has Field Description: The date of the registrant’s filing with the Commission. has Source: EDGAR has Format: DATE (yyyymmdd) has Max Size: 8 has May be NULL: No has Key: \n",
      "  Field Name: accepted has Field Description: The acceptance date and time of the registrant’s filing with the Commission. has Source: EDGAR has Format: DATETIME (yyyy‑mm‑dd hh:mm:ss) has Max Size: 19 has May be NULL: No has Key: \n",
      "  Field Name: prevrpt has Field Description: Previous Report –TRUE indicates that the submission information was subsequently amended. has Source: EDGAR has Format: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: detail has Field Description: TRUE indicates that the XBRL submission contains quantitative disclosures within the footnotes and schedules at the required detail level (e.g., each amount). has Source: XBRL has Format: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: instance has Field Description: The name of the submitted XBRL Instance Document. The name often begins with the company ticker symbol. has Source: EDGAR has Format: ALPHANUMERIC (e.g. abcd‑yyyymmdd.xml) has Max Size: 40 has May be NULL: No has Key: \n",
      "  Field Name: nciks has Field Description: Number of Central Index Keys (CIK) of registrants (i.e., business units) included in the consolidating entity’s submitted filing. has Source: EDGAR has Format: NUMERIC has Max Size: 4 has May be NULL: No has Key: \n",
      "  Field Name: aciks has Field Description: Additional CIKs of co-registrants included in a consolidating entity’s EDGAR submission, separated by spaces. If there are no other co-registrants (i.e., nciks=1), the value of aciks is NULL. For a very small number of filers, the entire list of co-registrants is too long to fit in the field. Where this is the case, users should refer to the complete submission file for all CIK information. has Source: EDGAR has Format: ALPHANUMERIC (space delimited) has Max Size: 120 has May be NULL: Yes has Key: \n",
      "\n",
      "==================================================\n",
      "docs: --- 5.2 TAG (Tags)\n",
      "\n",
      "The TAG data set contains the standard taxonomy tags and the custom taxonomy tags defined in the submissions. The source is the “as filed” XBRL filer submissions. The standard tags are derived from taxonomies in https://www.sec.gov/data-research/standard-taxonomies.\n",
      "docs: Figure Figure 3. Fields in the TAG data set\n",
      "==================================================\n",
      "\n",
      "  Field Name: tag has Field Description: The unique identifier (name) for a tag in a specific taxonomy release. has Field Type: ALPHANUMERIC has Max Size: 256 has May be NULL: No has Key: *\n",
      "  Field Name: version has Field Description: For a standard tag, an identifier for the taxonomy; otherwise the accession number where the tag was defined. has Field Type: ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: custom has Field Description: 1 if tag is custom (version=adsh), 0 if it is standard. Note: This flag is technically redundant with the version and adsh columns. has Field Type: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: abstract has Field Description: 1 if the tag is not used to represent a numeric fact. has Field Type: BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: datatype has Field Description: If abstract=1, then NULL, otherwise the data type (e.g., monetary) for the tag. has Field Type: ALPHANUMERIC has Max Size: 20 has May be NULL: Yes has Key: \n",
      "  Field Name: iord has Field Description: If abstract=1, then NULL; otherwise, “I” if the value is a point-in time, or “D” if the value is a duration. has Field Type: ALPHANUMERIC has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: crdr has Field Description: If datatype = monetary, then the tag’s natural accounting balance (debit or credit); if not defined, then NULL. has Field Type: ALPHANUMERIC (“C” or “D”) has Max Size: 1 has May be NULL: Yes has Key: \n",
      "  Field Name: tlabel has Field Description: If a standard tag, then the label text provided by the taxonomy, otherwise the text provided by the filer. A tag which had neither would have a NULL value here. has Field Type: ALPHANUMERIC has Max Size: 512 has May be NULL: Yes has Key: \n",
      "  Field Name: doc has Field Description: The detailed definition for the tag. If a standard tag, then the text provided by the taxonomy, otherwise the text assigned by the filer. Some tags have neither, and this field is NULL. has Field Type: ALPHANUMERIC has Max Size:  has May be NULL: Yes has Key: \n",
      "\n",
      "==================================================\n",
      "docs: --- 5.3 NUM (Numbers)\n",
      "\n",
      "The NUM data set contains numeric data, one row per data point as rendered by the Commission on the primary financial statements. The source for the table is the “as filed” XBRL filer submissions.\n",
      "docs: Figure Figure 4. Fields in the NUM data set\n",
      "==================================================\n",
      "\n",
      "  Field Name: adsh has Field Description: Accession Number. The 20-character string formed from the 18-digit number assigned by the SEC to each EDGAR submission. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: tag has Field Description: The unique identifier (name) for a tag in a specific taxonomy release. has Field Type (format): ALPHANUMERIC has Max Size: 256 has May be NULL: No has Key: *\n",
      "  Field Name: version has Field Description: For a standard tag, an identifier for the taxonomy; otherwise the accession number where the tag was defined. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: ddate has Field Description: The end date for the data value, rounded to the nearest month end. has Field Type (format): DATE (yyyymmdd) has Max Size: 8 has May be NULL: No has Key: *\n",
      "  Field Name: qtrs has Field Description: The count of the number of quarters represented by the data value, rounded to the nearest whole number. “0” indicates it is a point-in-time value. has Field Type (format): NUMERIC has Max Size: 8 has May be NULL: No has Key: *\n",
      "  Field Name: uom has Field Description: The unit of measure for the value. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: segments has Field Description: Tags used to represent axis and member reporting. has Field Type (format): ALPHANUMERIC has Max Size: 1024 has May be NULL: Yes has Key: *\n",
      "  Field Name: coreg has Field Description: If specified, indicates a specific co-registrant, the parent company, or other entity (e.g., guarantor). NULL indicates the consolidated entity. has Field Type (format): ALPHANUMERIC has Max Size: 256 has May be NULL: Yes has Key: *\n",
      "  Field Name: value has Field Description: The value. This is not scaled, it is as found in the Interactive Data file, but is limited to four digits to the right of the decimal point. has Field Type (format): NUMERIC(28,4) has Max Size: 16 has May be NULL: Yes has Key: \n",
      "  Field Name: footnote has Field Description: The text of any superscripted footnotes on the value, as shown on the statement page, truncated to 512 characters, or if there is no footnote, then this field will be blank. has Field Type (format): ALPHANUMERIC has Max Size: 512 has May be NULL: Yes has Key: \n",
      "\n",
      "==================================================\n",
      "docs: --- 5.4 PRE (Presentation of Statements)\n",
      "\n",
      "The PRE data set contains one row for each line of the financial statements tagged by the filer. The source for the data set is the “as filed” XBRL filer submissions. Note that there may be more than one row per entry in NUM because the same tag can appear in more than one statement (the tag NetIncome, for example can appear in both the Income Statement and Cash Flows in a single financial statement, and the tag Cash may appear in both the Balance Sheet and Cash Flows).\n",
      "docs: Figure Figure 5. Fields in the PRE data set\n",
      "==================================================\n",
      "\n",
      "  Field Name: adsh has Field Description: Accession Number. The 20-character string formed from the 18-digit number assigned by the SEC to each EDGAR submission. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: *\n",
      "  Field Name: report has Field Description: Represents the report grouping. This field corresponds to the statement (stmt) field, which indicates the type of statement. The numeric value refers to the “R file” as posted on the EDGAR Web site. has Field Type (format): NUMERIC has Max Size: 6 has May be NULL: No has Key: *\n",
      "  Field Name: line has Field Description: Represents the tag’s presentation line order for a given report. Together with the statement and report field, presentation location, order and grouping can be derived. has Field Type (format): NUMERIC has Max Size: 6 has May be NULL: No has Key: *\n",
      "  Field Name: stmt has Field Description: The financial statement location to which the value of the “report field pertains. has Field Type (format): ALPHANUMERIC (BS = Balance Sheet, IS = Income Statement, CF = Cash Flow, EQ = Equity, CI = Comprehensive Income, SI = Schedule of Investments, UN = Unclassifiable Statement). has Max Size: 2 has May be NULL: No has Key: \n",
      "  Field Name: inpth has Field Description: Value was presented “parenthetically” instead of in columns within the financial statements. For example: Receivables (net of allowance for bad debts of $200 in 2012) $700. has Field Type (format): BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: rfile has Field Description: The type of interactive data file rendered on the EDGAR web site, H = .htm file, X = .xml file. has Field Type (format): ALPHANUMERIC has Max Size: 1 has May be NULL: No has Key: \n",
      "  Field Name: tag has Field Description: The tag chosen by the filer for this line item. has Field Type (format): ALPHANUMERIC has Max Size: 256 has May be NULL: No has Key: \n",
      "  Field Name: version has Field Description: The taxonomy identifier if the tag is a standard tag, otherwise adsh. has Field Type (format): ALPHANUMERIC has Max Size: 20 has May be NULL: No has Key: \n",
      "  Field Name: plabel has Field Description: The text presented on the line item, also known as a “preferred” label. has Field Type (format): ALPHANUMERIC has Max Size: 512 has May be NULL: No has Key: \n",
      "  Field Name: negating has Field Description: Flag to indicate whether the plabel is negating. has Field Type (format): BOOLEAN (1 if true and 0 if false) has Max Size: 1 has May be NULL: No has Key: \n",
      "\n",
      "==================================================\n",
      "✅ Loaded and split text into 20 sections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoduy\\AppData\\Local\\Temp\\ipykernel_3720\\1026261183.py:48: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Đã thêm 20 tài liệu vào vector DB.\n",
      "✅ Created vector DB with 20 documents at d:\\hk2_nam3\\langraph\\day4\\kiennguyen\\financial_data_db.\n",
      "✅ Loaded existing Vector DB with 20 documents.\n",
      "✅ Retriever initialized with similarity_score_threshold.\n",
      "---RETRIEVAL FROM VECTOR DB---\n",
      "📁 Retrieved 3 documents:\n",
      "Document 1:\n",
      "Figure Figure 1. Data relationships\n",
      "==================================================\n",
      "\n",
      "  Dataset: NUM has Columns referencing other datasets: adsh has Referenced dataset: SUB has Referenced columns: ...\n",
      "Metadata: {'source': 'fixed_formatted_content_v4.txt', 'section_id': 9}\n",
      "\n",
      "Document 2:\n",
      "--- 5.3 NUM (Numbers)\n",
      "\n",
      "The NUM data set contains numeric data, one row per data point as rendered by the Commission on the primary financial statements. The source for the table is the “as filed” XBRL...\n",
      "Metadata: {'source': 'fixed_formatted_content_v4.txt', 'section_id': 16}\n",
      "\n",
      "Document 3:\n",
      "-- 3 Organization\n",
      "\n",
      "Note that this data set represents quarterly and annual uncorrected and “as filed” EDGAR document submissions containing multiple reporting periods (including amendments of prior su...\n",
      "Metadata: {'section_id': 8, 'source': 'fixed_formatted_content_v4.txt'}\n",
      "\n",
      "\n",
      "✅ Trả lời tổng hợp:\n",
      "The NUM data set references the SUB data set through the 'adsh' column. The Accession Number (adsh) found in the NUM data set can be used to retrieve information about the submission in SUB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "OPENAI_API_KEY=\"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\"\n",
    "\n",
    "\n",
    "# Cấu hình API key OpenAI\n",
    "openai_embed_model = OpenAIEmbeddings(\n",
    "    model='text-embedding-3-small',\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "def load_and_split_text(file_path):\n",
    "    \"\"\"Đọc file văn bản và chia thành các đoạn nhỏ.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Chia văn bản thành các đoạn theo Figure, Section\n",
    "        sections = re.split(r'\\n\\s*(?=(?:--|---|Figure|\\d+)\\s+.*?\\n)', text)\n",
    "        documents = []\n",
    "        for i, section in enumerate(sections):\n",
    "            section = section.strip()\n",
    "            print(\"docs:\",section)\n",
    "            if section:\n",
    "                doc = Document(\n",
    "                    page_content=section,\n",
    "                    metadata={\n",
    "                        \"section_id\": i,\n",
    "                        \"source\": file_path\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        print(f\"✅ Loaded and split text into {len(documents)} sections.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load and split text: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_vector_store(documents, persist_directory):\n",
    "    \"\"\"Tạo và lưu trữ vector database từ các tài liệu.\"\"\"\n",
    "    try:\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        chroma_db.delete_collection()\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        chroma_db.add_documents(documents)\n",
    "        doc_count = chroma_db._collection.count()\n",
    "        print(f\"📄 Đã thêm {len(documents)} tài liệu vào vector DB.\")\n",
    "        print(f\"✅ Created vector DB with {doc_count} documents at {persist_directory}.\")\n",
    "        return chroma_db\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create vector DB: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_vector_store_and_retriever(persist_directory):\n",
    "    \"\"\"Tải vector database và khởi tạo retriever.\"\"\"\n",
    "    try:\n",
    "        chroma_db = Chroma(\n",
    "            collection_name='financial_data_db',\n",
    "            embedding_function=openai_embed_model,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        doc_count = chroma_db._collection.count()\n",
    "        if doc_count > 0:\n",
    "            print(f\"✅ Loaded existing Vector DB with {doc_count} documents.\")\n",
    "        else:\n",
    "            print(\"⚠️ No documents found in the vector DB.\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            retriever = chroma_db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\",\n",
    "                search_kwargs={\"k\": 3, \"score_threshold\": 0.3}\n",
    "            )\n",
    "            print(\"✅ Retriever initialized with similarity_score_threshold.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Fallback: {e}\")\n",
    "            retriever = chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "            print(\"🔁 Fallback to regular similarity retriever.\")\n",
    "        return chroma_db, retriever\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load Chroma DB: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def retrieve_and_answer(question, chroma_db, retriever):\n",
    "    \"\"\"Truy xuất tài liệu và tạo câu trả lời tổng hợp.\"\"\"\n",
    "    print(\"---RETRIEVAL FROM VECTOR DB---\")\n",
    "    if not retriever:\n",
    "        print(\"❌ No retriever available.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        documents = retriever.invoke(question)\n",
    "        if not documents:\n",
    "            print(\"⚠️ No relevant documents retrieved.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"📁 Retrieved {len(documents)} documents:\")\n",
    "        for i, doc in enumerate(documents):\n",
    "            print(f\"Document {i+1}:\")\n",
    "            print(doc.page_content[:200] + \"...\")\n",
    "            print(f\"Metadata: {doc.metadata}\\n\")\n",
    "\n",
    "        # Gộp nội dung để tạo prompt\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # LLM để trả lời\n",
    "        llm = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "Trả lời câu hỏi sau dựa trên nội dung tài liệu bên dưới:\n",
    "\n",
    "Câu hỏi: {question}\n",
    "\n",
    "Tài liệu:\n",
    "{context}\n",
    "\n",
    "Trả lời ngắn gọn, rõ ràng và chính xác:\n",
    "\"\"\")\n",
    "        chain = prompt | llm\n",
    "        answer = chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "        print(\"\\n✅ Trả lời tổng hợp:\")\n",
    "        print(answer.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during retrieval or answer generation: {e}\")\n",
    "\n",
    "def main():\n",
    "    file_path = \"fixed_formatted_content_v4.txt\"\n",
    "    BASE_DIR = os.getcwd()\n",
    "    persist_directory = os.path.abspath(os.path.join(BASE_DIR, \"financial_data_db\"))\n",
    "\n",
    "    documents = load_and_split_text(file_path)\n",
    "    if not documents:\n",
    "        return\n",
    "\n",
    "    chroma_db = create_vector_store(documents, persist_directory)\n",
    "    if chroma_db is None:\n",
    "        return\n",
    "\n",
    "    chroma_db, retriever = get_vector_store_and_retriever(persist_directory)\n",
    "    if chroma_db is None or retriever is None:\n",
    "        return\n",
    "\n",
    "    question = \"What is the relationship between the data sets NUM and SUB?\"\n",
    "    retrieve_and_answer(question, chroma_db, retriever)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "746f7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Vector DB contains: 20 documents\n",
      "\n",
      "✅ Trả lời:\n",
      "Tên trường chứa mã zip của địa chỉ kinh doanh của người đăng ký là \"zipba\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "# Embedding model\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model='text-embedding-3-small',\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "def get_retriever(persist_directory):\n",
    "    chroma_db = Chroma(\n",
    "        collection_name='financial_data_db',\n",
    "        embedding_function=embedding_model,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    print(\"📄 Vector DB contains:\", chroma_db._collection.count(), \"documents\")\n",
    "    \n",
    "    try:\n",
    "        retriever = chroma_db.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\"k\": 5, \"score_threshold\": 0.1}\n",
    "        )\n",
    "    except:\n",
    "        retriever = chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def ask_question(question, retriever):\n",
    "    documents = retriever.invoke(question)\n",
    "    if not documents:\n",
    "        print(\"⚠️ No relevant documents found.\")\n",
    "        return\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Trả lời câu hỏi sau dựa trên nội dung tài liệu bên dưới:\n",
    "\n",
    "Câu hỏi: {question}\n",
    "\n",
    "Tài liệu:\n",
    "{context}\n",
    "\n",
    "Trả lời ngắn gọn, rõ ràng và chính xác:\n",
    "\"\"\")\n",
    "    chain = prompt | llm\n",
    "    answer = chain.invoke({\"question\": question, \"context\": context})\n",
    "    print(\"\\n✅ Trả lời:\")\n",
    "    print(answer.content)\n",
    "\n",
    "def main():\n",
    "    persist_dir = os.path.abspath(os.path.join(os.getcwd(), \"financial_data_db\"))\n",
    "    retriever = get_retriever(persist_dir)\n",
    "    \n",
    "    # 👉 Nhập câu hỏi tại đây\n",
    "    # question = \"How many records does the sub include per XBRL submission?\"\n",
    "    question = \"The zip code of the registrant's business address là gì\"\n",
    "    ask_question(question, retriever)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b833c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
